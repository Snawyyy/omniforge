[
  {
    "id": 0,
    "hash": "43b76714d230bbe319bdf0de327c7192",
    "content": "RAG stands for Retrieval-Augmented Generation, a technique that combines information retrieval with language generation.",
    "file": "document_0"
  },
  {
    "id": 1,
    "hash": "dac60aa8d5e5861796640a8d9b3190a3",
    "content": "The RAG model retrieves relevant documents from a knowledge base and uses them to generate more accurate responses.",
    "file": "document_0"
  },
  {
    "id": 2,
    "hash": "f9ed7c2f9da6b1462806979d984ab5f3",
    "content": "FAISS is a library for efficient similarity search and clustering of dense vectors, often used in RAG systems.",
    "file": "document_0"
  },
  {
    "id": 3,
    "hash": "192336175c79d0194d2e51a613cbfa1b",
    "content": "Sentence transformers are used to create embeddings for documents and queries in RAG systems.",
    "file": "document_0"
  },
  {
    "id": 4,
    "hash": "92c5abaa3019f7294e5af489622302f5",
    "content": "Retrieval-Augmented Generation improves language models by allowing them to access external knowledge sources.",
    "file": "document_0"
  },
  {
    "id": 5,
    "hash": "92408c5c2b26cc7bb3e006f75f4f6f08",
    "content": "In RAG systems, documents are indexed and searchable by their semantic embeddings rather than just keywords.",
    "file": "document_0"
  },
  {
    "id": 6,
    "hash": "26ed14b833f20c4d6715cd2f9c44381a",
    "content": "Python is a great language for implementing RAG systems due to libraries like sentence-transformers and faiss.",
    "file": "document_0"
  },
  {
    "id": 7,
    "hash": "964a8aed30280d1b398f7b17dc6366bb",
    "content": "The retrieval component of RAG is crucial for finding relevant information before generation.",
    "file": "document_0"
  },
  {
    "id": 8,
    "hash": "1b61704bafe182154c395ea08a3acb39",
    "content": "\"\"\"\nOmni - AI-powered code generation and project-aware editing CLI tool\n\nIntegrates modular UI, memory, personality, and AST-based code editing.\n\"\"\"\nimport os\nimport subprocess\nimport requests\nimport json\nimport sys\nimport re\nimport argparse\nimport ast\nfrom datetime import datetime\nimport threading\nimport queue as Queue\nimport time\nfrom typing import List, Dict, Optional\nfrom rich import print\nfrom rich.panel import Panel\nfrom rich.console import Console\nfrom rich.tree import Tree\nfrom ui_manager import UIManager\nfrom personality_manager import PersonalityManager\nfrom memory_manager import MemoryManager\nfrom code_editor import CodeEditor\nfrom file_creator import FileCreator\nfrom git_manager import GitManager\nimport traceback\nDEFAULT_BACKEND = 'openrouter'\nOLLAMA_MODEL = 'phi4-reasoning'\nOPENROUTER_MODEL = 'qwen/qwen3-coder'\nDEFAULT_SAVE_DIR = os.path.expanduser('/mnt/ProjectData/omni/omni_saves/')\nCONFIG_FILE = 'config.json'\nMEMORY_FILE = 'memory.json'\nOLLAMA_API_URL = 'http://localhost:11434/api/generate'\nOPENROUTER_API_URL = 'https://openrouter.ai/api/v1/chat/completions'\nOPENROUTER_MODELS_API_URL = 'https://openrouter.ai/api/v1/models'\nOPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')\ncurrent_backend = DEFAULT_BACKEND\ncurrent_model = (OLLAMA_MODEL if DEFAULT_BACKEND == 'ollama' else\n    OPENROUTER_MODEL)\nOLLAMA_MODELS = {'deepseek': 'deepseek-coder:6.7b', 'codellama':\n    'codellama:13b', 'mistral': 'mistral:latest', 'llama2': 'llama2:latest',\n    'phind': 'phind-codellama:34b'}\nos.makedirs(DEFAULT_SAVE_DIR, exist_ok=True)\nTEMPLATES = {'flask':\n    \"\"\"from flask import Flask, jsonify\n\napp = Flask(__name__)\n\n@app.route('/')\ndef home():\n    return '<h1>Hello, Flask!</h1>'\n\nif __name__ == '__main__':\n    app.run(debug=True)\"\"\"\n    , 'html5':\n    \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>New Page</title>\n</head>\n<body>\n    <h1>Hello World</h1>\n</body>\n</html>\"\"\"\n    , 'scraper':\n    \"\"\"import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.content, 'html.parser')\n        print(soup.title.text)\n    except Exception as e:\n        print(f\"[bold red]Error scraping {url}:[/] {e}\")\n\nif __name__ == \"__main__\":\n    scrape(\"https://example.com\")\"\"\"\n    }\nconsole = Console()\npersonality_manager = PersonalityManager(CONFIG_FILE)\nmemory_manager = MemoryManager(MEMORY_FILE)\nui_manager = UIManager()\nlast_query: Optional[str] = None\nlast_response: Optional[str] = None\nlast_code: Optional[str] = None\n\n\ndef start_ollama_server() ->None:\n    if current_backend != 'ollama':\n        return\n    try:\n        requests.get('http://localhost:11434', timeout=1)\n    except requests.exceptions.ConnectionError:\n        print('[cyan]Starting Ollama server...[/]')\n        subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL)\n\n\ndef query_llm(prompt: str) ->str:\n    personality = personality_manager.get_current_personality()\n    system_prompt = personality.get('system_prompt', '') if personality else ''\n    memory_context = memory_manager.get_memory_context()\n    rag_context = ''\n    try:\n        from rag_manager import RAGManager\n        project_root = memory_manager.get_project_root()\n        if project_root:\n            rag_manager = RAGManager()\n            if rag_manager.get_document_count() > 0:\n                results = rag_manager.search(prompt, k=3)\n                if results:\n                    rag_context = '\\n\\nRelevant context from codebase:\\n'\n                    for i, (doc, score, meta) in enumerate(results, 1):\n                        file_path = meta.get('file', 'Unknown')\n                        rag_context += f'{i}. [{file_path}] {doc}\\n'\n    except Exception:\n        pass\n    full_prompt = (\n        f'{system_prompt}\\n\\n{memory_context}{rag_context}\\n\\nUser: {prompt}')\n    with ui_manager.show_spinner('AI is listening and thinking...'):\n        if current_backend == 'ollama':\n            response = query_ollama(full_prompt)\n        elif current_backend == 'openrouter':\n            response = query_openrouter(full_prompt)\n        else:\n            response = '[bold red]Error:[/] Unknown backend'\n    return response\n\n\ndef query_openrouter(prompt: str) ->str:\n    if not OPENROUTER_API_KEY:\n        return '[bold red]Error:[/] OPENROUTER_API_KEY not set.'\n    headers = {'Authorization': f'Bearer {OPENROUTER_API_KEY}',\n        'Content-Type': 'application/json'}\n    payload = {'model': current_model, 'messages': [{'role': 'user',\n        'content': prompt}]}\n    try:\n        response = requests.post(OPENROUTER_API_URL, headers=headers, json=\n            payload, timeout=90)\n        response.raise_for_status()\n        return response.json()['choices'][0]['message']['content']\n    except Exception as e:\n        error_details = ''\n        try:\n            error_details = response.json()\n        except:\n            error_details = response.text if hasattr(response, 'text'\n                ) else str(e)\n        return (\n            f'[bold red]OpenRouter Error:[/] {e}\\n[dim]Details: {error_details}[/dim]'\n            )\n\n\ndef query_ollama(prompt: str) ->str:\n    payload = {'model': current_model, 'prompt': prompt, 'stream': False}\n    try:\n        response = requests.post(OLLAMA_API_URL, json=payload, timeout=90)\n        response.raise_for_status()\n        return response.json()['response']\n    except Exception as e:\n        return f'[bold red]Ollama Error:[/] {e}'\n\n\ndef extract_code(text: str) ->List[tuple[str, str]]:\n    matches = re.findall('```(\\\\w*)\\\\n([\\\\s\\\\S]*?)```', text)\n    return [(lang or 'text', code.strip()) for lang, code in matches\n        ] if matches else []\n\n\ndef list_models(args: list=None) ->None:\n    if current_backend == 'ollama':\n        print('[bold cyan]Popular Ollama Models:[/]')\n        for name, model_id in OLLAMA_MODELS.items():\n            print(\n                f\"{'\u2b50' if model_id == current_model else '  '} [yellow]{name:12}[/] \u2192 {model_id}\"\n                )\n    elif current_backend == 'openrouter':\n        list_openrouter_models(args or [])\n\n\ndef list_openrouter_models(args: list):\n    try:\n        from simple_term_menu import TerminalMenu\n    except ImportError:\n        ui_manager.show_error(\n            \"'simple-term-menu' is required. `pip install simple-term-menu`\")\n        return\n    try:\n        with ui_manager.show_spinner('Fetching models...'):\n            response = requests.get(OPENROUTER_MODELS_API_URL)\n            response.raise_for_status()\n        api_models_data = response.json().get('data', [])\n    except requests.RequestException as e:\n        ui_manager.show_error(f'Error fetching models: {e}')\n        return\n    all_models, sources = [], set()\n    for model_data in api_models_data:\n        if (model_id := model_data.get('id')):\n            sources.add(model_id.split('/')[0])\n            pricing = model_data.get('pricing', {})\n            is_free = pricing.get('prompt') == '0' and pricing.get('completion'\n                ) == '0'\n            all_models.append({'id': model_id, 'name': model_data.get(\n                'name'), 'source': model_id.split('/')[0], 'is_free': is_free})\n    all_models.sort(key=lambda x: (x['source'], x['name']))\n    if args and args[0].lower() == 'sources':\n        print('[bold cyan]Available Model Sources:[/]')\n        [print(f'  [yellow]{s}[/]') for s in sorted(list(sources))]\n        return\n    models_to_display, title = all_models, 'Select an OpenRouter Model'\n    if args:\n        filter_keyword = args[0].lower()\n        title = f\"Select a Model from '{filter_keyword}'\"\n        models_to_display = [m for m in all_models if filter_keyword in m[\n            'source'].lower()]\n        if not models_to_display:\n            ui_manager.show_error(f\"No models for source: '{filter_keyword}'\")\n            return\n    menu_entries = [\n        f\"{'\u2b50' if m['id'] == current_model else '  '} {m['name']} [dim]({m['id']}){' [green](FREE)[/]' if m['is_free'] else ''}[/dim]\"\n         for m in models_to_display]\n    try:\n        cursor_idx = next((i for i, m in enumerate(models_to_display) if m[\n            'id'] == current_model), 0)\n        chosen_index = TerminalMenu(menu_entries, title=f'{title}',\n            cursor_index=cursor_idx, cycle_cursor=True, clear_screen=True\n            ).show()\n        if chosen_index is not None:\n            set_model(models_to_display[chosen_index]['id'])\n        else:\n            print('Model selection cancelled.')\n    except Exception as e:\n        ui_manager.show_error(f'Menu display error: {e}')\n\n\ndef set_model(model_id: str) ->None:\n    global current_model\n    current_model = model_id\n    ui_manager.show_success(f'Model set to: {current_model}')\n\n\ndef switch_backend(backend_name: str) ->None:\n    global current_backend, current_model\n    backend_name = backend_name.lower()\n    if backend_name not in ['ollama', 'openrouter']:\n        ui_manager.show_error(f'Unknown backend: {backend_name}')\n        return\n    current_backend = backend_name\n    current_model = (OLLAMA_MODEL if backend_name == 'ollama' else\n        OPENROUTER_MODEL)\n    ui_manager.show_success(\n        f'Switched to {backend_name} backend with model: {current_model}')\n    if backend_name == 'ollama':\n        start_ollama_server()\n\n\ndef generate_project_manifest(path: str) ->tuple[str, List[str]]:\n    manifest = ''\n    file_paths = []\n    tree = Tree(f'[bold cyan]Project: {os.path.basename(path)}[/]')\n    exclude_dirs = {'__pycache__', '.git', 'venv', 'node_modules', '.idea'}\n    for root, dirs, files in os.walk(path):\n        dirs[:] = [d for d in dirs if d not in exclude_dirs and not d.\n            startswith('.')]\n        relative_path = os.path.relpath(root, path)\n        branch = tree\n        if relative_path != '.':\n            parts = relative_path.split(os.sep)\n            for part in parts:\n                child = next((c for c in branch.children if c.label ==\n                    f'[magenta]{part}[/]'), None)\n                if not child:\n                    child = branch.add(f'[magenta]{part}[/]')\n                branch = child\n        for fname in sorted(files):\n            ext = os.path.splitext(fname)[1]\n            if ext not in ('.py', '.js', '.html', '.css', '.md', '.txt'):\n                continue\n            rel_path = os.path.join(relative_path, fname\n                ) if relative_path != '.' else fname\n            file_paths.append(rel_path)\n            branch.add(f'[green]{fname}[/]' if ext == '.py' else\n                f'[dim]{fname}[/]')\n            manifest += f'File: {rel_path}\\n\\n'\n    console.print(tree)\n    return manifest.strip(), file_paths\n\n\ndef look_command(path: str) ->None:\n    \"\"\"\n    Scans a directory or file and loads it into memory. It can resolve paths\n    relative to the current working directory or the project root in memory.\n    If a new directory is scanned, the previous 'look' context is cleared\n    to ensure the context remains relevant.\n    \"\"\"\n    resolved_path = resolve_file_path(path)\n    if not resolved_path:\n        ui_manager.show_error(f'\u274c Path not found: {path}')\n        return\n    if resolved_path != os.path.abspath(path):\n        ui_manager.show_success(\n            f\"Found '{path}' in project. Using: {resolved_path}\")\n    if os.path.isdir(resolved_path):\n        ui_manager.show_success(\n            \"New project directory detected. Clearing previous 'look' context.\"\n            )\n        memory_manager.memory['look'] = []\n        with ui_manager.show_spinner('Generating project manifest...'):\n            manifest = generate_project_manifest(resolved_path)\n        memory_manager.add_look_data(resolved_path, manifest)\n        ui_manager.show_success('\u2705 Project manifest added to memory.')\n    else:\n        try:\n            with ui_manager.show_spinner('Loading file...'):\n                with open(resolved_path, 'r', encoding='utf-8') as f:\n                    content = f.read().strip()\n            for item in memory_manager.memory['look']:\n                if item.get('file') == resolved_path:\n                    item['content'] = content\n                    memory_manager.save_memory()\n                    ui_manager.show_success(\n                        '\u2705 Refreshed file content in memory.')\n                    return\n            memory_manager.add_look_data(resolved_path, content)\n            ui_manager.show_success('\u2705 File content added to memory.')\n        except Exception as e:\n            ui_manager.show_error(f'\u274c Error reading file: {e}')\n\n\ndef resolve_file_path(path: str) ->Optional[str]:\n    \"\"\"Resolves a file path, checking CWD first, then against project root in memory.\"\"\"\n    if os.path.exists(path):\n        return os.path.abspath(path)\n    project_root = memory_manager.get_project_root()\n    if project_root:\n        full_path = os.path.join(project_root, path)\n        if os.path.exists(full_path):\n            return full_path\n    return None\n\n\ndef _create_prompt_for_file_creation(file_name: str, instruction: str) ->str:\n    \"\"\"\n    Generate a robust prompt for file creation that instructs the AI to act as an expert,\n    produce complete and clean code, and avoid any extra commentary.\n    \"\"\"\n    return f\"\"\"You are an expert programmer tasked with creating a new file. Your goal is to generate complete, production-ready content based on the user's instruction.\n\nIMPORTANT RULES:\n- Provide ONLY the raw file content - no explanations, notes, or commentary outside the file itself\n- Include all necessary imports, boilerplate, and complete implementations\n- If creating code, ensure it's syntactically correct and follows best practices\n- For configuration files, use appropriate formatting (JSON, YAML, etc.)\n- For documentation files, use proper markdown formatting\n\nFile to create: {file_name}\nUser instruction: {instruction}\n\nGenerate the complete file content now:\"\"\"\n\n\ndef handle_file_create_command(file_path: str, instruction: str):\n    \"\"\"\n    Uses the LLM to generate content for a new file based on an instruction.\n    \"\"\"\n    global last_code\n    if os.path.exists(file_path):\n        if ui_manager.get_user_input(\n            f\"File '{file_path}' already exists. Overwrite? (y/n): \").lower(\n            ) not in ['yes', 'y']:\n            ui_manager.show_error('File creation cancelled.')\n            return\n    prompt = _create_prompt_for_file_creation(os.path.basename(file_path),\n        instruction)\n    with ui_manager.show_spinner(\n        f\"AI is generating content for '{file_path}'...\"):\n        response = query_llm(prompt)\n    code_blocks = extract_code(response)\n    if code_blocks:\n        new_content = code_blocks[0][1]\n    else:\n        new_content = response.strip()\n    if not new_content:\n        ui_manager.show_error('AI did not return any content.')\n        print(Panel(response, title=\"[yellow]AI's Raw Response[/]\"))\n        return\n    print(Panel(new_content, title=\n        f'[bold yellow]Proposed content for {file_path}[/]', border_style=\n        'yellow'))\n    if ui_manager.get_user_input('Create this file? (y/n): ').lower() in ['yes'\n        , 'y']:\n        try:\n            FileCreator.create(file_path, new_content)\n            last_code = new_content\n            ui_manager.show_success(f'File created successfully: {file_path}')\n        except IOError as e:\n            ui_manager.show_error(f'Error creating file: {e}')\n    else:\n        ui_manager.show_error('File creation cancelled.')\n\n\ndef look_all_command() ->None:\n    \"\"\"\n    Finds the project manifest in memory, reads every file listed, and adds their content to memory.\n    \"\"\"\n    project_root = memory_manager.get_project_root()\n    if not project_root:\n        ui_manager.show_error(\n            \"No project context in memory. Use 'look <directory>' first to generate a manifest.\"\n            )\n        return\n    manifest_data = None\n    for item in memory_manager.memory.get('look', []):\n        if item.get('file') == project_root and item.get('type'\n            ) == 'directory':\n            manifest_data = item.get('content')\n            break\n    if not manifest_data or not isinstance(manifest_data, (list, tuple)\n        ) or len(manifest_data) != 2:\n        ui_manager.show_error(\n            \"Could not find a valid project manifest in memory. Please run 'look <directory>' again.\"\n            )\n        return\n    file_paths = manifest_data[1]\n    if not file_paths:\n        ui_manager.show_error('No files found in the project manifest.')\n        return\n    total_files = len(file_paths)\n    loaded_count = 0\n    with ui_manager.show_spinner(\n        f'Loading {total_files} files from project manifest...'):\n        for file_path_relative in file_paths:\n            full_path = os.path.join(project_root, file_path_relative)\n            if os.path.isfile(full_path):\n                try:\n                    with open(full_path, 'r', encoding='utf-8') as f:\n                        content = f.read().strip()\n                    if not any(look['file'] == full_path for look in\n                        memory_manager.memory['look']):\n                        memory_manager.add_look_data(full_path, content)\n                        loaded_count += 1\n                except Exception as e:\n                    print(\n                        f\"[yellow]Skipping '{file_path_relative}': {e}[/yellow]\"\n                        )\n    ui_manager.show_success(\n        f'\u2705 Loaded content for {loaded_count} new files into memory.')\n\n\ndef _load_all_project_files_if_needed():\n    \"\"\"\n    Checks if a project is loaded and automatically loads any files from its\n    manifest that are not already in the 'look' memory. This ensures a\n    complete context for editing and refactoring commands.\n    \"\"\"\n    project_root = memory_manager.get_project_root()\n    if not project_root:\n        return\n    manifest_content = None\n    for item in memory_manager.memory.get('look', []):\n        if item.get('file') == project_root and 'File:' in item.get('content',\n            ''):\n            manifest_content = item['content']\n            break\n    if not manifest_content:\n        return\n    existing_file_paths = {item['file'] for item in memory_manager.memory.\n        get('look', []) if item.get('type') == 'file'}\n    file_paths_relative = re.findall('File: (.*)', manifest_content)\n    files_to_load = []\n    for rel_path in file_paths_relative:\n        full_path = os.path.join(project_root, rel_path)\n        if full_path not in existing_file_paths and os.path.isfile(full_path):\n            files_to_load.append((full_path, rel_path))\n    if not files_to_load:\n        return\n    loaded_count = 0\n    with ui_manager.show_spinner(\n        f'Auto-loading {len(files_to_load)} project files for context...'):\n        for full_path, file_path_relative in files_to_load:\n            try:\n                with open(full_path, 'r', encoding='utf-8') as f:\n                    content = f.read().strip()\n                memory_manager.add_look_data(full_path, content)\n                loaded_count += 1\n            except Exception as e:\n                print(f\"[yellow]Skipping '{file_path_relative}': {e}[/yellow]\")\n    if loaded_count > 0:\n        ui_manager.show_success(\n            f'\u2705 Loaded {loaded_count} new file(s) into memory for full project context.'\n            )\n\n\ndef _create_prompt_for_element_selection(file_name: str, instruction: str,\n    elements: List[str], element_structures: Dict[str, Dict]) ->str:\n    \"\"\"\n    Create a helper for the first stage of the 'edit' command. This prompt asks the AI\n    to analyze the user's instruction and intelligently select the most relevant code element to modify.\n    \"\"\"\n    element_details = []\n    for elem in elements:\n        if elem in element_structures:\n            struct = element_structures[elem]\n            detail = (\n                f\"{elem} ({struct['type']}, lines {struct['line_start']}-{struct['line_end']})\"\n                )\n            element_details.append(detail)\n        else:\n            element_details.append(elem)\n    return f\"\"\"You are an expert code analyzer. Your task is to identify what should be modified based on the user's instruction.\n\nFile: {file_name}\nAvailable elements: {', '.join(element_details) if element_details else 'None'}\n\nUser instruction: {instruction}\n\nRESPONSE FORMAT:\nChoose one of these response types:\n1. \"ELEMENT: <element_name>\" - to edit an entire function/class\n2. \"PARTIAL: <element_name> LINES: <start>-<end>\" - to edit specific lines within an element\n3. \"FILE\" - to edit the entire file or multiple elements\n\nRULES:\n- If the instruction mentions specific line numbers or a specific part of a function, use PARTIAL\n- If the instruction targets an entire function/class, use ELEMENT\n- If the instruction requires changes to multiple elements or file structure, use FILE\n- For PARTIAL edits, provide absolute line numbers from the original file\n\nWhat should be edited?\"\"\"\n\n\ndef _create_prompt_for_element_rewrite(file_name: str, element_name: str,\n    instruction: str, original_code: str, is_full_file: bool=False) ->str:\n    \"\"\"\n    Create a helper for the second stage of the 'edit' command. This prompt instructs the AI\n    to rewrite a specific code element (or the whole file) based on the user's request,\n    demanding a complete and syntactically correct code block as output.\n    \"\"\"\n    if is_full_file:\n        return f\"\"\"You are an expert programmer. Rewrite the entire file to accomplish the user's task.\n\nIMPORTANT RULES:\n- Provide ONLY the complete, updated code for the entire file\n- Ensure all syntax is correct and the code is ready to run\n- Preserve existing functionality unless explicitly asked to change it\n- Include all necessary imports and maintain the file's structure\n- Do NOT include any explanations or comments outside the code\n\nFile: {file_name}\nTask: {instruction}\n\nCurrent file content:\n```python\n{original_code}\n```\n\nGenerate the complete updated file now:\"\"\"\n    else:\n        return f\"\"\"You are an expert programmer. Rewrite the specified element to accomplish the user's task.\n\nIMPORTANT RULES:\n- Provide ONLY the complete, updated code for the element\n- Include any necessary imports at the top of your code block\n- Ensure the code is syntactically correct and maintains the same interface\n- Do NOT include explanations or comments outside the code block\n- The code must be a drop-in replacement for the original element\n\nFile: {file_name}\nElement to modify: {element_name}\nTask: {instruction}\n\nCurrent element code:\n```python\n{original_code}\n```\n\nGenerate the complete updated element now:\"\"\"\n\n\ndef _create_prompt_for_partial_edit(file_name: str, element_name: str,\n    instruction: str, original_snippet: str, line_start: int, line_end: int,\n    full_element_code: str) ->str:\n    \"\"\"\n    Create a prompt for partial edits within a function or class.\n    This allows surgical changes to specific parts of code.\n    \"\"\"\n    return f\"\"\"You are an expert programmer. Make a surgical edit to a specific part of a function/class.\n\nCONTEXT:\n- File: {file_name}\n- Element: {element_name}\n- Lines to modify: {line_start}-{line_end}\n- Task: {instruction}\n\nIMPORTANT RULES:\n- Provide ONLY the code that will replace lines {line_start}-{line_end}\n- Your code must fit seamlessly into the existing function\n- Maintain proper indentation (the code will be auto-indented)\n- Do NOT include the function definition or other parts\n- Do NOT include explanations outside the code\n\nFull element for context:\n```python\n{full_element_code}\n```\n\nCode section to replace (lines {line_start}-{line_end}):\n```python\n{original_snippet}\n```\n\nGenerate ONLY the replacement code for the specified lines:\"\"\"\n\n\ndef handle_file_edit_command(file_path: str, instruction: str):\n    \"\"\"\n    Handles the entire workflow for editing a single file, ensuring full\n    project context is loaded before the AI makes any decisions.\n    Now supports partial edits within functions.\n    \"\"\"\n    global last_code\n    _load_all_project_files_if_needed()\n    resolved_path = resolve_file_path(file_path)\n    if not resolved_path:\n        ui_manager.show_error(f'File not found: {file_path}')\n        return\n    if resolved_path != os.path.abspath(file_path):\n        ui_manager.show_success(\n            f\"Found '{file_path}' in project. Using: {resolved_path}\")\n    try:\n        editor = CodeEditor(resolved_path)\n    except (ValueError, FileNotFoundError) as e:\n        ui_manager.show_error(str(e))\n        return\n    elements = editor.list_elements()\n    element_structures = {}\n    for elem in elements:\n        struct = editor.get_element_structure(elem)\n        if struct:\n            element_structures[elem] = struct\n    prompt1 = _create_prompt_for_element_selection(os.path.basename(\n        resolved_path), instruction, elements, element_structures)\n    with ui_manager.show_spinner('AI is analyzing file...'):\n        ai_response = query_llm(prompt1).strip()\n    if ai_response.upper() == 'FILE':\n        ui_manager.show_success('AI has chosen to edit the entire file.')\n        original_snippet = editor.source_code\n        prompt2 = _create_prompt_for_element_rewrite(os.path.basename(\n            resolved_path), 'entire file', instruction, original_snippet,\n            is_full_file=True)\n        edit_type = 'FILE'\n        element_to_edit = None\n        line_range = None\n    elif ai_response.startswith('PARTIAL:'):\n        parts = ai_response.split()\n        element_to_edit = parts[1]\n        if 'LINES:' in ai_response:\n            line_part = ai_response.split('LINES:')[1].strip()\n            if '-' in line_part:\n                line_start, line_end = map(int, line_part.split('-'))\n                line_range = line_start, line_end\n            else:\n                ui_manager.show_error('Invalid line range format')\n                return\n        else:\n            ui_manager.show_error('Missing line range for partial edit')\n            return\n        if element_to_edit not in elements:\n            ui_manager.show_error(f\"Element '{element_to_edit}' not found\")\n            return\n        ui_manager.show_success(\n            f\"AI selected partial edit of '{element_to_edit}' (lines {line_start}-{line_end})\"\n            )\n        original_snippet = editor.get_element_body_snippet(element_to_edit,\n            line_start, line_end)\n        if not original_snippet:\n            original_snippet = editor.get_source_of(element_to_edit)\n        full_element_code = editor.get_source_of(element_to_edit)\n        prompt2 = _create_prompt_for_partial_edit(os.path.basename(\n            resolved_path), element_to_edit, instruction, original_snippet,\n            line_start, line_end, full_element_code)\n        edit_type = 'PARTIAL'\n    elif ai_response.startswith('ELEMENT:'):\n        element_to_edit = ai_response.split(':', 1)[1].strip()\n        if element_to_edit not in elements:\n            ui_manager.show_error(\n                f\"AI identified '{element_to_edit}', which is not a valid element. Aborting.\"\n                )\n            return\n        ui_manager.show_success(f\"AI selected '{element_to_edit}' for editing.\"\n            )\n        original_snippet = editor.get_source_of(element_to_edit)\n        prompt2 = _create_prompt_for_element_rewrite(os.path.basename(\n            resolved_path), element_to_edit, instruction, original_snippet)\n        edit_type = 'ELEMENT'\n        line_range = None\n    else:\n        element_to_edit = ai_response.splitlines()[0]\n        if element_to_edit not in elements:\n            ui_manager.show_error(\n                f\"AI identified '{element_to_edit}', which is not a valid element. Aborting.\"\n                )\n            return\n        ui_manager.show_success(f\"AI selected '{element_to_edit}' for editing.\"\n            )\n        original_snippet = editor.get_source_of(element_to_edit)\n        prompt2 = _create_prompt_for_element_rewrite(os.path.basename(\n            resolved_path), element_to_edit, instruction, original_snippet)\n        edit_type = 'ELEMENT'\n        line_range = None\n    with ui_manager.show_spinner(f'AI is editing...'):\n        response = query_llm(prompt2)\n    code_blocks = extract_code(response)\n    if not code_blocks:\n        ui_manager.show_error('AI did not return a valid code block.')\n        print(Panel(response, title=\"[yellow]AI's Raw Response[/]\"))\n        return\n    new_code = code_blocks[0][1]\n    success = False\n    if edit_type == 'FILE':\n        try:\n            editor.tree = ast.parse(new_code)\n            success = True\n        except SyntaxError as e:\n            ui_manager.show_error(f'AI returned invalid Python syntax: {e}')\n            print(Panel(response, title=\"[yellow]AI's Raw Response[/]\"))\n            return\n    elif edit_type == 'PARTIAL':\n        success = editor.replace_partial(element_to_edit, new_code,\n            line_start=line_range[0], line_end=line_range[1])\n        if not success:\n            ui_manager.show_error('Failed to apply partial edit.')\n            print(Panel(response, title=\"[yellow]AI's Raw Response[/]\"))\n            return\n    else:\n        success = editor.replace_element(element_to_edit, new_code)\n        if not success:\n            ui_manager.show_error(\n                'AI returned invalid code; could not be parsed or applied.')\n            print(Panel(response, title=\"[yellow]AI's Raw Response[/]\"))\n            return\n    if not (diff := editor.get_diff()):\n        ui_manager.show_success('AI made no changes.')\n        return\n    print(Panel(diff, title=\n        f'[bold yellow]Proposed Changes for {resolved_path}[/]'))\n    if ui_manager.get_user_input('Apply changes? (y/n): ').lower() in ['yes',\n        'y']:\n        editor.save_changes()\n        last_code = editor.get_modified_source()\n        ui_manager.show_success(f'Changes saved to {resolved_path}.')\n    else:\n        ui_manager.show_error('Changes discarded.')\n\n\ndef _create_prompt_for_refactor_plan(instruction: str, memory_context: str\n    ) ->str:\n    \"\"\"\n    Create a specialized prompt-generation function for the 'refactor' command.\n    This prompt will explicitly define the required JSON structure for the plan\n    and instruct the AI to act as an expert project manager.\n    \"\"\"\n    return f\"\"\"You are an expert project manager and software architect. Analyze the project context and create a detailed refactoring plan.\n\nYour plan must be a valid JSON object with this exact structure:\n{{\n    \"actions\": [\n        {{\n            \"type\": \"MODIFY\" | \"CREATE\" | \"DELETE\" | \"PARTIAL\",\n            \"file\": \"relative/path/to/file.py\",\n            \"element\": \"function_or_class_name\",  // For MODIFY/DELETE/PARTIAL\n            \"element_name\": \"new_element_name\",    // For CREATE\n            \"line_start\": 10,                      // For PARTIAL only\n            \"line_end\": 20,                        // For PARTIAL only\n            \"reason\": \"Clear explanation of why this change is needed\",\n            \"description\": \"What this action will accomplish\",\n            \"anchor_element\": \"optional_anchor\",   // Optional for CREATE\n            \"position\": \"before\" | \"after\"         // Optional for CREATE\n        }}\n    ]\n}}\n\nACTION TYPES:\n- MODIFY: Change an entire function, class, or method\n- PARTIAL: Change specific lines within a function/class (requires line_start and line_end)\n- CREATE: Add new functions, classes, or files\n- DELETE: Remove functions, classes, variables, or imports\n\nRULES FOR YOUR PLAN:\n- Use PARTIAL when you only need to change a small part of a function\n- Use MODIFY when restructuring an entire function or class\n- Each action must have all required fields based on its type\n- File paths must be relative to the project root\n- Be specific and surgical - avoid unnecessary changes\n- Consider dependencies between changes\n- Order actions logically (e.g., create dependencies before using them)\n\n### Project Context ###\n{memory_context}\n\n### Refactoring Request ###\n{instruction}\n\nGenerate ONLY the JSON plan - no explanations or markdown:\"\"\"\n\n\ndef _get_refactor_plan(instruction: str) ->Optional[List[Dict]]:\n    \"\"\"\n    Generates a refactoring plan from the LLM.\n\n    This function encapsulates the logic for checking project context,\n    constructing a prompt, querying the LLM, and parsing the resulting\n    JSON plan for a refactoring task.\n\n    Args:\n        instruction: The user's high-level refactoring instruction.\n\n    Returns:\n        A list of action dictionaries if a valid plan is generated,\n        otherwise None.\n    \"\"\"\n    if not memory_manager.get_project_root():\n        ui_manager.show_error(\n            \"No project context in memory. Use 'look <directory>' first.\")\n        return None\n    memory_context = memory_manager.get_memory_context()\n    plan_prompt = _create_prompt_for_refactor_plan(instruction, memory_context)\n    with ui_manager.show_spinner('AI is creating an execution plan...'):\n        plan_str = query_llm(plan_prompt)\n    try:\n        match = re.search('\\\\{.*\\\\}', plan_str, re.DOTALL)\n        if not match:\n            raise ValueError('No JSON object found in the response.')\n        plan = json.loads(match.group(0))\n        actions = plan.get('actions', [])\n        if not actions:\n            raise ValueError(\"No 'actions' key found in plan or plan is empty.\"\n                )\n        return actions\n    except (json.JSONDecodeError, ValueError) as e:\n        ui_manager.show_error(f'AI failed to generate a valid plan: {e}')\n        print(Panel(plan_str, title=\"[yellow]AI's Invalid Plan Response[/]\",\n            border_style='yellow'))\n        return None\n\n\ndef _display_and_confirm_plan(plan: Dict) ->bool:\n    \"\"\"\n    Displays the generated execution plan to the user and asks for confirmation.\n\n    This helper function separates the UI interaction of plan confirmation from\n    the main refactoring logic.\n\n    Args:\n        plan: A dictionary, expected to contain an 'actions' key with a list of action dicts.\n\n    Returns:\n        True if the user confirms the plan, False otherwise.\n    \"\"\"\n    actions = plan.get('actions', [])\n    if not actions:\n        ui_manager.show_error('The generated plan is empty. Aborting.')\n        return False\n    ui_manager.show_success('AI has created a plan:')\n    for i, action in enumerate(actions):\n        action_type = action.get('type', 'N/A')\n        element = action.get('element') or action.get('element_name', 'N/A')\n        reason = action.get('reason') or action.get('description', '')\n        file_path = action.get('file', '')\n        if action_type == 'PARTIAL':\n            line_start = action.get('line_start', '?')\n            line_end = action.get('line_end', '?')\n            print(\n                f'  [cyan]{i + 1}. {action_type}:[/] {file_path}/{element} (lines {line_start}-{line_end}) - {reason}'\n                )\n        else:\n            print(\n                f'  [cyan]{i + 1}. {action_type}:[/] {file_path}/{element} - {reason}'\n                )\n    if ui_manager.get_user_input('\\nProceed with this plan? (y/n): ').lower(\n        ) in ['yes', 'y']:\n        return True\n    else:\n        ui_manager.show_error('Execution aborted by user.')\n        return False\n\n\ndef _apply_refactor_changes(editors: Dict[str, CodeEditor]) ->None:\n    \"\"\"\n    Consolidates changes from multiple CodeEditor instances, shows a unified\n    diff, and prompts the user to apply them.\n    \n    This helper function abstracts the final step of a refactor, ensuring\n    all proposed modifications are presented to the user for a final review\n    before any files are written to disk.\n\n    Args:\n        editors: A dictionary mapping absolute file paths to their\n                 corresponding CodeEditor instances which hold the\n                 proposed changes in their AST.\n    \"\"\"\n    full_diff = ''\n    for editor in editors.values():\n        diff = editor.get_diff()\n        if diff:\n            full_diff += diff + '\\n'\n    if not full_diff.strip():\n        ui_manager.show_success('AI made no changes.')\n        return\n    print(Panel(full_diff, title=\n        '[bold yellow]Proposed Project-Wide Changes[/]'))\n    if ui_manager.get_user_input('Apply all changes? (y/n): ').lower() in [\n        'yes', 'y']:\n        for editor in editors.values():\n            editor.save_changes()\n        ui_manager.show_success('\u2705 Project changes applied successfully.')\n    else:\n        ui_manager.show_error('Changes discarded.')\n\n\ndef _create_prompt_for_refactor_action(action_type: str, file_path: str,\n    action_details: Dict) ->str:\n    \"\"\"\n    Create a helper to generate prompts for individual 'CREATE' or 'MODIFY' steps\n    within a refactor plan. This ensures the AI produces code for the specific\n    sub-task in the correct context.\n    \"\"\"\n    if action_type == 'MODIFY':\n        element_name = action_details['element_name']\n        reason = action_details['reason']\n        original_code = action_details['original_code']\n        return f\"\"\"You are implementing a specific refactoring task as part of a larger plan.\n\nREFACTORING CONTEXT:\n- File: {file_path}\n- Element: {element_name}\n- Reason for change: {reason}\n\nRULES:\n- Provide ONLY the complete updated code for the element\n- Include any necessary imports at the top\n- Ensure the code integrates properly with the rest of the file\n- Maintain the same function/class signature unless the change requires otherwise\n- No explanations outside the code block\n\nCurrent element code:\n```python\n{original_code}\n```\n\nGenerate the updated element code:\"\"\"\n    elif action_type == 'CREATE':\n        element_name = action_details['element_name']\n        description = action_details['description']\n        return f\"\"\"You are implementing a specific refactoring task as part of a larger plan.\n\nREFACTORING CONTEXT:\n- File: {file_path}\n- New element to create: {element_name}\n- Purpose: {description}\n\nRULES:\n- Provide ONLY the complete code for the new element\n- Include all necessary imports at the top\n- Follow the coding style and patterns used in the project\n- Ensure the code is production-ready and well-structured\n- For non-Python files, provide the complete file content\n- No explanations outside the code block\n\nGenerate the new element code:\"\"\"\n\n\ndef _process_refactor_action(action: Dict, project_base_path: str, editors:\n    Dict) ->bool:\n    \"\"\"\n    Processes a single refactoring action from the plan.\n\n    This function handles the execution of a single action from the refactoring plan,\n    including LLM code generation and applying changes to in-memory editors or files.\n\n    Args:\n        action: A dictionary containing action details (type, file, element, etc.)\n        project_base_path: The absolute path to the project root\n        editors: Dictionary mapping file paths to their CodeEditor instances\n\n    Returns:\n        True if the action was processed successfully, False otherwise\n    \"\"\"\n    file_path_relative = action.get('file')\n    if not file_path_relative:\n        ui_manager.show_error(\n            f\"Action is missing 'file' key. Skipping: {action}\")\n        return False\n    file_path_absolute = os.path.join(project_base_path, file_path_relative)\n    action_type = action.get('type', '').upper()\n    prompt, element_name = '', ''\n    if action_type == 'MODIFY':\n        element_name = action.get('element')\n        reason = action.get('reason')\n        if file_path_relative.endswith('.py'):\n            if file_path_absolute not in editors:\n                try:\n                    editors[file_path_absolute] = CodeEditor(file_path_absolute\n                        )\n                except Exception as e:\n                    ui_manager.show_error(\n                        f'Error loading file {file_path_absolute}: {e}')\n                    return False\n            editor = editors[file_path_absolute]\n            original_snippet = editor.get_source_of(element_name)\n            if not original_snippet:\n                ui_manager.show_error(\n                    f\"Element '{element_name}' in '{file_path_relative}' not found. Skipping.\"\n                    )\n                return False\n            action_details = {'element_name': element_name, 'reason':\n                reason, 'original_code': original_snippet}\n            prompt = _create_prompt_for_refactor_action('MODIFY',\n                file_path_relative, action_details)\n    elif action_type == 'PARTIAL':\n        element_name = action.get('element')\n        reason = action.get('reason')\n        line_start = action.get('line_start')\n        line_end = action.get('line_end')\n        if not all([element_name, line_start, line_end]):\n            ui_manager.show_error(\n                f'PARTIAL action missing required fields. Skipping: {action}')\n            return False\n        if file_path_relative.endswith('.py'):\n            if file_path_absolute not in editors:\n                try:\n                    editors[file_path_absolute] = CodeEditor(file_path_absolute\n                        )\n                except Exception as e:\n                    ui_manager.show_error(\n                        f'Error loading file {file_path_absolute}: {e}')\n                    return False\n            editor = editors[file_path_absolute]\n            original_snippet = editor.get_element_body_snippet(element_name,\n                line_start, line_end)\n            if not original_snippet:\n                original_snippet = editor.get_source_of(element_name)\n                if not original_snippet:\n                    ui_manager.show_error(\n                        f\"Element '{element_name}' in '{file_path_relative}' not found. Skipping.\"\n                        )\n                    return False\n            full_element_code = editor.get_source_of(element_name)\n            prompt = _create_prompt_for_partial_edit(file_path_relative,\n                element_name, reason, original_snippet, line_start,\n                line_end, full_element_code)\n    elif action_type == 'CREATE':\n        element_name = action.get('element_name')\n        description = action.get('description')\n        action_details = {'element_name': element_name, 'description':\n            description}\n        prompt = _create_prompt_for_refactor_action('CREATE',\n            file_path_relative, action_details)\n    else:\n        ui_manager.show_error(f\"Invalid action type '{action_type}'. Skipping.\"\n            )\n        return False\n    with ui_manager.show_spinner(\n        f\"AI: {action_type} on '{element_name or file_path_relative}'...\"):\n        response = query_llm(prompt)\n    code_blocks = extract_code(response)\n    new_content = code_blocks[0][1] if code_blocks else response.strip()\n    if not new_content:\n        ui_manager.show_error(\n            f'AI failed to generate content for action: {action}')\n        print(Panel(response, title=\"[yellow]AI's Raw Response[/]\"))\n        return False\n    if not file_path_relative.endswith('.py'):\n        try:\n            FileCreator.create(file_path_absolute, new_content)\n            ui_manager.show_success(\n                f\"File '{file_path_relative}' created/updated.\")\n            return True\n        except IOError as e:\n            ui_manager.show_error(\n                f\"Failed to create file '{file_path_relative}': {e}\")\n            return False\n    if file_path_absolute not in editors:\n        try:\n            if not os.path.exists(file_path_absolute):\n                os.makedirs(os.path.dirname(file_path_absolute), exist_ok=True)\n                with open(file_path_absolute, 'w') as f:\n                    f.write('')\n            editors[file_path_absolute] = CodeEditor(file_path_absolute)\n        except Exception as e:\n            ui_manager.show_error(\n                f'Error loading file {file_path_absolute}: {e}')\n            return False\n    editor = editors[file_path_absolute]\n    if action_type == 'MODIFY':\n        if not editor.replace_element(element_name, new_content):\n            ui_manager.show_error(\n                f\"Failed to apply MODIFY change to '{element_name}'.\")\n            print(Panel(new_content, title=\n                f\"[red]Problematic MODIFY Code for '{element_name}'[/]\",\n                border_style='red'))\n            return False\n    elif action_type == 'PARTIAL':\n        if not editor.replace_partial(element_name, new_content, line_start,\n            line_end):\n            ui_manager.show_error(\n                f\"Failed to apply PARTIAL change to '{element_name}'.\")\n            print(Panel(new_content, title=\n                f\"[red]Problematic PARTIAL Code for '{element_name}'[/]\",\n                border_style='red'))\n            return False\n    elif action_type == 'CREATE':\n        anchor = action.get('anchor_element')\n        position = action.get('position', 'after')\n        if not editor.add_element(new_content, anchor_name=anchor, before=\n            position == 'before'):\n            ui_manager.show_error(\n                f\"Failed to apply CREATE change for '{element_name}'.\")\n            print(Panel(new_content, title=\n                f\"[red]Problematic CREATE Code for '{element_name}'[/]\",\n                border_style='red'))\n            return False\n    return True\n\n\ndef handle_project_refactor_command(instruction: str):\n    \"\"\"\n    Orchestrates a multi-file, multi-step code refactoring process.\n    \n    This function serves as a high-level orchestrator that delegates specific tasks\n    to helper functions, improving readability, modularity, and maintainability.\n    \"\"\"\n    _load_all_project_files_if_needed()\n    actions = _get_refactor_plan(instruction)\n    if not actions:\n        return\n    plan = {'actions': actions}\n    if not _display_and_confirm_plan(plan):\n        return\n    editors: Dict[str, CodeEditor] = {}\n    project_base_path = memory_manager.get_project_root()\n    successful_actions = 0\n    total_actions = len(actions)\n    for i, action in enumerate(actions, 1):\n        ui_manager.show_success(f'Processing action {i}/{total_actions}...')\n        action_type = action.get('type', '').upper()\n        file_path_relative = action.get('file')\n        if not file_path_relative:\n            ui_manager.show_error(\n                f\"Action is missing 'file' key. Skipping: {action}\")\n            continue\n        file_path_absolute = os.path.join(project_base_path, file_path_relative\n            )\n        if action_type == 'DELETE':\n            element_name = action.get('element')\n            if not element_name:\n                ui_manager.show_error(\n                    f\"DELETE action missing 'element' key. Skipping: {action}\")\n                continue\n            if not file_path_relative.endswith('.py'):\n                ui_manager.show_error(\n                    f'DELETE actions are only supported for Python files. Skipping.'\n                    )\n                continue\n            if file_path_absolute not in editors:\n                try:\n                    editors[file_path_absolute] = CodeEditor(file_path_absolute\n                        )\n                except Exception as e:\n                    ui_manager.show_error(\n                        f'Error loading file {file_path_absolute}: {e}')\n                    continue\n            editor = editors[file_path_absolute]\n            if editor.delete_element(element_name):\n                successful_actions += 1\n                ui_manager.show_success(\n                    f\"Successfully deleted '{element_name}' from '{file_path_relative}'.\"\n                    )\n            else:\n                ui_manager.show_error(\n                    f\"Failed to delete '{element_name}' from '{file_path_relative}'.\"\n                    )\n        elif _process_refactor_action(action, project_base_path, editors):\n            successful_actions += 1\n        else:\n            ui_manager.show_error(\n                f'Action {i} failed, continuing with remaining actions...')\n    if successful_actions == 0:\n        ui_manager.show_error('No actions were successfully executed.')\n        return\n    elif successful_actions < total_actions:\n        ui_manager.show_error(\n            f'Only {successful_actions}/{total_actions} actions completed successfully.'\n            )\n    else:\n        ui_manager.show_success(\n            f'All {total_actions} actions completed successfully.')\n    _apply_refactor_changes(editors)\n\n\ndef _create_prompt_for_commit_message(diff: str) ->str:\n    \"\"\"\n    Create a dedicated prompt function for the 'commit' command. This prompt will\n    instruct the AI to analyze a git diff and generate a concise commit message\n    following the Conventional Commits standard.\n    \"\"\"\n    return f\"\"\"You are an expert developer writing a Git commit message. Your task is to analyze the provided git diff and create a professional commit message.\n\nCOMMIT MESSAGE RULES:\n- Follow the Conventional Commits specification\n- Format: <type>(<optional scope>): <subject>\n- Types: feat, fix, docs, style, refactor, perf, test, build, ci, chore, revert\n- Subject line: max 50 characters, imperative mood, no period\n- Optional body: explain what and why (not how), wrap at 72 characters\n- Be specific and concise\n- Focus on the intent and impact of the changes\n\nEXAMPLES OF GOOD COMMIT MESSAGES:\n- feat(auth): add OAuth2 integration for Google login\n- fix(api): handle null response in user endpoint\n- refactor(database): optimize query performance for large datasets\n- docs(readme): update installation instructions for Windows\n\nRespond with ONLY the commit message - no markdown, quotes, or explanations.\n\n--- GIT DIFF TO ANALYZE ---\n{diff}\n\nGenerate the commit message:\"\"\"\n\n\ndef handle_commit_command():\n    \"\"\"\n    Orchestrates an AI-assisted Git commit workflow with improved error handling.\n    \"\"\"\n    project_root = memory_manager.get_project_root()\n    if not project_root:\n        ui_manager.show_error(\n            \"No project context in memory. Use 'look <directory>' first.\")\n        return\n    try:\n        git_manager = GitManager(project_root)\n    except ValueError as e:\n        ui_manager.show_error(str(e))\n        return\n    changed_files = git_manager.get_changed_files()\n    if not changed_files:\n        ui_manager.show_success(\n            'No changes to commit. Everything is up to date.')\n        return\n    staged_diff = git_manager.get_diff(staged=True)\n    unstaged_diff = git_manager.get_diff()\n    full_diff = f'{staged_diff}\\n{unstaged_diff}'.strip()\n    if not full_diff.strip():\n        ui_manager.show_success(\n            'No content changes detected (e.g., only file mode changes).')\n        return\n    prompt = _create_prompt_for_commit_message(full_diff)\n    commit_message = query_llm(prompt).strip()\n    if not commit_message:\n        ui_manager.show_error(\n            'AI failed to generate a commit message. Aborting.')\n        return\n    files_to_commit_str = '\\n'.join(f'- {f}' for f in changed_files)\n    plan_panel_content = f\"\"\"[bold]Files to be staged:[/]\n[yellow]{files_to_commit_str}[/]\n\n[bold]AI-Generated Commit Message:[/]\n[green]{commit_message}[/]\"\"\"\n    print(Panel(plan_panel_content, title='[bold cyan]Commit Plan[/]',\n        border_style='cyan'))\n    if ui_manager.get_user_input('\\nProceed with commit? (y/n): ').lower() in [\n        'yes', 'y']:\n        try:\n            git_manager.add(changed_files)\n            ui_manager.show_success('\u2705 Files staged.')\n        except subprocess.CalledProcessError as e:\n            ui_manager.show_error(f'Staging failed: {e.stderr}')\n            return\n        try:\n            git_manager.commit(commit_message)\n            ui_manager.show_success('\u2705 Commit successful.')\n        except subprocess.CalledProcessError as e:\n            ui_manager.show_error(f'Commit failed: {e.stderr}')\n            return\n        if ui_manager.get_user_input('Push changes to remote? (y/n): ').lower(\n            ) in ['yes', 'y']:\n            try:\n                git_manager.push()\n                ui_manager.show_success('\u2705 Push successful.')\n            except subprocess.CalledProcessError as e:\n                ui_manager.show_error(f'Push failed: {e.stderr}')\n        else:\n            ui_manager.show_error('Push cancelled.')\n    else:\n        ui_manager.show_error('Commit aborted by user.')\n\n\ndef handle_rag_query_command(query: str):\n    \"\"\"\n    Handles RAG query commands in the CLI.\n    \n    This function provides a way to query the RAG system from the command line interface.\n    It loads the RAG manager, performs the query, and displays the results.\n    \n    Args:\n        query: The query string to search for in the RAG system.\n    \"\"\"\n    try:\n        rag_manager = RAGManager()\n        if rag_manager.get_document_count() == 0:\n            ui_manager.show_error('RAG index is empty. Add documents first.')\n            return\n        results = rag_manager.search(query, k=3)\n        if not results:\n            ui_manager.show_error('No relevant documents found.')\n            return\n        print(Panel(f'[bold cyan]RAG Query:[/bold cyan] {query}', title=\n            '[bold]Retrieval-Augmented Generation Results[/bold]',\n            border_style='cyan'))\n        for i, (doc, score, metadata) in enumerate(results, 1):\n            file_info = metadata.get('file', 'Unknown source')\n            content_preview = doc[:200] + '...' if len(doc) > 200 else doc\n            result_panel = Panel(\n                f\"\"\"[dim]Source:[/] {file_info}\n[dim]Relevance:[/] {score:.4f}\n\n{content_preview}\"\"\"\n                , title=f'[bold]Result {i}[/bold]', border_style='blue',\n                expand=False)\n            print(result_panel)\n        if ui_manager.get_user_input(\n            '\\nGenerate detailed response with AI? (y/n): ').lower() in ['yes',\n            'y']:\n            context = '\\n\\n'.join([\n                f'Document {i} (Score: {score:.4f}):\\n{doc}' for i, (doc,\n                score, _) in enumerate(results, 1)])\n            prompt = f\"\"\"Based on the following retrieved documents, please answer the query: \"{query}\"\n\nRetrieved Documents:\n{context}\n\nPlease provide a comprehensive answer based only on the information in the documents above.\nIf the documents don't contain enough information to answer the query, please say so.\"\"\"\n            with ui_manager.show_spinner('AI is generating response...'):\n                response = query_llm(prompt)\n            print(Panel(response, title=\n                '[bold green]AI-Generated Response[/bold green]',\n                border_style='green'))\n    except Exception as e:\n        ui_manager.show_error(f'Error processing RAG query: {e}')\n        if os.getenv('OMNIFORGE_DEBUG'):\n            import traceback\n            traceback.print_exc()\n\n\ndef handle_rag_query_command(query: str):\n    \"\"\"\n    Handles RAG query commands in the CLI.\n    \n    This function provides a way to query the RAG system from the command line interface.\n    It loads the RAG manager, performs the query, and displays the results.\n    \n    Args:\n        query: The query string to search for in the RAG system.\n    \"\"\"\n    try:\n        rag_manager = RAGManager()\n        if rag_manager.get_document_count() == 0:\n            ui_manager.show_error('RAG index is empty. Add documents first.')\n            return\n        results = rag_manager.search(query, k=3)\n        if not results:\n            ui_manager.show_error('No relevant documents found.')\n            return\n        print(Panel(f'[bold cyan]RAG Query:[/bold cyan] {query}', title=\n            '[bold]Retrieval-Augmented Generation Results[/bold]',\n            border_style='cyan'))\n        for i, (doc, score, metadata) in enumerate(results, 1):\n            file_info = metadata.get('file', 'Unknown source')\n            content_preview = doc[:200] + '...' if len(doc) > 200 else doc\n            result_panel = Panel(\n                f\"\"\"[dim]Source:[/] {file_info}\n[dim]Relevance:[/] {score:.4f}\n\n{content_preview}\"\"\"\n                , title=f'[bold]Result {i}[/bold]', border_style='blue',\n                expand=False)\n            print(result_panel)\n        if ui_manager.get_user_input(\n            '\\nGenerate detailed response with AI? (y/n): ').lower() in ['yes',\n            'y']:\n            context = '\\n\\n'.join([\n                f'Document {i} (Score: {score:.4f}):\\n{doc}' for i, (doc,\n                score, _) in enumerate(results, 1)])\n            prompt = f\"\"\"Based on the following retrieved documents, please answer the query: \"{query}\"\n\nRetrieved Documents:\n{context}\n\nPlease provide a comprehensive answer based only on the information in the documents above.\nIf the documents don't contain enough information to answer the query, please say so.\"\"\"\n            with ui_manager.show_spinner('AI is generating response...'):\n                response = query_llm(prompt)\n            print(Panel(response, title=\n                '[bold green]AI-Generated Response[/bold green]',\n                border_style='green'))\n    except Exception as e:\n        ui_manager.show_error(f'Error processing RAG query: {e}')\n        if os.getenv('OMNIFORGE_DEBUG'):\n            import traceback\n            traceback.print_exc()\n\n\ndef interactive_mode() ->None:\n    global last_query, last_response, last_code\n    try:\n        from Testing.overlay_engine import show_sequential_popup\n        gui_available = True\n    except ImportError:\n        gui_available = False\n    print(Panel(\n        \"\"\"[bold cyan]Omni Interactive Mode[/]\n[dim]Type 'help' for commands, 'exit' to quit.[/dim]\"\"\"\n        , border_style='cyan'))\n    personality_name = personality_manager.get_current_personality().get('name'\n        , 'Default')\n    gui_enabled = False\n    if gui_available:\n        try:\n            with open(CONFIG_FILE, 'r') as f:\n                config = json.load(f)\n                if config.get('gui_enabled', False):\n                    gui_enabled = True\n        except (FileNotFoundError, json.JSONDecodeError):\n            pass\n    refresh_status_panel(personality_name)\n    while True:\n        try:\n            user_input = ui_manager.get_user_input('\\n> ')\n            if not user_input:\n                continue\n            command, *args = user_input.split(maxsplit=1)\n            arg_str = args[0] if args else ''\n            if command == 'exit':\n                memory_manager.save_memory()\n                print('[bold cyan]Goodbye![/]')\n                break\n            elif command == 'help':\n                print(\n                    \"\"\"[bold]Commands:[/]\n\n  [bold cyan]Core & Project Commands[/]\n  [yellow]send <prompt>[/]        - Ask the LLM a question.\n  [yellow]look <path>[/]          - Read file or scan directory into memory.\n  [yellow]look_all[/]            - Recursively scan the project directory into memory.\n  [yellow]create <file> \"instr\"[/] - Create a new file using AI.\n  [yellow]edit <file> \"instr\"[/]   - Edit a specific file using AI.\n  [yellow]refactor \"instr\"[/]      - Refactor project in memory based on instruction.\n  [yellow]commit[/]               - Commit changes with an AI-generated message.\n  [yellow]rag <query>[/]         - Query the RAG system for context retrieval.\n\n  [bold cyan]File & Code Management[/]\n  [yellow]save <filename>[/]     - Save last AI response to a file.\n  [yellow]list[/]               - List saved files.\n  [yellow]run[/]                 - Run the last generated Python code.\n\n  [bold cyan]Session & Config[/]\n  [yellow]history[/]             - Show the full chat history.\n  [yellow]memory clear[/]        - Clear the chat and file memory.\n  [yellow]backend <name>[/]      - Switch AI backend (e.g., openrouter, ollama).\n  [yellow]models [src][/]        - Interactively list and select models.\n  [yellow]set model <id>[/]      - Set the model directly by its ID.\n  [yellow]personality <cmd>[/]   - Manage AI personalities ('list', 'set', 'add').\n\"\"\"\n                    )\n            elif command == 'send':\n                last_query = arg_str\n                response = query_llm(arg_str)\n                last_response = response\n                if gui_enabled:\n                    threading.Thread(target=show_sequential_popup, args=(\n                        100, 100, response, f'Omni - {personality_name}'),\n                        daemon=True).start()\n                print(Panel(response, title='[cyan]Response[/]'))\n                if (code_blocks := extract_code(response)):\n                    last_code = code_blocks[0][1]\n            elif command == 'look':\n                look_command(arg_str)\n            elif command == 'look_all':\n                look_all_command()\n            elif command == 'create':\n                try:\n                    file_path, instruction = arg_str.split(' ', 1)\n                    handle_file_create_command(file_path.strip('\"'),\n                        instruction.strip('\"'))\n                except (ValueError, IndexError):\n                    ui_manager.show_error(\n                        'Usage: create <file_path> \"<instruction>\"')\n            elif command == 'edit':\n                try:\n                    file_path, instruction = arg_str.split(' ', 1)\n                    handle_file_edit_command(file_path.strip('\"'),\n                        instruction.strip('\"'))\n                except (ValueError, IndexError):\n                    ui_manager.show_error(\n                        'Usage: edit <file_path> \"<instruction>\"')\n            elif command == 'refactor':\n                if not arg_str:\n                    ui_manager.show_error('Usage: refactor \"<instruction>\"')\n                else:\n                    handle_project_refactor_command(arg_str.strip('\"'))\n            elif command == 'commit':\n                handle_commit_command()\n            elif command == 'models':\n                list_models(arg_str.split())\n            elif command == 'set' and arg_str.startswith('model '):\n                set_model(arg_str[6:])\n            elif command == 'backend':\n                switch_backend(arg_str)\n            elif command == 'history':\n                ui_manager.display_history(memory_manager.get_memory_context())\n            elif command == 'memory' and arg_str == 'clear':\n                memory_manager.clear_memory()\n                ui_manager.show_success('\u2705 Memory cleared')\n            elif command == 'personality':\n                p_args = arg_str.split(maxsplit=1)\n                cmd = p_args[0] if p_args else ''\n                p_arg_str = p_args[1] if len(p_args) > 1 else ''\n                if cmd == 'list':\n                    for p in personality_manager.list_personalities():\n                        print(f\"- {p['name']}: {p['description']}\")\n                elif cmd == 'set' and p_arg_str:\n                    if personality_manager.set_current_personality(p_arg_str):\n                        personality_name = p_arg_str\n                        ui_manager.show_success(\n                            f'Set personality to {personality_name}')\n                    else:\n                        ui_manager.show_error('Personality not found.')\n                else:\n                    ui_manager.show_error(\n                        \"Invalid personality command. Use 'list' or 'set <name>'.\"\n                        )\n            elif command == 'run':\n                run_python_code()\n            elif command == 'save':\n                if last_response:\n                    save_code(last_response, arg_str or\n                        f\"omni_save_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n                        )\n                else:\n                    ui_manager.show_error('No response to save.')\n            elif command == 'list':\n                files = sorted(os.listdir(DEFAULT_SAVE_DIR))\n                print('\\n'.join(f'  - {f}' for f in files) if files else\n                    '[yellow]No saved files.[/]')\n            elif command == 'rag':\n                if not arg_str:\n                    ui_manager.show_error('Usage: rag \"<query>\"')\n                else:\n                    from rag_manager import RAGManager\n                    project_root = memory_manager.get_project_root()\n                    if not project_root:\n                        ui_manager.show_error(\n                            \"No project context in memory. Use 'look <directory>' first.\"\n                            )\n                        continue\n                    rag = RAGManager()\n                    if rag.get_document_count() == 0:\n                        ui_manager.show_error(\n                            'RAG index is empty. Please add documents first.')\n                        continue\n                    results = rag.search(arg_str, k=3)\n                    if not results:\n                        ui_manager.show_error('No relevant documents found.')\n                        continue\n                    print(Panel('[bold]RAG Results:[/]', title=\n                        '[cyan]Retrieval-Augmented Generation[/]'))\n                    for i, (content, score, metadata) in enumerate(results, 1):\n                        file_path = metadata.get('file', 'Unknown')\n                        print(\n                            f'[bold cyan]{i}. {file_path}[/] (Score: {score:.4f})'\n                            )\n                        print(Panel(content[:500] + '...' if len(content) >\n                            500 else content, border_style='dim'))\n                    follow_up = ui_manager.get_user_input(\n                        \"\"\"\nWould you like to ask a follow-up question with this context? (y/n): \"\"\"\n                        )\n                    if follow_up.lower() in ['y', 'yes']:\n                        follow_up_query = ui_manager.get_user_input(\n                            'Follow-up query: ')\n                        if follow_up_query:\n                            context_parts = [\n                                f'Document {i} (Score: {score:.4f}):\\n{content}'\n                                 for i, (content, score, _) in enumerate(\n                                results, 1)]\n                            context = '\\n\\n'.join(context_parts)\n                            rag_prompt = f\"\"\"Based on the following context, please answer the question.\n\nContext:\n{context}\n\nQuestion: {follow_up_query}\"\"\"\n                            response = query_llm(rag_prompt)\n                            print(Panel(response, title=\n                                '[cyan]RAG-Augmented Response[/]'))\n            else:\n                ui_manager.show_error(\"Unknown command. Type 'help'.\")\n            refresh_status_panel(personality_name)\n        except KeyboardInterrupt:\n            memory_manager.save_memory()\n            print('\\n[bold cyan]Goodbye![/]')\n            break\n        except Exception as e:\n            ui_manager.show_error(f'An unexpected error occurred: {e}')\n\n\ndef run_python_code() ->None:\n    global last_code\n    if not last_code:\n        ui_manager.show_error('No Python code in memory to run.')\n        return\n    temp_file = os.path.join(DEFAULT_SAVE_DIR, 'temp_run.py')\n    try:\n        with open(temp_file, 'w') as f:\n            f.write(last_code)\n        print('[bold cyan]\\n--- Running Code ---\\n[/]')\n        subprocess.run([sys.executable, temp_file], check=True)\n        print('[bold cyan]\\n--- Code Finished ---\\n[/]')\n    except Exception as e:\n        ui_manager.show_error(f'Error running code: {e}')\n    finally:\n        if os.path.exists(temp_file):\n            os.remove(temp_file)\n\n\ndef save_code(content: str, filename: str) ->None:\n    filepath = os.path.join(DEFAULT_SAVE_DIR, filename)\n    try:\n        with open(filepath, 'w') as f:\n            f.write(content)\n        ui_manager.show_success(f'Saved to: {filepath}')\n    except IOError as e:\n        ui_manager.show_error(f'Error saving file: {e}')\n\n\ndef main() ->None:\n    try:\n        import astor\n    except ImportError:\n        print(\"[bold red]Error:[/] 'astor' is required. `pip install astor`\")\n        sys.exit(1)\n    try:\n        import simple_term_menu\n    except ImportError:\n        print(\n            \"[bold red]Error:[/] 'simple-term-menu' is required. `pip install simple-term-menu`\"\n            )\n        sys.exit(1)\n    parser = argparse.ArgumentParser(description=\n        'Omni - AI-powered code tool', add_help=False)\n    parser.add_argument('command', nargs='?', help='Main command.')\n    parser.add_argument('args', nargs='*', help='Arguments for the command.')\n    parser.add_argument('-h', '--help', action='store_true')\n    args, _ = parser.parse_known_args()\n    if args.help or not args.command:\n        interactive_mode()\n    elif args.command == 'look' and args.args:\n        look_command(args.args[0])\n    elif args.command == 'edit' and len(args.args) >= 2:\n        handle_file_edit_command(args.args[0], ' '.join(args.args[1:]))\n    elif args.command == 'models':\n        list_models(args.args)\n    else:\n        interactive_mode()\n\n\ndef refresh_status_panel(personality_name: str) ->None:\n    ui_manager.display_status_panel(personality_name, current_backend,\n        current_model, len(memory_manager.memory.get('chat', [])), len(\n        memory_manager.memory.get('look', [])))\n\n\nif __name__ == '__main__':\n    main()\n",
    "file": "/mnt/ProjectData/omni/omni.py"
  },
  {
    "id": 9,
    "hash": "392cb7cce7185587da5fc1b6a30c1fef",
    "content": "import json\nimport os\nfrom typing import List, Dict, Optional\nfrom rag_manager import RAGManager\n\n\nclass MemoryManager:\n    \"\"\"Manages persistent chat memory, look data, and RAG integration via JSON.\"\"\"\n\n    def __init__(self, memory_file: str):\n        self.memory_file = memory_file\n        self.memory: Dict[str, List] = self.load_memory()\n        self.rag_manager = RAGManager()\n\n    def load_memory(self) ->Dict[str, List]:\n        try:\n            with open(self.memory_file, 'r') as f:\n                return json.load(f)\n        except FileNotFoundError:\n            default = {'chat': [], 'look': []}\n            self.save_memory(default)\n            return default\n        except json.JSONDecodeError:\n            print('[yellow]Invalid memory file. Resetting.[/]')\n            return {'chat': [], 'look': []}\n\n    def save_memory(self, memory: Optional[Dict[str, List]]=None) ->None:\n        if memory is None:\n            memory = self.memory\n        with open(self.memory_file, 'w') as f:\n            json.dump(memory, f, indent=4)\n\n    def add_message(self, role: str, content: str) ->None:\n        self.memory['chat'].append({'role': role, 'content': content})\n        self.save_memory()\n\n    def add_look_data(self, file_path: str, content: str) ->None:\n        \"\"\"\n    Adds a watched item (directory or file) to memory, distinguishing its type.\n\n    This method stores structured data that differentiates between a project\n    directory (containing a manifest) and a single file (containing its content).\n    It also prevents duplicate entries by updating existing ones.\n\n    Args:\n        file_path: The path to the directory or file.\n        content: The manifest for a directory or the content for a file.\n    \"\"\"\n        item_type = 'directory' if os.path.isdir(file_path) else 'file'\n        for item in self.memory['look']:\n            if item.get('file') == file_path:\n                item['content'] = content\n                item['type'] = item_type\n                self.save_memory()\n                return\n        self.memory['look'].append({'type': item_type, 'file': file_path,\n            'content': content})\n        self.save_memory()\n        if item_type == 'file':\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    file_content = f.read()\n                self.rag_manager.add_documents([file_content], [{'file':\n                    file_path}])\n            except Exception as e:\n                print(\n                    f'[yellow]Warning: Could not add {file_path} to RAG index: {e}[/]'\n                    )\n\n    def get_memory_context(self) -> str:\n        \"\"\"\n        Dynamically builds the context using RAG and chat history.\n\n        It retrieves relevant file content from the RAG index based on the latest\n        user query, includes project manifests for directories, and appends the\n        recent chat history.\n        \"\"\"\n        context = ''\n        # 1. Add project manifests for any watched directories\n        for look in self.memory.get('look', []):\n            path = look.get('file')\n            if path and os.path.isdir(path):\n                content = look.get('content', '')\n                context += f'--- Project Manifest for {path} ---\\n{content}\\n\\n'\n\n        # 2. Find the last user message to use as a query for the RAG system\n        last_user_message = next((msg['content'] for msg in reversed(self.memory.get('chat', [])) if msg['role'] == 'user'), None)\n\n        # 3. If a user message exists, search the RAG index for relevant context\n        if last_user_message:\n            rag_results = self.search_rag(last_user_message, k=3)\n            if rag_results:\n                context += '--- Relevant context from RAG ---\\n'\n                # Format and add each RAG result to the context\n                for doc, score, meta in rag_results:\n                    file_path = meta.get('file', 'Unknown source')\n                    context += f'Source: {file_path} (Score: {score:.4f})\\n'\n                    context += f'Content: {doc}\\n---\\n'\n                context += '\\n'\n\n        # 4. Append the full chat history for conversational context\n        for msg in self.memory.get('chat', []):\n            context += f\"{msg['role'].capitalize()}: {msg['content']}\\n\"\n\n        return context.strip()\n\n    def clear_memory(self) ->None:\n        self.memory = {'chat': [], 'look': []}\n        self.save_memory()\n        self.rag_manager.clear_index()\n\n    def search_rag(self, query: str, k: int=3) ->List[tuple]:\n        \"\"\"\n        Search the RAG index for relevant documents.\n        \n        Args:\n            query: The search query\n            k: Number of results to return\n            \n        Returns:\n            List of (document_content, score, metadata) tuples\n        \"\"\"\n        return self.rag_manager.search(query, k)\n",
    "file": "/mnt/ProjectData/omni/memory_manager.py"
  },
  {
    "id": 10,
    "hash": "2914352eefed9d60de098e021dd79b67",
    "content": "import os\nimport sys\nimport argparse\nfrom typing import List\nfrom rag_manager import RAGManager\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__),\n    '..')))\n\n\ndef create_sample_data() ->List[str]:\n    \"\"\"Create sample documents for the RAG example.\"\"\"\n    return [\n        'RAG stands for Retrieval-Augmented Generation, a technique that combines information retrieval with language generation.'\n        ,\n        'The RAG model retrieves relevant documents from a knowledge base and uses them to generate more accurate responses.'\n        ,\n        'FAISS is a library for efficient similarity search and clustering of dense vectors, often used in RAG systems.'\n        ,\n        'Sentence transformers are used to create embeddings for documents and queries in RAG systems.'\n        ,\n        'Retrieval-Augmented Generation improves language models by allowing them to access external knowledge sources.'\n        ,\n        'In RAG systems, documents are indexed and searchable by their semantic embeddings rather than just keywords.'\n        ,\n        'Python is a great language for implementing RAG systems due to libraries like sentence-transformers and faiss.'\n        ,\n        'The retrieval component of RAG is crucial for finding relevant information before generation.'\n        ]\n\n\ndef main():\n    \"\"\"Main function demonstrating RAG through CLI.\"\"\"\n    parser = argparse.ArgumentParser(description='RAG CLI Example')\n    parser.add_argument('--query', '-q', type=str, help='Query to search for')\n    parser.add_argument('--add', '-a', type=str, help=\n        'Add a new document to the index')\n    parser.add_argument('--list', '-l', action='store_true', help=\n        'List all documents in the index')\n    parser.add_argument('--clear', '-c', action='store_true', help=\n        'Clear the index')\n    parser.add_argument('--init', '-i', action='store_true', help=\n        'Initialize with sample data')\n    args = parser.parse_args()\n    rag_manager = RAGManager()\n    if args.clear:\n        rag_manager.clear_index()\n        print('Index cleared.')\n        return\n    if args.init:\n        documents = create_sample_data()\n        rag_manager.add_documents(documents)\n        print(f'Added {len(documents)} sample documents to index.')\n        return\n    if args.add:\n        rag_manager.add_documents([args.add])\n        print(f'Added document: {args.add}')\n        return\n    if args.list:\n        if rag_manager.get_document_count() == 0:\n            print(\n                'No documents in index. Use --init to add sample data or --add to add documents.'\n                )\n        else:\n            print(\n                f'Documents in index ({rag_manager.get_document_count()} total):'\n                )\n            for i, meta in enumerate(rag_manager.metadata):\n                print(f\"  {i + 1}. {meta['content']}\")\n        return\n    if args.query:\n        if rag_manager.get_document_count() == 0:\n            print(\n                'Index is empty. Use --init to add sample data or --add to add documents.'\n                )\n            return\n        results = rag_manager.search(args.query, k=3)\n        print(f\"Top 3 results for '{args.query}':\")\n        for i, (doc, score, meta) in enumerate(results, 1):\n            print(f'  {i}. [Score: {score:.4f}] {doc}')\n        return\n    parser.print_help()\n\n\ndef main():\n    \"\"\"Main function demonstrating RAG through CLI.\"\"\"\n    parser = argparse.ArgumentParser(description='RAG CLI Example')\n    parser.add_argument('--query', '-q', type=str, help='Query to search for')\n    parser.add_argument('--add', '-a', type=str, help=\n        'Add a new document to the index')\n    parser.add_argument('--list', '-l', action='store_true', help=\n        'List all documents in the index')\n    parser.add_argument('--clear', '-c', action='store_true', help=\n        'Clear the index')\n    parser.add_argument('--init', '-i', action='store_true', help=\n        'Initialize with sample data')\n    args = parser.parse_args()\n    rag_manager = RAGManager()\n    if args.clear:\n        rag_manager.clear_index()\n        print('Index cleared.')\n        return\n    if args.init:\n        documents = create_sample_data()\n        rag_manager.add_documents(documents)\n        print(f'Added {len(documents)} sample documents to index.')\n        return\n    if args.add:\n        rag_manager.add_documents([args.add])\n        print(f'Added document: {args.add}')\n        return\n    if args.list:\n        if rag_manager.get_document_count() == 0:\n            print(\n                'No documents in index. Use --init to add sample data or --add to add documents.'\n                )\n        else:\n            print(\n                f'Documents in index ({rag_manager.get_document_count()} total):'\n                )\n            for i, meta in enumerate(rag_manager.metadata):\n                print(f\"  {i + 1}. {meta['content']}\")\n        return\n    if args.query:\n        if rag_manager.get_document_count() == 0:\n            print(\n                'Index is empty. Use --init to add sample data or --add to add documents.'\n                )\n            return\n        results = rag_manager.search(args.query, k=3)\n        print(f\"Top 3 results for '{args.query}':\")\n        for i, (doc, score, meta) in enumerate(results, 1):\n            print(f'  {i}. [Score: {score:.4f}] {doc}')\n        return\n    parser.print_help()\n\n\nif __name__ == '__main__':\n    main()\n",
    "file": "/mnt/ProjectData/omni/rag_cli_example.py"
  },
  {
    "id": 11,
    "hash": "f59a27842056dbb0dc95d0f29c1c5de8",
    "content": "Hello! I'm OmniForge, an AI coding assistant. I can help you scan, understand, and refactor your codebase using local (Ollama) or remote (OpenRouter) LLMs.\n\nI see you've opened the OmniForge project directory. Some key things I can help with:\n\n1. **Project Analysis**: I can look at your files to understand the structure\n2. **Code Editing**: I can make precise edits to functions/classes using AST-based transformations\n3. **Refactoring**: I can perform multi-file architectural changes with a plan\n4. **File Creation**: I can generate new files based on instructions\n\nFor example:\n- `look .` to scan the project\n- `edit code_editor.py \"add docstrings to main methods\"`\n- `refactor \"extract git operations into a separate module\"`\n- `create new_module.py \"implement a simple HTTP client\"`\n\nWhat would you like to do with this project?",
    "file": "/mnt/ProjectData/omni/requirements.txt"
  },
  {
    "id": 12,
    "hash": "dbd143839288b400cafdedb655258434",
    "content": "import os\nimport json\nimport hashlib\nfrom typing import List, Dict, Optional, Tuple\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nimport faiss\nfrom pathlib import Path\n\n\nclass VectorDBManager:\n    \"\"\"Manages vector database operations for RAG using sentence transformers and FAISS.\"\"\"\n\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\n        Optional[str]=None):\n        \"\"\"\n        Initialize the VectorDB manager.\n\n        Args:\n            model_name: Name of the sentence transformer model to use\n            index_path: Path to save/load the FAISS index\n        \"\"\"\n        self.model = SentenceTransformer(model_name)\n        self.index_path = index_path or 'vectordb_index.bin'\n        self.metadata_path = self.index_path.replace('.bin', '_metadata.json')\n        self.dimension = self.model.get_sentence_embedding_dimension()\n        self.index = None\n        self.metadata: List[Dict] = []\n        self._initialize_index()\n\n    def _initialize_index(self):\n        \"\"\"Initialize or load the FAISS index.\"\"\"\n        if os.path.exists(self.index_path) and os.path.exists(self.\n            metadata_path):\n            self.index = faiss.read_index(self.index_path)\n            with open(self.metadata_path, 'r') as f:\n                self.metadata = json.load(f)\n        else:\n            self.index = faiss.IndexFlatIP(self.dimension)\n            self.metadata = []\n\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\n        Dict]]=None):\n        \"\"\"\n        Add documents to the vector database.\n\n        Args:\n            documents: List of text documents to add\n            metadatas: Optional list of metadata for each document\n        \"\"\"\n        if metadatas is None:\n            metadatas = [{}] * len(documents)\n        embeddings = self.model.encode(documents)\n        faiss.normalize_L2(embeddings)\n        self.index.add(embeddings.astype(np.float32))\n        for i, meta in enumerate(metadatas):\n            doc_hash = hashlib.md5(documents[i].encode()).hexdigest()\n            meta_entry = {'id': len(self.metadata), 'hash': doc_hash,\n                'content': documents[i], **meta}\n            self.metadata.append(meta_entry)\n        self._save_index()\n\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\n        \"\"\"\n        Search for relevant documents.\n\n        Args:\n            query: Query string\n            k: Number of results to return\n\n        Returns:\n            List of (document, score, metadata) tuples\n        \"\"\"\n        query_embedding = self.model.encode([query])\n        faiss.normalize_L2(query_embedding)\n        scores, indices = self.index.search(query_embedding.astype(np.\n            float32), k)\n        results = []\n        for score, idx in zip(scores[0], indices[0]):\n            if idx < len(self.metadata):\n                doc_info = self.metadata[idx]\n                results.append((doc_info['content'], float(score), doc_info))\n        return results\n\n    def _save_index(self):\n        \"\"\"Save the FAISS index and metadata to disk.\"\"\"\n        faiss.write_index(self.index, self.index_path)\n        with open(self.metadata_path, 'w') as f:\n            json.dump(self.metadata, f, indent=2)\n\n    def get_document_count(self) ->int:\n        \"\"\"Get the number of documents in the index.\"\"\"\n        return len(self.metadata)\n\n    def clear_index(self):\n        \"\"\"Clear the index and metadata.\"\"\"\n        self.index = faiss.IndexFlatIP(self.dimension)\n        self.metadata = []\n        self._save_index()\n\n\nclass VectorDBManager:\n    \"\"\"Manages vector database operations for RAG using sentence transformers and FAISS.\"\"\"\n\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\n        Optional[str]=None):\n        \"\"\"\n        Initialize the VectorDB manager.\n\n        Args:\n            model_name: Name of the sentence transformer model to use\n            index_path: Path to save/load the FAISS index\n        \"\"\"\n        self.model = SentenceTransformer(model_name)\n        self.index_path = index_path or 'vectordb_index.bin'\n        self.metadata_path = self.index_path.replace('.bin', '_metadata.json')\n        self.dimension = self.model.get_sentence_embedding_dimension()\n        self.index = None\n        self.metadata: List[Dict] = []\n        self._initialize_index()\n\n    def _initialize_index(self):\n        \"\"\"Initialize or load the FAISS index.\"\"\"\n        if os.path.exists(self.index_path) and os.path.exists(self.\n            metadata_path):\n            self.index = faiss.read_index(self.index_path)\n            with open(self.metadata_path, 'r') as f:\n                self.metadata = json.load(f)\n        else:\n            self.index = faiss.IndexFlatIP(self.dimension)\n            self.metadata = []\n\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\n        Dict]]=None):\n        \"\"\"\n        Add documents to the vector database.\n\n        Args:\n            documents: List of text documents to add\n            metadatas: Optional list of metadata for each document\n        \"\"\"\n        if metadatas is None:\n            metadatas = [{}] * len(documents)\n        embeddings = self.model.encode(documents)\n        faiss.normalize_L2(embeddings)\n        self.index.add(embeddings.astype(np.float32))\n        for i, meta in enumerate(metadatas):\n            doc_hash = hashlib.md5(documents[i].encode()).hexdigest()\n            meta_entry = {'id': len(self.metadata), 'hash': doc_hash,\n                'content': documents[i], **meta}\n            self.metadata.append(meta_entry)\n        self._save_index()\n\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\n        \"\"\"\n        Search for relevant documents.\n\n        Args:\n            query: Query string\n            k: Number of results to return\n\n        Returns:\n            List of (document, score, metadata) tuples\n        \"\"\"\n        query_embedding = self.model.encode([query])\n        faiss.normalize_L2(query_embedding)\n        scores, indices = self.index.search(query_embedding.astype(np.\n            float32), k)\n        results = []\n        for score, idx in zip(scores[0], indices[0]):\n            if idx < len(self.metadata):\n                doc_info = self.metadata[idx]\n                results.append((doc_info['content'], float(score), doc_info))\n        return results\n\n    def _save_index(self):\n        \"\"\"Save the FAISS index and metadata to disk.\"\"\"\n        faiss.write_index(self.index, self.index_path)\n        with open(self.metadata_path, 'w') as f:\n            json.dump(self.metadata, f, indent=2)\n\n    def get_document_count(self) ->int:\n        \"\"\"Get the number of documents in the index.\"\"\"\n        return len(self.metadata)\n\n    def clear_index(self):\n        \"\"\"Clear the index and metadata.\"\"\"\n        self.index = faiss.IndexFlatIP(self.dimension)\n        self.metadata = []\n        self._save_index()\n\n\nclass VectorDBManager:\n    \"\"\"Manages vector database operations for RAG using sentence transformers and FAISS.\"\"\"\n\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\n        Optional[str]=None):\n        \"\"\"\n        Initialize the VectorDB manager.\n\n        Args:\n            model_name: Name of the sentence transformer model to use\n            index_path: Path to save/load the FAISS index\n        \"\"\"\n        self.model = SentenceTransformer(model_name)\n        self.index_path = index_path or 'vectordb_index.bin'\n        self.metadata_path = self.index_path.replace('.bin', '_metadata.json')\n        self.dimension = self.model.get_sentence_embedding_dimension()\n        self.index = None\n        self.metadata: List[Dict] = []\n        self._initialize_index()\n\n    def _initialize_index(self):\n        \"\"\"Initialize or load the FAISS index.\"\"\"\n        if os.path.exists(self.index_path) and os.path.exists(self.\n            metadata_path):\n            self.index = faiss.read_index(self.index_path)\n            with open(self.metadata_path, 'r') as f:\n                self.metadata = json.load(f)\n        else:\n            self.index = faiss.IndexFlatIP(self.dimension)\n            self.metadata = []\n\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\n        Dict]]=None):\n        \"\"\"\n        Add documents to the vector database.\n\n        Args:\n            documents: List of text documents to add\n            metadatas: Optional list of metadata for each document\n        \"\"\"\n        if metadatas is None:\n            metadatas = [{}] * len(documents)\n        embeddings = self.model.encode(documents)\n        faiss.normalize_L2(embeddings)\n        self.index.add(embeddings.astype(np.float32))\n        for i, meta in enumerate(metadatas):\n            doc_hash = hashlib.md5(documents[i].encode()).hexdigest()\n            meta_entry = {'id': len(self.metadata), 'hash': doc_hash,\n                'content': documents[i], **meta}\n            self.metadata.append(meta_entry)\n        self._save_index()\n\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\n        \"\"\"\n        Search for relevant documents.\n\n        Args:\n            query: Query string\n            k: Number of results to return\n\n        Returns:\n            List of (document, score, metadata) tuples\n        \"\"\"\n        query_embedding = self.model.encode([query])\n        faiss.normalize_L2(query_embedding)\n        scores, indices = self.index.search(query_embedding.astype(np.\n            float32), k)\n        results = []\n        for score, idx in zip(scores[0], indices[0]):\n            if idx < len(self.metadata):\n                doc_info = self.metadata[idx]\n                results.append((doc_info['content'], float(score), doc_info))\n        return results\n\n    def _save_index(self):\n        \"\"\"Save the FAISS index and metadata to disk.\"\"\"\n        faiss.write_index(self.index, self.index_path)\n        with open(self.metadata_path, 'w') as f:\n            json.dump(self.metadata, f, indent=2)\n\n    def get_document_count(self) ->int:\n        \"\"\"Get the number of documents in the index.\"\"\"\n        return len(self.metadata)\n\n    def clear_index(self):\n        \"\"\"Clear the index and metadata.\"\"\"\n        self.index = faiss.IndexFlatIP(self.dimension)\n        self.metadata = []\n        self._save_index()\n",
    "file": "/mnt/ProjectData/omni/vectordb_manager.py"
  },
  {
    "id": 13,
    "hash": "c27b11d2eaa8e13cdf8e698417efb75e",
    "content": "import os\nimport sys\nfrom typing import List, Dict, Tuple\n\"\"\"\nExample script demonstrating RAG (Retrieval-Augmented Generation) usage.\n\nThis script shows how to use a simple RAG implementation to answer questions\nbased on a given context or knowledge base.\n\"\"\"\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__),\n    '..')))\n\n\nclass SimpleRAG:\n    \"\"\"A simple RAG implementation for demonstration purposes.\"\"\"\n\n    def __init__(self, knowledge_base: List[str]):\n        \"\"\"\n        Initialize the RAG with a knowledge base.\n        \n        Args:\n            knowledge_base: A list of strings representing the knowledge base.\n        \"\"\"\n        self.knowledge_base = knowledge_base\n\n    def retrieve(self, query: str, top_k: int=3) ->List[str]:\n        \"\"\"\n        Retrieve relevant documents from the knowledge base.\n        \n        This is a simplified implementation using keyword matching.\n        In a real implementation, you would use embeddings and vector search.\n        \n        Args:\n            query: The query string.\n            top_k: Number of top documents to retrieve.\n            \n        Returns:\n            A list of relevant documents.\n        \"\"\"\n        query_words = set(query.lower().split())\n        scores = []\n        for doc in self.knowledge_base:\n            doc_words = set(doc.lower().split())\n            score = len(query_words.intersection(doc_words))\n            scores.append((score, doc))\n        scores.sort(reverse=True)\n        return [doc for score, doc in scores[:top_k]]\n\n    def generate(self, query: str, retrieved_docs: List[str]) ->str:\n        \"\"\"\n        Generate an answer based on the query and retrieved documents.\n        \n        In a real implementation, this would use an LLM to generate the response.\n        For this example, we'll create a simple template-based response.\n        \n        Args:\n            query: The query string.\n            retrieved_docs: The retrieved documents.\n            \n        Returns:\n            A generated answer.\n        \"\"\"\n        context = '\\n'.join(retrieved_docs)\n        prompt = f\"\"\"\n        Context information:\n        {context}\n        \n        Question: {query}\n        \n        Based on the context provided above, please answer the question.\n        If the context doesn't contain relevant information, say so.\n        \"\"\"\n        return self._simple_response_generator(query, retrieved_docs)\n\n    def _simple_response_generator(self, query: str, retrieved_docs: List[str]\n        ) ->str:\n        \"\"\"\n        A simple response generator for demonstration.\n        \"\"\"\n        for doc in retrieved_docs:\n            if 'example' in doc.lower():\n                return (\n                    f'Based on the context provided, I found information about examples. {query} relates to the examples in the knowledge base.'\n                    )\n        return (\n            f\"I couldn't find specific information about '{query}' in the provided context. Please provide more details or check the knowledge base.\"\n            )\n\n    def query(self, query: str, top_k: int=3) ->str:\n        \"\"\"\n        Process a query through the full RAG pipeline.\n        \n        Args:\n            query: The query string.\n            top_k: Number of top documents to retrieve.\n            \n        Returns:\n            The generated answer.\n        \"\"\"\n        retrieved_docs = self.retrieve(query, top_k)\n        answer = self.generate(query, retrieved_docs)\n        return answer\n\n\ndef main():\n    \"\"\"Main function demonstrating the RAG implementation.\"\"\"\n    knowledge_base = ['This is an example document about machine learning.',\n        'Natural language processing is a subfield of artificial intelligence.'\n        , 'Python is a popular programming language for data science.',\n        'The quick brown fox jumps over the lazy dog.',\n        'RAG stands for Retrieval-Augmented Generation.',\n        'Vector databases are used for similarity search in RAG systems.',\n        'Transformers are a type of neural network architecture.',\n        'This example shows how to implement a simple RAG system.']\n    rag = SimpleRAG(knowledge_base)\n    queries = ['What is RAG?', 'How is Python used in data science?',\n        'Tell me about machine learning']\n    print('Simple RAG Example')\n    print('=' * 50)\n    for query in queries:\n        print(f'\\nQuery: {query}')\n        answer = rag.query(query)\n        print(f'Answer: {answer}')\n        print('-' * 30)\n    print(\"\\nInteractive Mode (type 'quit' to exit):\")\n    while True:\n        try:\n            user_query = input('\\nEnter your question: ').strip()\n            if user_query.lower() in ['quit', 'exit', 'q']:\n                break\n            if user_query:\n                answer = rag.query(user_query)\n                print(f'Answer: {answer}')\n        except KeyboardInterrupt:\n            print('\\nGoodbye!')\n            break\n        except EOFError:\n            break\n\n\nif __name__ == '__main__':\n    main()\n",
    "file": "/mnt/ProjectData/omni/rag_example.py"
  },
  {
    "id": 14,
    "hash": "f151986bf2efa7602242590a357fab83",
    "content": "import os\nimport json\nimport hashlib\nfrom typing import List, Dict, Optional, Tuple\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nimport faiss\nfrom pathlib import Path\n\n\nclass RAGManager:\n    \"\"\"Manages Retrieval-Augmented Generation operations using sentence transformers.\"\"\"\n\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\n        Optional[str]=None):\n        \"\"\"\n    Initialize the RAG manager.\n\n    Args:\n        model_name: Name of the sentence transformer model to use\n        index_path: Path to save/load the FAISS index\n    \"\"\"\n        self.model_name = model_name\n        from vectordb_manager import VectorDBManager\n        self.vectordb = VectorDBManager(model_name, index_path)\n        self.model = self.vectordb.model\n        self.index_path = self.vectordb.index_path\n        self.metadata_path = self.vectordb.metadata_path\n        self.dimension = self.vectordb.dimension\n        self.index = self.vectordb.index\n        self.metadata = self.vectordb.metadata\n\n    def _initialize_index(self):\n        \"\"\"Initialize or load the FAISS index.\"\"\"\n        if os.path.exists(self.index_path) and os.path.exists(self.\n            metadata_path):\n            self.index = faiss.read_index(self.index_path)\n            with open(self.metadata_path, 'r') as f:\n                self.metadata = json.load(f)\n        else:\n            self.index = faiss.IndexFlatIP(self.dimension)\n            self.metadata = []\n\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\n        Dict]]=None):\n        \"\"\"\n        Add documents to the RAG index.\n\n        Args:\n            documents: List of text documents to add\n            metadatas: Optional list of metadata for each document\n        \"\"\"\n        if metadatas is None:\n            metadatas = [{}] * len(documents)\n        for i, meta in enumerate(metadatas):\n            if 'file' not in meta:\n                meta['file'] = f'document_{len(self.metadata) + i}'\n        self.vectordb.add_documents(documents, metadatas)\n        self.metadata = self.vectordb.metadata\n\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\n        \"\"\"\n        Search for relevant documents.\n\n        Args:\n            query: Query string\n            k: Number of results to return\n\n        Returns:\n            List of (document, score, metadata) tuples\n        \"\"\"\n        from vectordb_manager import VectorDBManager\n        temp_vdb = VectorDBManager(model_name=self.model_name, index_path=\n            self.index_path)\n        results = temp_vdb.search(query, k)\n        return results\n\n    def _save_index(self):\n        \"\"\"Save the FAISS index and metadata to disk.\"\"\"\n        faiss.write_index(self.index, self.index_path)\n        with open(self.metadata_path, 'w') as f:\n            json.dump(self.metadata, f, indent=2)\n\n    def get_document_count(self) ->int:\n        \"\"\"Get the number of documents in the index.\"\"\"\n        return len(self.metadata)\n\n    def clear_index(self):\n        \"\"\"Clear the index and metadata.\"\"\"\n        self.index = faiss.IndexFlatIP(self.dimension)\n        self.metadata = []\n        self._save_index()\n\n\nclass RAGManager:\n    \"\"\"Manages Retrieval-Augmented Generation operations using sentence transformers.\"\"\"\n\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\n        Optional[str]=None):\n        \"\"\"\n        Initialize the RAG manager.\n\n        Args:\n            model_name: Name of the sentence transformer model to use\n            index_path: Path to save/load the FAISS index\n        \"\"\"\n        self.model_name = model_name\n        from vectordb_manager import VectorDBManager\n        self.vectordb = VectorDBManager(model_name, index_path)\n        self.model = self.vectordb.model\n        self.index_path = self.vectordb.index_path\n        self.metadata_path = self.vectordb.metadata_path\n        self.dimension = self.vectordb.dimension\n        self.index = self.vectordb.index\n        self.metadata = self.vectordb.metadata\n\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\n        Dict]]=None):\n        \"\"\"\n        Add documents to the RAG index.\n\n        Args:\n            documents: List of text documents to add\n            metadatas: Optional list of metadata for each document\n        \"\"\"\n        if metadatas is None:\n            metadatas = [{}] * len(documents)\n        for i, meta in enumerate(metadatas):\n            if 'file' not in meta:\n                meta['file'] = f'document_{len(self.metadata) + i}'\n        self.vectordb.add_documents(documents, metadatas)\n        self.metadata = self.vectordb.metadata\n\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\n        \"\"\"\n        Search for relevant documents.\n\n        Args:\n            query: Query string\n            k: Number of results to return\n\n        Returns:\n            List of (document, score, metadata) tuples\n        \"\"\"\n        from vectordb_manager import VectorDBManager\n        temp_vdb = VectorDBManager(model_name=self.model_name, index_path=\n            self.index_path)\n        results = temp_vdb.search(query, k)\n        return results\n\n    def get_document_count(self) ->int:\n        \"\"\"Get the number of documents in the index.\"\"\"\n        return len(self.metadata)\n\n    def clear_index(self):\n        \"\"\"Clear the index and metadata.\"\"\"\n        self.index = faiss.IndexFlatIP(self.dimension)\n        self.metadata = []\n",
    "file": "/mnt/ProjectData/omni/rag_manager.py"
  },
  {
    "id": 15,
    "hash": "99dbcf73fb2f6f7d59ca44f4bba10e71",
    "content": "[\n  {\n    \"id\": 0,\n    \"hash\": \"43b76714d230bbe319bdf0de327c7192\",\n    \"content\": \"RAG stands for Retrieval-Augmented Generation, a technique that combines information retrieval with language generation.\",\n    \"file\": \"document_0\"\n  },\n  {\n    \"id\": 1,\n    \"hash\": \"dac60aa8d5e5861796640a8d9b3190a3\",\n    \"content\": \"The RAG model retrieves relevant documents from a knowledge base and uses them to generate more accurate responses.\",\n    \"file\": \"document_0\"\n  },\n  {\n    \"id\": 2,\n    \"hash\": \"f9ed7c2f9da6b1462806979d984ab5f3\",\n    \"content\": \"FAISS is a library for efficient similarity search and clustering of dense vectors, often used in RAG systems.\",\n    \"file\": \"document_0\"\n  },\n  {\n    \"id\": 3,\n    \"hash\": \"192336175c79d0194d2e51a613cbfa1b\",\n    \"content\": \"Sentence transformers are used to create embeddings for documents and queries in RAG systems.\",\n    \"file\": \"document_0\"\n  },\n  {\n    \"id\": 4,\n    \"hash\": \"92c5abaa3019f7294e5af489622302f5\",\n    \"content\": \"Retrieval-Augmented Generation improves language models by allowing them to access external knowledge sources.\",\n    \"file\": \"document_0\"\n  },\n  {\n    \"id\": 5,\n    \"hash\": \"92408c5c2b26cc7bb3e006f75f4f6f08\",\n    \"content\": \"In RAG systems, documents are indexed and searchable by their semantic embeddings rather than just keywords.\",\n    \"file\": \"document_0\"\n  },\n  {\n    \"id\": 6,\n    \"hash\": \"26ed14b833f20c4d6715cd2f9c44381a\",\n    \"content\": \"Python is a great language for implementing RAG systems due to libraries like sentence-transformers and faiss.\",\n    \"file\": \"document_0\"\n  },\n  {\n    \"id\": 7,\n    \"hash\": \"964a8aed30280d1b398f7b17dc6366bb\",\n    \"content\": \"The retrieval component of RAG is crucial for finding relevant information before generation.\",\n    \"file\": \"document_0\"\n  },\n  {\n    \"id\": 8,\n    \"hash\": \"1b61704bafe182154c395ea08a3acb39\",\n    \"content\": \"\\\"\\\"\\\"\\nOmni - AI-powered code generation and project-aware editing CLI tool\\n\\nIntegrates modular UI, memory, personality, and AST-based code editing.\\n\\\"\\\"\\\"\\nimport os\\nimport subprocess\\nimport requests\\nimport json\\nimport sys\\nimport re\\nimport argparse\\nimport ast\\nfrom datetime import datetime\\nimport threading\\nimport queue as Queue\\nimport time\\nfrom typing import List, Dict, Optional\\nfrom rich import print\\nfrom rich.panel import Panel\\nfrom rich.console import Console\\nfrom rich.tree import Tree\\nfrom ui_manager import UIManager\\nfrom personality_manager import PersonalityManager\\nfrom memory_manager import MemoryManager\\nfrom code_editor import CodeEditor\\nfrom file_creator import FileCreator\\nfrom git_manager import GitManager\\nimport traceback\\nDEFAULT_BACKEND = 'openrouter'\\nOLLAMA_MODEL = 'phi4-reasoning'\\nOPENROUTER_MODEL = 'qwen/qwen3-coder'\\nDEFAULT_SAVE_DIR = os.path.expanduser('/mnt/ProjectData/omni/omni_saves/')\\nCONFIG_FILE = 'config.json'\\nMEMORY_FILE = 'memory.json'\\nOLLAMA_API_URL = 'http://localhost:11434/api/generate'\\nOPENROUTER_API_URL = 'https://openrouter.ai/api/v1/chat/completions'\\nOPENROUTER_MODELS_API_URL = 'https://openrouter.ai/api/v1/models'\\nOPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')\\ncurrent_backend = DEFAULT_BACKEND\\ncurrent_model = (OLLAMA_MODEL if DEFAULT_BACKEND == 'ollama' else\\n    OPENROUTER_MODEL)\\nOLLAMA_MODELS = {'deepseek': 'deepseek-coder:6.7b', 'codellama':\\n    'codellama:13b', 'mistral': 'mistral:latest', 'llama2': 'llama2:latest',\\n    'phind': 'phind-codellama:34b'}\\nos.makedirs(DEFAULT_SAVE_DIR, exist_ok=True)\\nTEMPLATES = {'flask':\\n    \\\"\\\"\\\"from flask import Flask, jsonify\\n\\napp = Flask(__name__)\\n\\n@app.route('/')\\ndef home():\\n    return '<h1>Hello, Flask!</h1>'\\n\\nif __name__ == '__main__':\\n    app.run(debug=True)\\\"\\\"\\\"\\n    , 'html5':\\n    \\\"\\\"\\\"<!DOCTYPE html>\\n<html lang=\\\"en\\\">\\n<head>\\n    <meta charset=\\\"UTF-8\\\">\\n    <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\">\\n    <title>New Page</title>\\n</head>\\n<body>\\n    <h1>Hello World</h1>\\n</body>\\n</html>\\\"\\\"\\\"\\n    , 'scraper':\\n    \\\"\\\"\\\"import requests\\nfrom bs4 import BeautifulSoup\\n\\ndef scrape(url):\\n    try:\\n        response = requests.get(url)\\n        response.raise_for_status()\\n        soup = BeautifulSoup(response.content, 'html.parser')\\n        print(soup.title.text)\\n    except Exception as e:\\n        print(f\\\"[bold red]Error scraping {url}:[/] {e}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    scrape(\\\"https://example.com\\\")\\\"\\\"\\\"\\n    }\\nconsole = Console()\\npersonality_manager = PersonalityManager(CONFIG_FILE)\\nmemory_manager = MemoryManager(MEMORY_FILE)\\nui_manager = UIManager()\\nlast_query: Optional[str] = None\\nlast_response: Optional[str] = None\\nlast_code: Optional[str] = None\\n\\n\\ndef start_ollama_server() ->None:\\n    if current_backend != 'ollama':\\n        return\\n    try:\\n        requests.get('http://localhost:11434', timeout=1)\\n    except requests.exceptions.ConnectionError:\\n        print('[cyan]Starting Ollama server...[/]')\\n        subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL,\\n            stderr=subprocess.DEVNULL)\\n\\n\\ndef query_llm(prompt: str) ->str:\\n    personality = personality_manager.get_current_personality()\\n    system_prompt = personality.get('system_prompt', '') if personality else ''\\n    memory_context = memory_manager.get_memory_context()\\n    rag_context = ''\\n    try:\\n        from rag_manager import RAGManager\\n        project_root = memory_manager.get_project_root()\\n        if project_root:\\n            rag_manager = RAGManager()\\n            if rag_manager.get_document_count() > 0:\\n                results = rag_manager.search(prompt, k=3)\\n                if results:\\n                    rag_context = '\\\\n\\\\nRelevant context from codebase:\\\\n'\\n                    for i, (doc, score, meta) in enumerate(results, 1):\\n                        file_path = meta.get('file', 'Unknown')\\n                        rag_context += f'{i}. [{file_path}] {doc}\\\\n'\\n    except Exception:\\n        pass\\n    full_prompt = (\\n        f'{system_prompt}\\\\n\\\\n{memory_context}{rag_context}\\\\n\\\\nUser: {prompt}')\\n    with ui_manager.show_spinner('AI is listening and thinking...'):\\n        if current_backend == 'ollama':\\n            response = query_ollama(full_prompt)\\n        elif current_backend == 'openrouter':\\n            response = query_openrouter(full_prompt)\\n        else:\\n            response = '[bold red]Error:[/] Unknown backend'\\n    return response\\n\\n\\ndef query_openrouter(prompt: str) ->str:\\n    if not OPENROUTER_API_KEY:\\n        return '[bold red]Error:[/] OPENROUTER_API_KEY not set.'\\n    headers = {'Authorization': f'Bearer {OPENROUTER_API_KEY}',\\n        'Content-Type': 'application/json'}\\n    payload = {'model': current_model, 'messages': [{'role': 'user',\\n        'content': prompt}]}\\n    try:\\n        response = requests.post(OPENROUTER_API_URL, headers=headers, json=\\n            payload, timeout=90)\\n        response.raise_for_status()\\n        return response.json()['choices'][0]['message']['content']\\n    except Exception as e:\\n        error_details = ''\\n        try:\\n            error_details = response.json()\\n        except:\\n            error_details = response.text if hasattr(response, 'text'\\n                ) else str(e)\\n        return (\\n            f'[bold red]OpenRouter Error:[/] {e}\\\\n[dim]Details: {error_details}[/dim]'\\n            )\\n\\n\\ndef query_ollama(prompt: str) ->str:\\n    payload = {'model': current_model, 'prompt': prompt, 'stream': False}\\n    try:\\n        response = requests.post(OLLAMA_API_URL, json=payload, timeout=90)\\n        response.raise_for_status()\\n        return response.json()['response']\\n    except Exception as e:\\n        return f'[bold red]Ollama Error:[/] {e}'\\n\\n\\ndef extract_code(text: str) ->List[tuple[str, str]]:\\n    matches = re.findall('```(\\\\\\\\w*)\\\\\\\\n([\\\\\\\\s\\\\\\\\S]*?)```', text)\\n    return [(lang or 'text', code.strip()) for lang, code in matches\\n        ] if matches else []\\n\\n\\ndef list_models(args: list=None) ->None:\\n    if current_backend == 'ollama':\\n        print('[bold cyan]Popular Ollama Models:[/]')\\n        for name, model_id in OLLAMA_MODELS.items():\\n            print(\\n                f\\\"{'\\u2b50' if model_id == current_model else '  '} [yellow]{name:12}[/] \\u2192 {model_id}\\\"\\n                )\\n    elif current_backend == 'openrouter':\\n        list_openrouter_models(args or [])\\n\\n\\ndef list_openrouter_models(args: list):\\n    try:\\n        from simple_term_menu import TerminalMenu\\n    except ImportError:\\n        ui_manager.show_error(\\n            \\\"'simple-term-menu' is required. `pip install simple-term-menu`\\\")\\n        return\\n    try:\\n        with ui_manager.show_spinner('Fetching models...'):\\n            response = requests.get(OPENROUTER_MODELS_API_URL)\\n            response.raise_for_status()\\n        api_models_data = response.json().get('data', [])\\n    except requests.RequestException as e:\\n        ui_manager.show_error(f'Error fetching models: {e}')\\n        return\\n    all_models, sources = [], set()\\n    for model_data in api_models_data:\\n        if (model_id := model_data.get('id')):\\n            sources.add(model_id.split('/')[0])\\n            pricing = model_data.get('pricing', {})\\n            is_free = pricing.get('prompt') == '0' and pricing.get('completion'\\n                ) == '0'\\n            all_models.append({'id': model_id, 'name': model_data.get(\\n                'name'), 'source': model_id.split('/')[0], 'is_free': is_free})\\n    all_models.sort(key=lambda x: (x['source'], x['name']))\\n    if args and args[0].lower() == 'sources':\\n        print('[bold cyan]Available Model Sources:[/]')\\n        [print(f'  [yellow]{s}[/]') for s in sorted(list(sources))]\\n        return\\n    models_to_display, title = all_models, 'Select an OpenRouter Model'\\n    if args:\\n        filter_keyword = args[0].lower()\\n        title = f\\\"Select a Model from '{filter_keyword}'\\\"\\n        models_to_display = [m for m in all_models if filter_keyword in m[\\n            'source'].lower()]\\n        if not models_to_display:\\n            ui_manager.show_error(f\\\"No models for source: '{filter_keyword}'\\\")\\n            return\\n    menu_entries = [\\n        f\\\"{'\\u2b50' if m['id'] == current_model else '  '} {m['name']} [dim]({m['id']}){' [green](FREE)[/]' if m['is_free'] else ''}[/dim]\\\"\\n         for m in models_to_display]\\n    try:\\n        cursor_idx = next((i for i, m in enumerate(models_to_display) if m[\\n            'id'] == current_model), 0)\\n        chosen_index = TerminalMenu(menu_entries, title=f'{title}',\\n            cursor_index=cursor_idx, cycle_cursor=True, clear_screen=True\\n            ).show()\\n        if chosen_index is not None:\\n            set_model(models_to_display[chosen_index]['id'])\\n        else:\\n            print('Model selection cancelled.')\\n    except Exception as e:\\n        ui_manager.show_error(f'Menu display error: {e}')\\n\\n\\ndef set_model(model_id: str) ->None:\\n    global current_model\\n    current_model = model_id\\n    ui_manager.show_success(f'Model set to: {current_model}')\\n\\n\\ndef switch_backend(backend_name: str) ->None:\\n    global current_backend, current_model\\n    backend_name = backend_name.lower()\\n    if backend_name not in ['ollama', 'openrouter']:\\n        ui_manager.show_error(f'Unknown backend: {backend_name}')\\n        return\\n    current_backend = backend_name\\n    current_model = (OLLAMA_MODEL if backend_name == 'ollama' else\\n        OPENROUTER_MODEL)\\n    ui_manager.show_success(\\n        f'Switched to {backend_name} backend with model: {current_model}')\\n    if backend_name == 'ollama':\\n        start_ollama_server()\\n\\n\\ndef generate_project_manifest(path: str) ->tuple[str, List[str]]:\\n    manifest = ''\\n    file_paths = []\\n    tree = Tree(f'[bold cyan]Project: {os.path.basename(path)}[/]')\\n    exclude_dirs = {'__pycache__', '.git', 'venv', 'node_modules', '.idea'}\\n    for root, dirs, files in os.walk(path):\\n        dirs[:] = [d for d in dirs if d not in exclude_dirs and not d.\\n            startswith('.')]\\n        relative_path = os.path.relpath(root, path)\\n        branch = tree\\n        if relative_path != '.':\\n            parts = relative_path.split(os.sep)\\n            for part in parts:\\n                child = next((c for c in branch.children if c.label ==\\n                    f'[magenta]{part}[/]'), None)\\n                if not child:\\n                    child = branch.add(f'[magenta]{part}[/]')\\n                branch = child\\n        for fname in sorted(files):\\n            ext = os.path.splitext(fname)[1]\\n            if ext not in ('.py', '.js', '.html', '.css', '.md', '.txt'):\\n                continue\\n            rel_path = os.path.join(relative_path, fname\\n                ) if relative_path != '.' else fname\\n            file_paths.append(rel_path)\\n            branch.add(f'[green]{fname}[/]' if ext == '.py' else\\n                f'[dim]{fname}[/]')\\n            manifest += f'File: {rel_path}\\\\n\\\\n'\\n    console.print(tree)\\n    return manifest.strip(), file_paths\\n\\n\\ndef look_command(path: str) ->None:\\n    \\\"\\\"\\\"\\n    Scans a directory or file and loads it into memory. It can resolve paths\\n    relative to the current working directory or the project root in memory.\\n    If a new directory is scanned, the previous 'look' context is cleared\\n    to ensure the context remains relevant.\\n    \\\"\\\"\\\"\\n    resolved_path = resolve_file_path(path)\\n    if not resolved_path:\\n        ui_manager.show_error(f'\\u274c Path not found: {path}')\\n        return\\n    if resolved_path != os.path.abspath(path):\\n        ui_manager.show_success(\\n            f\\\"Found '{path}' in project. Using: {resolved_path}\\\")\\n    if os.path.isdir(resolved_path):\\n        ui_manager.show_success(\\n            \\\"New project directory detected. Clearing previous 'look' context.\\\"\\n            )\\n        memory_manager.memory['look'] = []\\n        with ui_manager.show_spinner('Generating project manifest...'):\\n            manifest = generate_project_manifest(resolved_path)\\n        memory_manager.add_look_data(resolved_path, manifest)\\n        ui_manager.show_success('\\u2705 Project manifest added to memory.')\\n    else:\\n        try:\\n            with ui_manager.show_spinner('Loading file...'):\\n                with open(resolved_path, 'r', encoding='utf-8') as f:\\n                    content = f.read().strip()\\n            for item in memory_manager.memory['look']:\\n                if item.get('file') == resolved_path:\\n                    item['content'] = content\\n                    memory_manager.save_memory()\\n                    ui_manager.show_success(\\n                        '\\u2705 Refreshed file content in memory.')\\n                    return\\n            memory_manager.add_look_data(resolved_path, content)\\n            ui_manager.show_success('\\u2705 File content added to memory.')\\n        except Exception as e:\\n            ui_manager.show_error(f'\\u274c Error reading file: {e}')\\n\\n\\ndef resolve_file_path(path: str) ->Optional[str]:\\n    \\\"\\\"\\\"Resolves a file path, checking CWD first, then against project root in memory.\\\"\\\"\\\"\\n    if os.path.exists(path):\\n        return os.path.abspath(path)\\n    project_root = memory_manager.get_project_root()\\n    if project_root:\\n        full_path = os.path.join(project_root, path)\\n        if os.path.exists(full_path):\\n            return full_path\\n    return None\\n\\n\\ndef _create_prompt_for_file_creation(file_name: str, instruction: str) ->str:\\n    \\\"\\\"\\\"\\n    Generate a robust prompt for file creation that instructs the AI to act as an expert,\\n    produce complete and clean code, and avoid any extra commentary.\\n    \\\"\\\"\\\"\\n    return f\\\"\\\"\\\"You are an expert programmer tasked with creating a new file. Your goal is to generate complete, production-ready content based on the user's instruction.\\n\\nIMPORTANT RULES:\\n- Provide ONLY the raw file content - no explanations, notes, or commentary outside the file itself\\n- Include all necessary imports, boilerplate, and complete implementations\\n- If creating code, ensure it's syntactically correct and follows best practices\\n- For configuration files, use appropriate formatting (JSON, YAML, etc.)\\n- For documentation files, use proper markdown formatting\\n\\nFile to create: {file_name}\\nUser instruction: {instruction}\\n\\nGenerate the complete file content now:\\\"\\\"\\\"\\n\\n\\ndef handle_file_create_command(file_path: str, instruction: str):\\n    \\\"\\\"\\\"\\n    Uses the LLM to generate content for a new file based on an instruction.\\n    \\\"\\\"\\\"\\n    global last_code\\n    if os.path.exists(file_path):\\n        if ui_manager.get_user_input(\\n            f\\\"File '{file_path}' already exists. Overwrite? (y/n): \\\").lower(\\n            ) not in ['yes', 'y']:\\n            ui_manager.show_error('File creation cancelled.')\\n            return\\n    prompt = _create_prompt_for_file_creation(os.path.basename(file_path),\\n        instruction)\\n    with ui_manager.show_spinner(\\n        f\\\"AI is generating content for '{file_path}'...\\\"):\\n        response = query_llm(prompt)\\n    code_blocks = extract_code(response)\\n    if code_blocks:\\n        new_content = code_blocks[0][1]\\n    else:\\n        new_content = response.strip()\\n    if not new_content:\\n        ui_manager.show_error('AI did not return any content.')\\n        print(Panel(response, title=\\\"[yellow]AI's Raw Response[/]\\\"))\\n        return\\n    print(Panel(new_content, title=\\n        f'[bold yellow]Proposed content for {file_path}[/]', border_style=\\n        'yellow'))\\n    if ui_manager.get_user_input('Create this file? (y/n): ').lower() in ['yes'\\n        , 'y']:\\n        try:\\n            FileCreator.create(file_path, new_content)\\n            last_code = new_content\\n            ui_manager.show_success(f'File created successfully: {file_path}')\\n        except IOError as e:\\n            ui_manager.show_error(f'Error creating file: {e}')\\n    else:\\n        ui_manager.show_error('File creation cancelled.')\\n\\n\\ndef look_all_command() ->None:\\n    \\\"\\\"\\\"\\n    Finds the project manifest in memory, reads every file listed, and adds their content to memory.\\n    \\\"\\\"\\\"\\n    project_root = memory_manager.get_project_root()\\n    if not project_root:\\n        ui_manager.show_error(\\n            \\\"No project context in memory. Use 'look <directory>' first to generate a manifest.\\\"\\n            )\\n        return\\n    manifest_data = None\\n    for item in memory_manager.memory.get('look', []):\\n        if item.get('file') == project_root and item.get('type'\\n            ) == 'directory':\\n            manifest_data = item.get('content')\\n            break\\n    if not manifest_data or not isinstance(manifest_data, (list, tuple)\\n        ) or len(manifest_data) != 2:\\n        ui_manager.show_error(\\n            \\\"Could not find a valid project manifest in memory. Please run 'look <directory>' again.\\\"\\n            )\\n        return\\n    file_paths = manifest_data[1]\\n    if not file_paths:\\n        ui_manager.show_error('No files found in the project manifest.')\\n        return\\n    total_files = len(file_paths)\\n    loaded_count = 0\\n    with ui_manager.show_spinner(\\n        f'Loading {total_files} files from project manifest...'):\\n        for file_path_relative in file_paths:\\n            full_path = os.path.join(project_root, file_path_relative)\\n            if os.path.isfile(full_path):\\n                try:\\n                    with open(full_path, 'r', encoding='utf-8') as f:\\n                        content = f.read().strip()\\n                    if not any(look['file'] == full_path for look in\\n                        memory_manager.memory['look']):\\n                        memory_manager.add_look_data(full_path, content)\\n                        loaded_count += 1\\n                except Exception as e:\\n                    print(\\n                        f\\\"[yellow]Skipping '{file_path_relative}': {e}[/yellow]\\\"\\n                        )\\n    ui_manager.show_success(\\n        f'\\u2705 Loaded content for {loaded_count} new files into memory.')\\n\\n\\ndef _load_all_project_files_if_needed():\\n    \\\"\\\"\\\"\\n    Checks if a project is loaded and automatically loads any files from its\\n    manifest that are not already in the 'look' memory. This ensures a\\n    complete context for editing and refactoring commands.\\n    \\\"\\\"\\\"\\n    project_root = memory_manager.get_project_root()\\n    if not project_root:\\n        return\\n    manifest_content = None\\n    for item in memory_manager.memory.get('look', []):\\n        if item.get('file') == project_root and 'File:' in item.get('content',\\n            ''):\\n            manifest_content = item['content']\\n            break\\n    if not manifest_content:\\n        return\\n    existing_file_paths = {item['file'] for item in memory_manager.memory.\\n        get('look', []) if item.get('type') == 'file'}\\n    file_paths_relative = re.findall('File: (.*)', manifest_content)\\n    files_to_load = []\\n    for rel_path in file_paths_relative:\\n        full_path = os.path.join(project_root, rel_path)\\n        if full_path not in existing_file_paths and os.path.isfile(full_path):\\n            files_to_load.append((full_path, rel_path))\\n    if not files_to_load:\\n        return\\n    loaded_count = 0\\n    with ui_manager.show_spinner(\\n        f'Auto-loading {len(files_to_load)} project files for context...'):\\n        for full_path, file_path_relative in files_to_load:\\n            try:\\n                with open(full_path, 'r', encoding='utf-8') as f:\\n                    content = f.read().strip()\\n                memory_manager.add_look_data(full_path, content)\\n                loaded_count += 1\\n            except Exception as e:\\n                print(f\\\"[yellow]Skipping '{file_path_relative}': {e}[/yellow]\\\")\\n    if loaded_count > 0:\\n        ui_manager.show_success(\\n            f'\\u2705 Loaded {loaded_count} new file(s) into memory for full project context.'\\n            )\\n\\n\\ndef _create_prompt_for_element_selection(file_name: str, instruction: str,\\n    elements: List[str], element_structures: Dict[str, Dict]) ->str:\\n    \\\"\\\"\\\"\\n    Create a helper for the first stage of the 'edit' command. This prompt asks the AI\\n    to analyze the user's instruction and intelligently select the most relevant code element to modify.\\n    \\\"\\\"\\\"\\n    element_details = []\\n    for elem in elements:\\n        if elem in element_structures:\\n            struct = element_structures[elem]\\n            detail = (\\n                f\\\"{elem} ({struct['type']}, lines {struct['line_start']}-{struct['line_end']})\\\"\\n                )\\n            element_details.append(detail)\\n        else:\\n            element_details.append(elem)\\n    return f\\\"\\\"\\\"You are an expert code analyzer. Your task is to identify what should be modified based on the user's instruction.\\n\\nFile: {file_name}\\nAvailable elements: {', '.join(element_details) if element_details else 'None'}\\n\\nUser instruction: {instruction}\\n\\nRESPONSE FORMAT:\\nChoose one of these response types:\\n1. \\\"ELEMENT: <element_name>\\\" - to edit an entire function/class\\n2. \\\"PARTIAL: <element_name> LINES: <start>-<end>\\\" - to edit specific lines within an element\\n3. \\\"FILE\\\" - to edit the entire file or multiple elements\\n\\nRULES:\\n- If the instruction mentions specific line numbers or a specific part of a function, use PARTIAL\\n- If the instruction targets an entire function/class, use ELEMENT\\n- If the instruction requires changes to multiple elements or file structure, use FILE\\n- For PARTIAL edits, provide absolute line numbers from the original file\\n\\nWhat should be edited?\\\"\\\"\\\"\\n\\n\\ndef _create_prompt_for_element_rewrite(file_name: str, element_name: str,\\n    instruction: str, original_code: str, is_full_file: bool=False) ->str:\\n    \\\"\\\"\\\"\\n    Create a helper for the second stage of the 'edit' command. This prompt instructs the AI\\n    to rewrite a specific code element (or the whole file) based on the user's request,\\n    demanding a complete and syntactically correct code block as output.\\n    \\\"\\\"\\\"\\n    if is_full_file:\\n        return f\\\"\\\"\\\"You are an expert programmer. Rewrite the entire file to accomplish the user's task.\\n\\nIMPORTANT RULES:\\n- Provide ONLY the complete, updated code for the entire file\\n- Ensure all syntax is correct and the code is ready to run\\n- Preserve existing functionality unless explicitly asked to change it\\n- Include all necessary imports and maintain the file's structure\\n- Do NOT include any explanations or comments outside the code\\n\\nFile: {file_name}\\nTask: {instruction}\\n\\nCurrent file content:\\n```python\\n{original_code}\\n```\\n\\nGenerate the complete updated file now:\\\"\\\"\\\"\\n    else:\\n        return f\\\"\\\"\\\"You are an expert programmer. Rewrite the specified element to accomplish the user's task.\\n\\nIMPORTANT RULES:\\n- Provide ONLY the complete, updated code for the element\\n- Include any necessary imports at the top of your code block\\n- Ensure the code is syntactically correct and maintains the same interface\\n- Do NOT include explanations or comments outside the code block\\n- The code must be a drop-in replacement for the original element\\n\\nFile: {file_name}\\nElement to modify: {element_name}\\nTask: {instruction}\\n\\nCurrent element code:\\n```python\\n{original_code}\\n```\\n\\nGenerate the complete updated element now:\\\"\\\"\\\"\\n\\n\\ndef _create_prompt_for_partial_edit(file_name: str, element_name: str,\\n    instruction: str, original_snippet: str, line_start: int, line_end: int,\\n    full_element_code: str) ->str:\\n    \\\"\\\"\\\"\\n    Create a prompt for partial edits within a function or class.\\n    This allows surgical changes to specific parts of code.\\n    \\\"\\\"\\\"\\n    return f\\\"\\\"\\\"You are an expert programmer. Make a surgical edit to a specific part of a function/class.\\n\\nCONTEXT:\\n- File: {file_name}\\n- Element: {element_name}\\n- Lines to modify: {line_start}-{line_end}\\n- Task: {instruction}\\n\\nIMPORTANT RULES:\\n- Provide ONLY the code that will replace lines {line_start}-{line_end}\\n- Your code must fit seamlessly into the existing function\\n- Maintain proper indentation (the code will be auto-indented)\\n- Do NOT include the function definition or other parts\\n- Do NOT include explanations outside the code\\n\\nFull element for context:\\n```python\\n{full_element_code}\\n```\\n\\nCode section to replace (lines {line_start}-{line_end}):\\n```python\\n{original_snippet}\\n```\\n\\nGenerate ONLY the replacement code for the specified lines:\\\"\\\"\\\"\\n\\n\\ndef handle_file_edit_command(file_path: str, instruction: str):\\n    \\\"\\\"\\\"\\n    Handles the entire workflow for editing a single file, ensuring full\\n    project context is loaded before the AI makes any decisions.\\n    Now supports partial edits within functions.\\n    \\\"\\\"\\\"\\n    global last_code\\n    _load_all_project_files_if_needed()\\n    resolved_path = resolve_file_path(file_path)\\n    if not resolved_path:\\n        ui_manager.show_error(f'File not found: {file_path}')\\n        return\\n    if resolved_path != os.path.abspath(file_path):\\n        ui_manager.show_success(\\n            f\\\"Found '{file_path}' in project. Using: {resolved_path}\\\")\\n    try:\\n        editor = CodeEditor(resolved_path)\\n    except (ValueError, FileNotFoundError) as e:\\n        ui_manager.show_error(str(e))\\n        return\\n    elements = editor.list_elements()\\n    element_structures = {}\\n    for elem in elements:\\n        struct = editor.get_element_structure(elem)\\n        if struct:\\n            element_structures[elem] = struct\\n    prompt1 = _create_prompt_for_element_selection(os.path.basename(\\n        resolved_path), instruction, elements, element_structures)\\n    with ui_manager.show_spinner('AI is analyzing file...'):\\n        ai_response = query_llm(prompt1).strip()\\n    if ai_response.upper() == 'FILE':\\n        ui_manager.show_success('AI has chosen to edit the entire file.')\\n        original_snippet = editor.source_code\\n        prompt2 = _create_prompt_for_element_rewrite(os.path.basename(\\n            resolved_path), 'entire file', instruction, original_snippet,\\n            is_full_file=True)\\n        edit_type = 'FILE'\\n        element_to_edit = None\\n        line_range = None\\n    elif ai_response.startswith('PARTIAL:'):\\n        parts = ai_response.split()\\n        element_to_edit = parts[1]\\n        if 'LINES:' in ai_response:\\n            line_part = ai_response.split('LINES:')[1].strip()\\n            if '-' in line_part:\\n                line_start, line_end = map(int, line_part.split('-'))\\n                line_range = line_start, line_end\\n            else:\\n                ui_manager.show_error('Invalid line range format')\\n                return\\n        else:\\n            ui_manager.show_error('Missing line range for partial edit')\\n            return\\n        if element_to_edit not in elements:\\n            ui_manager.show_error(f\\\"Element '{element_to_edit}' not found\\\")\\n            return\\n        ui_manager.show_success(\\n            f\\\"AI selected partial edit of '{element_to_edit}' (lines {line_start}-{line_end})\\\"\\n            )\\n        original_snippet = editor.get_element_body_snippet(element_to_edit,\\n            line_start, line_end)\\n        if not original_snippet:\\n            original_snippet = editor.get_source_of(element_to_edit)\\n        full_element_code = editor.get_source_of(element_to_edit)\\n        prompt2 = _create_prompt_for_partial_edit(os.path.basename(\\n            resolved_path), element_to_edit, instruction, original_snippet,\\n            line_start, line_end, full_element_code)\\n        edit_type = 'PARTIAL'\\n    elif ai_response.startswith('ELEMENT:'):\\n        element_to_edit = ai_response.split(':', 1)[1].strip()\\n        if element_to_edit not in elements:\\n            ui_manager.show_error(\\n                f\\\"AI identified '{element_to_edit}', which is not a valid element. Aborting.\\\"\\n                )\\n            return\\n        ui_manager.show_success(f\\\"AI selected '{element_to_edit}' for editing.\\\"\\n            )\\n        original_snippet = editor.get_source_of(element_to_edit)\\n        prompt2 = _create_prompt_for_element_rewrite(os.path.basename(\\n            resolved_path), element_to_edit, instruction, original_snippet)\\n        edit_type = 'ELEMENT'\\n        line_range = None\\n    else:\\n        element_to_edit = ai_response.splitlines()[0]\\n        if element_to_edit not in elements:\\n            ui_manager.show_error(\\n                f\\\"AI identified '{element_to_edit}', which is not a valid element. Aborting.\\\"\\n                )\\n            return\\n        ui_manager.show_success(f\\\"AI selected '{element_to_edit}' for editing.\\\"\\n            )\\n        original_snippet = editor.get_source_of(element_to_edit)\\n        prompt2 = _create_prompt_for_element_rewrite(os.path.basename(\\n            resolved_path), element_to_edit, instruction, original_snippet)\\n        edit_type = 'ELEMENT'\\n        line_range = None\\n    with ui_manager.show_spinner(f'AI is editing...'):\\n        response = query_llm(prompt2)\\n    code_blocks = extract_code(response)\\n    if not code_blocks:\\n        ui_manager.show_error('AI did not return a valid code block.')\\n        print(Panel(response, title=\\\"[yellow]AI's Raw Response[/]\\\"))\\n        return\\n    new_code = code_blocks[0][1]\\n    success = False\\n    if edit_type == 'FILE':\\n        try:\\n            editor.tree = ast.parse(new_code)\\n            success = True\\n        except SyntaxError as e:\\n            ui_manager.show_error(f'AI returned invalid Python syntax: {e}')\\n            print(Panel(response, title=\\\"[yellow]AI's Raw Response[/]\\\"))\\n            return\\n    elif edit_type == 'PARTIAL':\\n        success = editor.replace_partial(element_to_edit, new_code,\\n            line_start=line_range[0], line_end=line_range[1])\\n        if not success:\\n            ui_manager.show_error('Failed to apply partial edit.')\\n            print(Panel(response, title=\\\"[yellow]AI's Raw Response[/]\\\"))\\n            return\\n    else:\\n        success = editor.replace_element(element_to_edit, new_code)\\n        if not success:\\n            ui_manager.show_error(\\n                'AI returned invalid code; could not be parsed or applied.')\\n            print(Panel(response, title=\\\"[yellow]AI's Raw Response[/]\\\"))\\n            return\\n    if not (diff := editor.get_diff()):\\n        ui_manager.show_success('AI made no changes.')\\n        return\\n    print(Panel(diff, title=\\n        f'[bold yellow]Proposed Changes for {resolved_path}[/]'))\\n    if ui_manager.get_user_input('Apply changes? (y/n): ').lower() in ['yes',\\n        'y']:\\n        editor.save_changes()\\n        last_code = editor.get_modified_source()\\n        ui_manager.show_success(f'Changes saved to {resolved_path}.')\\n    else:\\n        ui_manager.show_error('Changes discarded.')\\n\\n\\ndef _create_prompt_for_refactor_plan(instruction: str, memory_context: str\\n    ) ->str:\\n    \\\"\\\"\\\"\\n    Create a specialized prompt-generation function for the 'refactor' command.\\n    This prompt will explicitly define the required JSON structure for the plan\\n    and instruct the AI to act as an expert project manager.\\n    \\\"\\\"\\\"\\n    return f\\\"\\\"\\\"You are an expert project manager and software architect. Analyze the project context and create a detailed refactoring plan.\\n\\nYour plan must be a valid JSON object with this exact structure:\\n{{\\n    \\\"actions\\\": [\\n        {{\\n            \\\"type\\\": \\\"MODIFY\\\" | \\\"CREATE\\\" | \\\"DELETE\\\" | \\\"PARTIAL\\\",\\n            \\\"file\\\": \\\"relative/path/to/file.py\\\",\\n            \\\"element\\\": \\\"function_or_class_name\\\",  // For MODIFY/DELETE/PARTIAL\\n            \\\"element_name\\\": \\\"new_element_name\\\",    // For CREATE\\n            \\\"line_start\\\": 10,                      // For PARTIAL only\\n            \\\"line_end\\\": 20,                        // For PARTIAL only\\n            \\\"reason\\\": \\\"Clear explanation of why this change is needed\\\",\\n            \\\"description\\\": \\\"What this action will accomplish\\\",\\n            \\\"anchor_element\\\": \\\"optional_anchor\\\",   // Optional for CREATE\\n            \\\"position\\\": \\\"before\\\" | \\\"after\\\"         // Optional for CREATE\\n        }}\\n    ]\\n}}\\n\\nACTION TYPES:\\n- MODIFY: Change an entire function, class, or method\\n- PARTIAL: Change specific lines within a function/class (requires line_start and line_end)\\n- CREATE: Add new functions, classes, or files\\n- DELETE: Remove functions, classes, variables, or imports\\n\\nRULES FOR YOUR PLAN:\\n- Use PARTIAL when you only need to change a small part of a function\\n- Use MODIFY when restructuring an entire function or class\\n- Each action must have all required fields based on its type\\n- File paths must be relative to the project root\\n- Be specific and surgical - avoid unnecessary changes\\n- Consider dependencies between changes\\n- Order actions logically (e.g., create dependencies before using them)\\n\\n### Project Context ###\\n{memory_context}\\n\\n### Refactoring Request ###\\n{instruction}\\n\\nGenerate ONLY the JSON plan - no explanations or markdown:\\\"\\\"\\\"\\n\\n\\ndef _get_refactor_plan(instruction: str) ->Optional[List[Dict]]:\\n    \\\"\\\"\\\"\\n    Generates a refactoring plan from the LLM.\\n\\n    This function encapsulates the logic for checking project context,\\n    constructing a prompt, querying the LLM, and parsing the resulting\\n    JSON plan for a refactoring task.\\n\\n    Args:\\n        instruction: The user's high-level refactoring instruction.\\n\\n    Returns:\\n        A list of action dictionaries if a valid plan is generated,\\n        otherwise None.\\n    \\\"\\\"\\\"\\n    if not memory_manager.get_project_root():\\n        ui_manager.show_error(\\n            \\\"No project context in memory. Use 'look <directory>' first.\\\")\\n        return None\\n    memory_context = memory_manager.get_memory_context()\\n    plan_prompt = _create_prompt_for_refactor_plan(instruction, memory_context)\\n    with ui_manager.show_spinner('AI is creating an execution plan...'):\\n        plan_str = query_llm(plan_prompt)\\n    try:\\n        match = re.search('\\\\\\\\{.*\\\\\\\\}', plan_str, re.DOTALL)\\n        if not match:\\n            raise ValueError('No JSON object found in the response.')\\n        plan = json.loads(match.group(0))\\n        actions = plan.get('actions', [])\\n        if not actions:\\n            raise ValueError(\\\"No 'actions' key found in plan or plan is empty.\\\"\\n                )\\n        return actions\\n    except (json.JSONDecodeError, ValueError) as e:\\n        ui_manager.show_error(f'AI failed to generate a valid plan: {e}')\\n        print(Panel(plan_str, title=\\\"[yellow]AI's Invalid Plan Response[/]\\\",\\n            border_style='yellow'))\\n        return None\\n\\n\\ndef _display_and_confirm_plan(plan: Dict) ->bool:\\n    \\\"\\\"\\\"\\n    Displays the generated execution plan to the user and asks for confirmation.\\n\\n    This helper function separates the UI interaction of plan confirmation from\\n    the main refactoring logic.\\n\\n    Args:\\n        plan: A dictionary, expected to contain an 'actions' key with a list of action dicts.\\n\\n    Returns:\\n        True if the user confirms the plan, False otherwise.\\n    \\\"\\\"\\\"\\n    actions = plan.get('actions', [])\\n    if not actions:\\n        ui_manager.show_error('The generated plan is empty. Aborting.')\\n        return False\\n    ui_manager.show_success('AI has created a plan:')\\n    for i, action in enumerate(actions):\\n        action_type = action.get('type', 'N/A')\\n        element = action.get('element') or action.get('element_name', 'N/A')\\n        reason = action.get('reason') or action.get('description', '')\\n        file_path = action.get('file', '')\\n        if action_type == 'PARTIAL':\\n            line_start = action.get('line_start', '?')\\n            line_end = action.get('line_end', '?')\\n            print(\\n                f'  [cyan]{i + 1}. {action_type}:[/] {file_path}/{element} (lines {line_start}-{line_end}) - {reason}'\\n                )\\n        else:\\n            print(\\n                f'  [cyan]{i + 1}. {action_type}:[/] {file_path}/{element} - {reason}'\\n                )\\n    if ui_manager.get_user_input('\\\\nProceed with this plan? (y/n): ').lower(\\n        ) in ['yes', 'y']:\\n        return True\\n    else:\\n        ui_manager.show_error('Execution aborted by user.')\\n        return False\\n\\n\\ndef _apply_refactor_changes(editors: Dict[str, CodeEditor]) ->None:\\n    \\\"\\\"\\\"\\n    Consolidates changes from multiple CodeEditor instances, shows a unified\\n    diff, and prompts the user to apply them.\\n    \\n    This helper function abstracts the final step of a refactor, ensuring\\n    all proposed modifications are presented to the user for a final review\\n    before any files are written to disk.\\n\\n    Args:\\n        editors: A dictionary mapping absolute file paths to their\\n                 corresponding CodeEditor instances which hold the\\n                 proposed changes in their AST.\\n    \\\"\\\"\\\"\\n    full_diff = ''\\n    for editor in editors.values():\\n        diff = editor.get_diff()\\n        if diff:\\n            full_diff += diff + '\\\\n'\\n    if not full_diff.strip():\\n        ui_manager.show_success('AI made no changes.')\\n        return\\n    print(Panel(full_diff, title=\\n        '[bold yellow]Proposed Project-Wide Changes[/]'))\\n    if ui_manager.get_user_input('Apply all changes? (y/n): ').lower() in [\\n        'yes', 'y']:\\n        for editor in editors.values():\\n            editor.save_changes()\\n        ui_manager.show_success('\\u2705 Project changes applied successfully.')\\n    else:\\n        ui_manager.show_error('Changes discarded.')\\n\\n\\ndef _create_prompt_for_refactor_action(action_type: str, file_path: str,\\n    action_details: Dict) ->str:\\n    \\\"\\\"\\\"\\n    Create a helper to generate prompts for individual 'CREATE' or 'MODIFY' steps\\n    within a refactor plan. This ensures the AI produces code for the specific\\n    sub-task in the correct context.\\n    \\\"\\\"\\\"\\n    if action_type == 'MODIFY':\\n        element_name = action_details['element_name']\\n        reason = action_details['reason']\\n        original_code = action_details['original_code']\\n        return f\\\"\\\"\\\"You are implementing a specific refactoring task as part of a larger plan.\\n\\nREFACTORING CONTEXT:\\n- File: {file_path}\\n- Element: {element_name}\\n- Reason for change: {reason}\\n\\nRULES:\\n- Provide ONLY the complete updated code for the element\\n- Include any necessary imports at the top\\n- Ensure the code integrates properly with the rest of the file\\n- Maintain the same function/class signature unless the change requires otherwise\\n- No explanations outside the code block\\n\\nCurrent element code:\\n```python\\n{original_code}\\n```\\n\\nGenerate the updated element code:\\\"\\\"\\\"\\n    elif action_type == 'CREATE':\\n        element_name = action_details['element_name']\\n        description = action_details['description']\\n        return f\\\"\\\"\\\"You are implementing a specific refactoring task as part of a larger plan.\\n\\nREFACTORING CONTEXT:\\n- File: {file_path}\\n- New element to create: {element_name}\\n- Purpose: {description}\\n\\nRULES:\\n- Provide ONLY the complete code for the new element\\n- Include all necessary imports at the top\\n- Follow the coding style and patterns used in the project\\n- Ensure the code is production-ready and well-structured\\n- For non-Python files, provide the complete file content\\n- No explanations outside the code block\\n\\nGenerate the new element code:\\\"\\\"\\\"\\n\\n\\ndef _process_refactor_action(action: Dict, project_base_path: str, editors:\\n    Dict) ->bool:\\n    \\\"\\\"\\\"\\n    Processes a single refactoring action from the plan.\\n\\n    This function handles the execution of a single action from the refactoring plan,\\n    including LLM code generation and applying changes to in-memory editors or files.\\n\\n    Args:\\n        action: A dictionary containing action details (type, file, element, etc.)\\n        project_base_path: The absolute path to the project root\\n        editors: Dictionary mapping file paths to their CodeEditor instances\\n\\n    Returns:\\n        True if the action was processed successfully, False otherwise\\n    \\\"\\\"\\\"\\n    file_path_relative = action.get('file')\\n    if not file_path_relative:\\n        ui_manager.show_error(\\n            f\\\"Action is missing 'file' key. Skipping: {action}\\\")\\n        return False\\n    file_path_absolute = os.path.join(project_base_path, file_path_relative)\\n    action_type = action.get('type', '').upper()\\n    prompt, element_name = '', ''\\n    if action_type == 'MODIFY':\\n        element_name = action.get('element')\\n        reason = action.get('reason')\\n        if file_path_relative.endswith('.py'):\\n            if file_path_absolute not in editors:\\n                try:\\n                    editors[file_path_absolute] = CodeEditor(file_path_absolute\\n                        )\\n                except Exception as e:\\n                    ui_manager.show_error(\\n                        f'Error loading file {file_path_absolute}: {e}')\\n                    return False\\n            editor = editors[file_path_absolute]\\n            original_snippet = editor.get_source_of(element_name)\\n            if not original_snippet:\\n                ui_manager.show_error(\\n                    f\\\"Element '{element_name}' in '{file_path_relative}' not found. Skipping.\\\"\\n                    )\\n                return False\\n            action_details = {'element_name': element_name, 'reason':\\n                reason, 'original_code': original_snippet}\\n            prompt = _create_prompt_for_refactor_action('MODIFY',\\n                file_path_relative, action_details)\\n    elif action_type == 'PARTIAL':\\n        element_name = action.get('element')\\n        reason = action.get('reason')\\n        line_start = action.get('line_start')\\n        line_end = action.get('line_end')\\n        if not all([element_name, line_start, line_end]):\\n            ui_manager.show_error(\\n                f'PARTIAL action missing required fields. Skipping: {action}')\\n            return False\\n        if file_path_relative.endswith('.py'):\\n            if file_path_absolute not in editors:\\n                try:\\n                    editors[file_path_absolute] = CodeEditor(file_path_absolute\\n                        )\\n                except Exception as e:\\n                    ui_manager.show_error(\\n                        f'Error loading file {file_path_absolute}: {e}')\\n                    return False\\n            editor = editors[file_path_absolute]\\n            original_snippet = editor.get_element_body_snippet(element_name,\\n                line_start, line_end)\\n            if not original_snippet:\\n                original_snippet = editor.get_source_of(element_name)\\n                if not original_snippet:\\n                    ui_manager.show_error(\\n                        f\\\"Element '{element_name}' in '{file_path_relative}' not found. Skipping.\\\"\\n                        )\\n                    return False\\n            full_element_code = editor.get_source_of(element_name)\\n            prompt = _create_prompt_for_partial_edit(file_path_relative,\\n                element_name, reason, original_snippet, line_start,\\n                line_end, full_element_code)\\n    elif action_type == 'CREATE':\\n        element_name = action.get('element_name')\\n        description = action.get('description')\\n        action_details = {'element_name': element_name, 'description':\\n            description}\\n        prompt = _create_prompt_for_refactor_action('CREATE',\\n            file_path_relative, action_details)\\n    else:\\n        ui_manager.show_error(f\\\"Invalid action type '{action_type}'. Skipping.\\\"\\n            )\\n        return False\\n    with ui_manager.show_spinner(\\n        f\\\"AI: {action_type} on '{element_name or file_path_relative}'...\\\"):\\n        response = query_llm(prompt)\\n    code_blocks = extract_code(response)\\n    new_content = code_blocks[0][1] if code_blocks else response.strip()\\n    if not new_content:\\n        ui_manager.show_error(\\n            f'AI failed to generate content for action: {action}')\\n        print(Panel(response, title=\\\"[yellow]AI's Raw Response[/]\\\"))\\n        return False\\n    if not file_path_relative.endswith('.py'):\\n        try:\\n            FileCreator.create(file_path_absolute, new_content)\\n            ui_manager.show_success(\\n                f\\\"File '{file_path_relative}' created/updated.\\\")\\n            return True\\n        except IOError as e:\\n            ui_manager.show_error(\\n                f\\\"Failed to create file '{file_path_relative}': {e}\\\")\\n            return False\\n    if file_path_absolute not in editors:\\n        try:\\n            if not os.path.exists(file_path_absolute):\\n                os.makedirs(os.path.dirname(file_path_absolute), exist_ok=True)\\n                with open(file_path_absolute, 'w') as f:\\n                    f.write('')\\n            editors[file_path_absolute] = CodeEditor(file_path_absolute)\\n        except Exception as e:\\n            ui_manager.show_error(\\n                f'Error loading file {file_path_absolute}: {e}')\\n            return False\\n    editor = editors[file_path_absolute]\\n    if action_type == 'MODIFY':\\n        if not editor.replace_element(element_name, new_content):\\n            ui_manager.show_error(\\n                f\\\"Failed to apply MODIFY change to '{element_name}'.\\\")\\n            print(Panel(new_content, title=\\n                f\\\"[red]Problematic MODIFY Code for '{element_name}'[/]\\\",\\n                border_style='red'))\\n            return False\\n    elif action_type == 'PARTIAL':\\n        if not editor.replace_partial(element_name, new_content, line_start,\\n            line_end):\\n            ui_manager.show_error(\\n                f\\\"Failed to apply PARTIAL change to '{element_name}'.\\\")\\n            print(Panel(new_content, title=\\n                f\\\"[red]Problematic PARTIAL Code for '{element_name}'[/]\\\",\\n                border_style='red'))\\n            return False\\n    elif action_type == 'CREATE':\\n        anchor = action.get('anchor_element')\\n        position = action.get('position', 'after')\\n        if not editor.add_element(new_content, anchor_name=anchor, before=\\n            position == 'before'):\\n            ui_manager.show_error(\\n                f\\\"Failed to apply CREATE change for '{element_name}'.\\\")\\n            print(Panel(new_content, title=\\n                f\\\"[red]Problematic CREATE Code for '{element_name}'[/]\\\",\\n                border_style='red'))\\n            return False\\n    return True\\n\\n\\ndef handle_project_refactor_command(instruction: str):\\n    \\\"\\\"\\\"\\n    Orchestrates a multi-file, multi-step code refactoring process.\\n    \\n    This function serves as a high-level orchestrator that delegates specific tasks\\n    to helper functions, improving readability, modularity, and maintainability.\\n    \\\"\\\"\\\"\\n    _load_all_project_files_if_needed()\\n    actions = _get_refactor_plan(instruction)\\n    if not actions:\\n        return\\n    plan = {'actions': actions}\\n    if not _display_and_confirm_plan(plan):\\n        return\\n    editors: Dict[str, CodeEditor] = {}\\n    project_base_path = memory_manager.get_project_root()\\n    successful_actions = 0\\n    total_actions = len(actions)\\n    for i, action in enumerate(actions, 1):\\n        ui_manager.show_success(f'Processing action {i}/{total_actions}...')\\n        action_type = action.get('type', '').upper()\\n        file_path_relative = action.get('file')\\n        if not file_path_relative:\\n            ui_manager.show_error(\\n                f\\\"Action is missing 'file' key. Skipping: {action}\\\")\\n            continue\\n        file_path_absolute = os.path.join(project_base_path, file_path_relative\\n            )\\n        if action_type == 'DELETE':\\n            element_name = action.get('element')\\n            if not element_name:\\n                ui_manager.show_error(\\n                    f\\\"DELETE action missing 'element' key. Skipping: {action}\\\")\\n                continue\\n            if not file_path_relative.endswith('.py'):\\n                ui_manager.show_error(\\n                    f'DELETE actions are only supported for Python files. Skipping.'\\n                    )\\n                continue\\n            if file_path_absolute not in editors:\\n                try:\\n                    editors[file_path_absolute] = CodeEditor(file_path_absolute\\n                        )\\n                except Exception as e:\\n                    ui_manager.show_error(\\n                        f'Error loading file {file_path_absolute}: {e}')\\n                    continue\\n            editor = editors[file_path_absolute]\\n            if editor.delete_element(element_name):\\n                successful_actions += 1\\n                ui_manager.show_success(\\n                    f\\\"Successfully deleted '{element_name}' from '{file_path_relative}'.\\\"\\n                    )\\n            else:\\n                ui_manager.show_error(\\n                    f\\\"Failed to delete '{element_name}' from '{file_path_relative}'.\\\"\\n                    )\\n        elif _process_refactor_action(action, project_base_path, editors):\\n            successful_actions += 1\\n        else:\\n            ui_manager.show_error(\\n                f'Action {i} failed, continuing with remaining actions...')\\n    if successful_actions == 0:\\n        ui_manager.show_error('No actions were successfully executed.')\\n        return\\n    elif successful_actions < total_actions:\\n        ui_manager.show_error(\\n            f'Only {successful_actions}/{total_actions} actions completed successfully.'\\n            )\\n    else:\\n        ui_manager.show_success(\\n            f'All {total_actions} actions completed successfully.')\\n    _apply_refactor_changes(editors)\\n\\n\\ndef _create_prompt_for_commit_message(diff: str) ->str:\\n    \\\"\\\"\\\"\\n    Create a dedicated prompt function for the 'commit' command. This prompt will\\n    instruct the AI to analyze a git diff and generate a concise commit message\\n    following the Conventional Commits standard.\\n    \\\"\\\"\\\"\\n    return f\\\"\\\"\\\"You are an expert developer writing a Git commit message. Your task is to analyze the provided git diff and create a professional commit message.\\n\\nCOMMIT MESSAGE RULES:\\n- Follow the Conventional Commits specification\\n- Format: <type>(<optional scope>): <subject>\\n- Types: feat, fix, docs, style, refactor, perf, test, build, ci, chore, revert\\n- Subject line: max 50 characters, imperative mood, no period\\n- Optional body: explain what and why (not how), wrap at 72 characters\\n- Be specific and concise\\n- Focus on the intent and impact of the changes\\n\\nEXAMPLES OF GOOD COMMIT MESSAGES:\\n- feat(auth): add OAuth2 integration for Google login\\n- fix(api): handle null response in user endpoint\\n- refactor(database): optimize query performance for large datasets\\n- docs(readme): update installation instructions for Windows\\n\\nRespond with ONLY the commit message - no markdown, quotes, or explanations.\\n\\n--- GIT DIFF TO ANALYZE ---\\n{diff}\\n\\nGenerate the commit message:\\\"\\\"\\\"\\n\\n\\ndef handle_commit_command():\\n    \\\"\\\"\\\"\\n    Orchestrates an AI-assisted Git commit workflow with improved error handling.\\n    \\\"\\\"\\\"\\n    project_root = memory_manager.get_project_root()\\n    if not project_root:\\n        ui_manager.show_error(\\n            \\\"No project context in memory. Use 'look <directory>' first.\\\")\\n        return\\n    try:\\n        git_manager = GitManager(project_root)\\n    except ValueError as e:\\n        ui_manager.show_error(str(e))\\n        return\\n    changed_files = git_manager.get_changed_files()\\n    if not changed_files:\\n        ui_manager.show_success(\\n            'No changes to commit. Everything is up to date.')\\n        return\\n    staged_diff = git_manager.get_diff(staged=True)\\n    unstaged_diff = git_manager.get_diff()\\n    full_diff = f'{staged_diff}\\\\n{unstaged_diff}'.strip()\\n    if not full_diff.strip():\\n        ui_manager.show_success(\\n            'No content changes detected (e.g., only file mode changes).')\\n        return\\n    prompt = _create_prompt_for_commit_message(full_diff)\\n    commit_message = query_llm(prompt).strip()\\n    if not commit_message:\\n        ui_manager.show_error(\\n            'AI failed to generate a commit message. Aborting.')\\n        return\\n    files_to_commit_str = '\\\\n'.join(f'- {f}' for f in changed_files)\\n    plan_panel_content = f\\\"\\\"\\\"[bold]Files to be staged:[/]\\n[yellow]{files_to_commit_str}[/]\\n\\n[bold]AI-Generated Commit Message:[/]\\n[green]{commit_message}[/]\\\"\\\"\\\"\\n    print(Panel(plan_panel_content, title='[bold cyan]Commit Plan[/]',\\n        border_style='cyan'))\\n    if ui_manager.get_user_input('\\\\nProceed with commit? (y/n): ').lower() in [\\n        'yes', 'y']:\\n        try:\\n            git_manager.add(changed_files)\\n            ui_manager.show_success('\\u2705 Files staged.')\\n        except subprocess.CalledProcessError as e:\\n            ui_manager.show_error(f'Staging failed: {e.stderr}')\\n            return\\n        try:\\n            git_manager.commit(commit_message)\\n            ui_manager.show_success('\\u2705 Commit successful.')\\n        except subprocess.CalledProcessError as e:\\n            ui_manager.show_error(f'Commit failed: {e.stderr}')\\n            return\\n        if ui_manager.get_user_input('Push changes to remote? (y/n): ').lower(\\n            ) in ['yes', 'y']:\\n            try:\\n                git_manager.push()\\n                ui_manager.show_success('\\u2705 Push successful.')\\n            except subprocess.CalledProcessError as e:\\n                ui_manager.show_error(f'Push failed: {e.stderr}')\\n        else:\\n            ui_manager.show_error('Push cancelled.')\\n    else:\\n        ui_manager.show_error('Commit aborted by user.')\\n\\n\\ndef handle_rag_query_command(query: str):\\n    \\\"\\\"\\\"\\n    Handles RAG query commands in the CLI.\\n    \\n    This function provides a way to query the RAG system from the command line interface.\\n    It loads the RAG manager, performs the query, and displays the results.\\n    \\n    Args:\\n        query: The query string to search for in the RAG system.\\n    \\\"\\\"\\\"\\n    try:\\n        rag_manager = RAGManager()\\n        if rag_manager.get_document_count() == 0:\\n            ui_manager.show_error('RAG index is empty. Add documents first.')\\n            return\\n        results = rag_manager.search(query, k=3)\\n        if not results:\\n            ui_manager.show_error('No relevant documents found.')\\n            return\\n        print(Panel(f'[bold cyan]RAG Query:[/bold cyan] {query}', title=\\n            '[bold]Retrieval-Augmented Generation Results[/bold]',\\n            border_style='cyan'))\\n        for i, (doc, score, metadata) in enumerate(results, 1):\\n            file_info = metadata.get('file', 'Unknown source')\\n            content_preview = doc[:200] + '...' if len(doc) > 200 else doc\\n            result_panel = Panel(\\n                f\\\"\\\"\\\"[dim]Source:[/] {file_info}\\n[dim]Relevance:[/] {score:.4f}\\n\\n{content_preview}\\\"\\\"\\\"\\n                , title=f'[bold]Result {i}[/bold]', border_style='blue',\\n                expand=False)\\n            print(result_panel)\\n        if ui_manager.get_user_input(\\n            '\\\\nGenerate detailed response with AI? (y/n): ').lower() in ['yes',\\n            'y']:\\n            context = '\\\\n\\\\n'.join([\\n                f'Document {i} (Score: {score:.4f}):\\\\n{doc}' for i, (doc,\\n                score, _) in enumerate(results, 1)])\\n            prompt = f\\\"\\\"\\\"Based on the following retrieved documents, please answer the query: \\\"{query}\\\"\\n\\nRetrieved Documents:\\n{context}\\n\\nPlease provide a comprehensive answer based only on the information in the documents above.\\nIf the documents don't contain enough information to answer the query, please say so.\\\"\\\"\\\"\\n            with ui_manager.show_spinner('AI is generating response...'):\\n                response = query_llm(prompt)\\n            print(Panel(response, title=\\n                '[bold green]AI-Generated Response[/bold green]',\\n                border_style='green'))\\n    except Exception as e:\\n        ui_manager.show_error(f'Error processing RAG query: {e}')\\n        if os.getenv('OMNIFORGE_DEBUG'):\\n            import traceback\\n            traceback.print_exc()\\n\\n\\ndef handle_rag_query_command(query: str):\\n    \\\"\\\"\\\"\\n    Handles RAG query commands in the CLI.\\n    \\n    This function provides a way to query the RAG system from the command line interface.\\n    It loads the RAG manager, performs the query, and displays the results.\\n    \\n    Args:\\n        query: The query string to search for in the RAG system.\\n    \\\"\\\"\\\"\\n    try:\\n        rag_manager = RAGManager()\\n        if rag_manager.get_document_count() == 0:\\n            ui_manager.show_error('RAG index is empty. Add documents first.')\\n            return\\n        results = rag_manager.search(query, k=3)\\n        if not results:\\n            ui_manager.show_error('No relevant documents found.')\\n            return\\n        print(Panel(f'[bold cyan]RAG Query:[/bold cyan] {query}', title=\\n            '[bold]Retrieval-Augmented Generation Results[/bold]',\\n            border_style='cyan'))\\n        for i, (doc, score, metadata) in enumerate(results, 1):\\n            file_info = metadata.get('file', 'Unknown source')\\n            content_preview = doc[:200] + '...' if len(doc) > 200 else doc\\n            result_panel = Panel(\\n                f\\\"\\\"\\\"[dim]Source:[/] {file_info}\\n[dim]Relevance:[/] {score:.4f}\\n\\n{content_preview}\\\"\\\"\\\"\\n                , title=f'[bold]Result {i}[/bold]', border_style='blue',\\n                expand=False)\\n            print(result_panel)\\n        if ui_manager.get_user_input(\\n            '\\\\nGenerate detailed response with AI? (y/n): ').lower() in ['yes',\\n            'y']:\\n            context = '\\\\n\\\\n'.join([\\n                f'Document {i} (Score: {score:.4f}):\\\\n{doc}' for i, (doc,\\n                score, _) in enumerate(results, 1)])\\n            prompt = f\\\"\\\"\\\"Based on the following retrieved documents, please answer the query: \\\"{query}\\\"\\n\\nRetrieved Documents:\\n{context}\\n\\nPlease provide a comprehensive answer based only on the information in the documents above.\\nIf the documents don't contain enough information to answer the query, please say so.\\\"\\\"\\\"\\n            with ui_manager.show_spinner('AI is generating response...'):\\n                response = query_llm(prompt)\\n            print(Panel(response, title=\\n                '[bold green]AI-Generated Response[/bold green]',\\n                border_style='green'))\\n    except Exception as e:\\n        ui_manager.show_error(f'Error processing RAG query: {e}')\\n        if os.getenv('OMNIFORGE_DEBUG'):\\n            import traceback\\n            traceback.print_exc()\\n\\n\\ndef interactive_mode() ->None:\\n    global last_query, last_response, last_code\\n    try:\\n        from Testing.overlay_engine import show_sequential_popup\\n        gui_available = True\\n    except ImportError:\\n        gui_available = False\\n    print(Panel(\\n        \\\"\\\"\\\"[bold cyan]Omni Interactive Mode[/]\\n[dim]Type 'help' for commands, 'exit' to quit.[/dim]\\\"\\\"\\\"\\n        , border_style='cyan'))\\n    personality_name = personality_manager.get_current_personality().get('name'\\n        , 'Default')\\n    gui_enabled = False\\n    if gui_available:\\n        try:\\n            with open(CONFIG_FILE, 'r') as f:\\n                config = json.load(f)\\n                if config.get('gui_enabled', False):\\n                    gui_enabled = True\\n        except (FileNotFoundError, json.JSONDecodeError):\\n            pass\\n    refresh_status_panel(personality_name)\\n    while True:\\n        try:\\n            user_input = ui_manager.get_user_input('\\\\n> ')\\n            if not user_input:\\n                continue\\n            command, *args = user_input.split(maxsplit=1)\\n            arg_str = args[0] if args else ''\\n            if command == 'exit':\\n                memory_manager.save_memory()\\n                print('[bold cyan]Goodbye![/]')\\n                break\\n            elif command == 'help':\\n                print(\\n                    \\\"\\\"\\\"[bold]Commands:[/]\\n\\n  [bold cyan]Core & Project Commands[/]\\n  [yellow]send <prompt>[/]        - Ask the LLM a question.\\n  [yellow]look <path>[/]          - Read file or scan directory into memory.\\n  [yellow]look_all[/]            - Recursively scan the project directory into memory.\\n  [yellow]create <file> \\\"instr\\\"[/] - Create a new file using AI.\\n  [yellow]edit <file> \\\"instr\\\"[/]   - Edit a specific file using AI.\\n  [yellow]refactor \\\"instr\\\"[/]      - Refactor project in memory based on instruction.\\n  [yellow]commit[/]               - Commit changes with an AI-generated message.\\n  [yellow]rag <query>[/]         - Query the RAG system for context retrieval.\\n\\n  [bold cyan]File & Code Management[/]\\n  [yellow]save <filename>[/]     - Save last AI response to a file.\\n  [yellow]list[/]               - List saved files.\\n  [yellow]run[/]                 - Run the last generated Python code.\\n\\n  [bold cyan]Session & Config[/]\\n  [yellow]history[/]             - Show the full chat history.\\n  [yellow]memory clear[/]        - Clear the chat and file memory.\\n  [yellow]backend <name>[/]      - Switch AI backend (e.g., openrouter, ollama).\\n  [yellow]models [src][/]        - Interactively list and select models.\\n  [yellow]set model <id>[/]      - Set the model directly by its ID.\\n  [yellow]personality <cmd>[/]   - Manage AI personalities ('list', 'set', 'add').\\n\\\"\\\"\\\"\\n                    )\\n            elif command == 'send':\\n                last_query = arg_str\\n                response = query_llm(arg_str)\\n                last_response = response\\n                if gui_enabled:\\n                    threading.Thread(target=show_sequential_popup, args=(\\n                        100, 100, response, f'Omni - {personality_name}'),\\n                        daemon=True).start()\\n                print(Panel(response, title='[cyan]Response[/]'))\\n                if (code_blocks := extract_code(response)):\\n                    last_code = code_blocks[0][1]\\n            elif command == 'look':\\n                look_command(arg_str)\\n            elif command == 'look_all':\\n                look_all_command()\\n            elif command == 'create':\\n                try:\\n                    file_path, instruction = arg_str.split(' ', 1)\\n                    handle_file_create_command(file_path.strip('\\\"'),\\n                        instruction.strip('\\\"'))\\n                except (ValueError, IndexError):\\n                    ui_manager.show_error(\\n                        'Usage: create <file_path> \\\"<instruction>\\\"')\\n            elif command == 'edit':\\n                try:\\n                    file_path, instruction = arg_str.split(' ', 1)\\n                    handle_file_edit_command(file_path.strip('\\\"'),\\n                        instruction.strip('\\\"'))\\n                except (ValueError, IndexError):\\n                    ui_manager.show_error(\\n                        'Usage: edit <file_path> \\\"<instruction>\\\"')\\n            elif command == 'refactor':\\n                if not arg_str:\\n                    ui_manager.show_error('Usage: refactor \\\"<instruction>\\\"')\\n                else:\\n                    handle_project_refactor_command(arg_str.strip('\\\"'))\\n            elif command == 'commit':\\n                handle_commit_command()\\n            elif command == 'models':\\n                list_models(arg_str.split())\\n            elif command == 'set' and arg_str.startswith('model '):\\n                set_model(arg_str[6:])\\n            elif command == 'backend':\\n                switch_backend(arg_str)\\n            elif command == 'history':\\n                ui_manager.display_history(memory_manager.get_memory_context())\\n            elif command == 'memory' and arg_str == 'clear':\\n                memory_manager.clear_memory()\\n                ui_manager.show_success('\\u2705 Memory cleared')\\n            elif command == 'personality':\\n                p_args = arg_str.split(maxsplit=1)\\n                cmd = p_args[0] if p_args else ''\\n                p_arg_str = p_args[1] if len(p_args) > 1 else ''\\n                if cmd == 'list':\\n                    for p in personality_manager.list_personalities():\\n                        print(f\\\"- {p['name']}: {p['description']}\\\")\\n                elif cmd == 'set' and p_arg_str:\\n                    if personality_manager.set_current_personality(p_arg_str):\\n                        personality_name = p_arg_str\\n                        ui_manager.show_success(\\n                            f'Set personality to {personality_name}')\\n                    else:\\n                        ui_manager.show_error('Personality not found.')\\n                else:\\n                    ui_manager.show_error(\\n                        \\\"Invalid personality command. Use 'list' or 'set <name>'.\\\"\\n                        )\\n            elif command == 'run':\\n                run_python_code()\\n            elif command == 'save':\\n                if last_response:\\n                    save_code(last_response, arg_str or\\n                        f\\\"omni_save_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\\\"\\n                        )\\n                else:\\n                    ui_manager.show_error('No response to save.')\\n            elif command == 'list':\\n                files = sorted(os.listdir(DEFAULT_SAVE_DIR))\\n                print('\\\\n'.join(f'  - {f}' for f in files) if files else\\n                    '[yellow]No saved files.[/]')\\n            elif command == 'rag':\\n                if not arg_str:\\n                    ui_manager.show_error('Usage: rag \\\"<query>\\\"')\\n                else:\\n                    from rag_manager import RAGManager\\n                    project_root = memory_manager.get_project_root()\\n                    if not project_root:\\n                        ui_manager.show_error(\\n                            \\\"No project context in memory. Use 'look <directory>' first.\\\"\\n                            )\\n                        continue\\n                    rag = RAGManager()\\n                    if rag.get_document_count() == 0:\\n                        ui_manager.show_error(\\n                            'RAG index is empty. Please add documents first.')\\n                        continue\\n                    results = rag.search(arg_str, k=3)\\n                    if not results:\\n                        ui_manager.show_error('No relevant documents found.')\\n                        continue\\n                    print(Panel('[bold]RAG Results:[/]', title=\\n                        '[cyan]Retrieval-Augmented Generation[/]'))\\n                    for i, (content, score, metadata) in enumerate(results, 1):\\n                        file_path = metadata.get('file', 'Unknown')\\n                        print(\\n                            f'[bold cyan]{i}. {file_path}[/] (Score: {score:.4f})'\\n                            )\\n                        print(Panel(content[:500] + '...' if len(content) >\\n                            500 else content, border_style='dim'))\\n                    follow_up = ui_manager.get_user_input(\\n                        \\\"\\\"\\\"\\nWould you like to ask a follow-up question with this context? (y/n): \\\"\\\"\\\"\\n                        )\\n                    if follow_up.lower() in ['y', 'yes']:\\n                        follow_up_query = ui_manager.get_user_input(\\n                            'Follow-up query: ')\\n                        if follow_up_query:\\n                            context_parts = [\\n                                f'Document {i} (Score: {score:.4f}):\\\\n{content}'\\n                                 for i, (content, score, _) in enumerate(\\n                                results, 1)]\\n                            context = '\\\\n\\\\n'.join(context_parts)\\n                            rag_prompt = f\\\"\\\"\\\"Based on the following context, please answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {follow_up_query}\\\"\\\"\\\"\\n                            response = query_llm(rag_prompt)\\n                            print(Panel(response, title=\\n                                '[cyan]RAG-Augmented Response[/]'))\\n            else:\\n                ui_manager.show_error(\\\"Unknown command. Type 'help'.\\\")\\n            refresh_status_panel(personality_name)\\n        except KeyboardInterrupt:\\n            memory_manager.save_memory()\\n            print('\\\\n[bold cyan]Goodbye![/]')\\n            break\\n        except Exception as e:\\n            ui_manager.show_error(f'An unexpected error occurred: {e}')\\n\\n\\ndef run_python_code() ->None:\\n    global last_code\\n    if not last_code:\\n        ui_manager.show_error('No Python code in memory to run.')\\n        return\\n    temp_file = os.path.join(DEFAULT_SAVE_DIR, 'temp_run.py')\\n    try:\\n        with open(temp_file, 'w') as f:\\n            f.write(last_code)\\n        print('[bold cyan]\\\\n--- Running Code ---\\\\n[/]')\\n        subprocess.run([sys.executable, temp_file], check=True)\\n        print('[bold cyan]\\\\n--- Code Finished ---\\\\n[/]')\\n    except Exception as e:\\n        ui_manager.show_error(f'Error running code: {e}')\\n    finally:\\n        if os.path.exists(temp_file):\\n            os.remove(temp_file)\\n\\n\\ndef save_code(content: str, filename: str) ->None:\\n    filepath = os.path.join(DEFAULT_SAVE_DIR, filename)\\n    try:\\n        with open(filepath, 'w') as f:\\n            f.write(content)\\n        ui_manager.show_success(f'Saved to: {filepath}')\\n    except IOError as e:\\n        ui_manager.show_error(f'Error saving file: {e}')\\n\\n\\ndef main() ->None:\\n    try:\\n        import astor\\n    except ImportError:\\n        print(\\\"[bold red]Error:[/] 'astor' is required. `pip install astor`\\\")\\n        sys.exit(1)\\n    try:\\n        import simple_term_menu\\n    except ImportError:\\n        print(\\n            \\\"[bold red]Error:[/] 'simple-term-menu' is required. `pip install simple-term-menu`\\\"\\n            )\\n        sys.exit(1)\\n    parser = argparse.ArgumentParser(description=\\n        'Omni - AI-powered code tool', add_help=False)\\n    parser.add_argument('command', nargs='?', help='Main command.')\\n    parser.add_argument('args', nargs='*', help='Arguments for the command.')\\n    parser.add_argument('-h', '--help', action='store_true')\\n    args, _ = parser.parse_known_args()\\n    if args.help or not args.command:\\n        interactive_mode()\\n    elif args.command == 'look' and args.args:\\n        look_command(args.args[0])\\n    elif args.command == 'edit' and len(args.args) >= 2:\\n        handle_file_edit_command(args.args[0], ' '.join(args.args[1:]))\\n    elif args.command == 'models':\\n        list_models(args.args)\\n    else:\\n        interactive_mode()\\n\\n\\ndef refresh_status_panel(personality_name: str) ->None:\\n    ui_manager.display_status_panel(personality_name, current_backend,\\n        current_model, len(memory_manager.memory.get('chat', [])), len(\\n        memory_manager.memory.get('look', [])))\\n\\n\\nif __name__ == '__main__':\\n    main()\\n\",\n    \"file\": \"/mnt/ProjectData/omni/omni.py\"\n  },\n  {\n    \"id\": 9,\n    \"hash\": \"392cb7cce7185587da5fc1b6a30c1fef\",\n    \"content\": \"import json\\nimport os\\nfrom typing import List, Dict, Optional\\nfrom rag_manager import RAGManager\\n\\n\\nclass MemoryManager:\\n    \\\"\\\"\\\"Manages persistent chat memory, look data, and RAG integration via JSON.\\\"\\\"\\\"\\n\\n    def __init__(self, memory_file: str):\\n        self.memory_file = memory_file\\n        self.memory: Dict[str, List] = self.load_memory()\\n        self.rag_manager = RAGManager()\\n\\n    def load_memory(self) ->Dict[str, List]:\\n        try:\\n            with open(self.memory_file, 'r') as f:\\n                return json.load(f)\\n        except FileNotFoundError:\\n            default = {'chat': [], 'look': []}\\n            self.save_memory(default)\\n            return default\\n        except json.JSONDecodeError:\\n            print('[yellow]Invalid memory file. Resetting.[/]')\\n            return {'chat': [], 'look': []}\\n\\n    def save_memory(self, memory: Optional[Dict[str, List]]=None) ->None:\\n        if memory is None:\\n            memory = self.memory\\n        with open(self.memory_file, 'w') as f:\\n            json.dump(memory, f, indent=4)\\n\\n    def add_message(self, role: str, content: str) ->None:\\n        self.memory['chat'].append({'role': role, 'content': content})\\n        self.save_memory()\\n\\n    def add_look_data(self, file_path: str, content: str) ->None:\\n        \\\"\\\"\\\"\\n    Adds a watched item (directory or file) to memory, distinguishing its type.\\n\\n    This method stores structured data that differentiates between a project\\n    directory (containing a manifest) and a single file (containing its content).\\n    It also prevents duplicate entries by updating existing ones.\\n\\n    Args:\\n        file_path: The path to the directory or file.\\n        content: The manifest for a directory or the content for a file.\\n    \\\"\\\"\\\"\\n        item_type = 'directory' if os.path.isdir(file_path) else 'file'\\n        for item in self.memory['look']:\\n            if item.get('file') == file_path:\\n                item['content'] = content\\n                item['type'] = item_type\\n                self.save_memory()\\n                return\\n        self.memory['look'].append({'type': item_type, 'file': file_path,\\n            'content': content})\\n        self.save_memory()\\n        if item_type == 'file':\\n            try:\\n                with open(file_path, 'r', encoding='utf-8') as f:\\n                    file_content = f.read()\\n                self.rag_manager.add_documents([file_content], [{'file':\\n                    file_path}])\\n            except Exception as e:\\n                print(\\n                    f'[yellow]Warning: Could not add {file_path} to RAG index: {e}[/]'\\n                    )\\n\\n    def get_memory_context(self) -> str:\\n        \\\"\\\"\\\"\\n        Dynamically builds the context using RAG and chat history.\\n\\n        It retrieves relevant file content from the RAG index based on the latest\\n        user query, includes project manifests for directories, and appends the\\n        recent chat history.\\n        \\\"\\\"\\\"\\n        context = ''\\n        # 1. Add project manifests for any watched directories\\n        for look in self.memory.get('look', []):\\n            path = look.get('file')\\n            if path and os.path.isdir(path):\\n                content = look.get('content', '')\\n                context += f'--- Project Manifest for {path} ---\\\\n{content}\\\\n\\\\n'\\n\\n        # 2. Find the last user message to use as a query for the RAG system\\n        last_user_message = next((msg['content'] for msg in reversed(self.memory.get('chat', [])) if msg['role'] == 'user'), None)\\n\\n        # 3. If a user message exists, search the RAG index for relevant context\\n        if last_user_message:\\n            rag_results = self.search_rag(last_user_message, k=3)\\n            if rag_results:\\n                context += '--- Relevant context from RAG ---\\\\n'\\n                # Format and add each RAG result to the context\\n                for doc, score, meta in rag_results:\\n                    file_path = meta.get('file', 'Unknown source')\\n                    context += f'Source: {file_path} (Score: {score:.4f})\\\\n'\\n                    context += f'Content: {doc}\\\\n---\\\\n'\\n                context += '\\\\n'\\n\\n        # 4. Append the full chat history for conversational context\\n        for msg in self.memory.get('chat', []):\\n            context += f\\\"{msg['role'].capitalize()}: {msg['content']}\\\\n\\\"\\n\\n        return context.strip()\\n\\n    def clear_memory(self) ->None:\\n        self.memory = {'chat': [], 'look': []}\\n        self.save_memory()\\n        self.rag_manager.clear_index()\\n\\n    def search_rag(self, query: str, k: int=3) ->List[tuple]:\\n        \\\"\\\"\\\"\\n        Search the RAG index for relevant documents.\\n        \\n        Args:\\n            query: The search query\\n            k: Number of results to return\\n            \\n        Returns:\\n            List of (document_content, score, metadata) tuples\\n        \\\"\\\"\\\"\\n        return self.rag_manager.search(query, k)\\n\",\n    \"file\": \"/mnt/ProjectData/omni/memory_manager.py\"\n  },\n  {\n    \"id\": 10,\n    \"hash\": \"2914352eefed9d60de098e021dd79b67\",\n    \"content\": \"import os\\nimport sys\\nimport argparse\\nfrom typing import List\\nfrom rag_manager import RAGManager\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__),\\n    '..')))\\n\\n\\ndef create_sample_data() ->List[str]:\\n    \\\"\\\"\\\"Create sample documents for the RAG example.\\\"\\\"\\\"\\n    return [\\n        'RAG stands for Retrieval-Augmented Generation, a technique that combines information retrieval with language generation.'\\n        ,\\n        'The RAG model retrieves relevant documents from a knowledge base and uses them to generate more accurate responses.'\\n        ,\\n        'FAISS is a library for efficient similarity search and clustering of dense vectors, often used in RAG systems.'\\n        ,\\n        'Sentence transformers are used to create embeddings for documents and queries in RAG systems.'\\n        ,\\n        'Retrieval-Augmented Generation improves language models by allowing them to access external knowledge sources.'\\n        ,\\n        'In RAG systems, documents are indexed and searchable by their semantic embeddings rather than just keywords.'\\n        ,\\n        'Python is a great language for implementing RAG systems due to libraries like sentence-transformers and faiss.'\\n        ,\\n        'The retrieval component of RAG is crucial for finding relevant information before generation.'\\n        ]\\n\\n\\ndef main():\\n    \\\"\\\"\\\"Main function demonstrating RAG through CLI.\\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(description='RAG CLI Example')\\n    parser.add_argument('--query', '-q', type=str, help='Query to search for')\\n    parser.add_argument('--add', '-a', type=str, help=\\n        'Add a new document to the index')\\n    parser.add_argument('--list', '-l', action='store_true', help=\\n        'List all documents in the index')\\n    parser.add_argument('--clear', '-c', action='store_true', help=\\n        'Clear the index')\\n    parser.add_argument('--init', '-i', action='store_true', help=\\n        'Initialize with sample data')\\n    args = parser.parse_args()\\n    rag_manager = RAGManager()\\n    if args.clear:\\n        rag_manager.clear_index()\\n        print('Index cleared.')\\n        return\\n    if args.init:\\n        documents = create_sample_data()\\n        rag_manager.add_documents(documents)\\n        print(f'Added {len(documents)} sample documents to index.')\\n        return\\n    if args.add:\\n        rag_manager.add_documents([args.add])\\n        print(f'Added document: {args.add}')\\n        return\\n    if args.list:\\n        if rag_manager.get_document_count() == 0:\\n            print(\\n                'No documents in index. Use --init to add sample data or --add to add documents.'\\n                )\\n        else:\\n            print(\\n                f'Documents in index ({rag_manager.get_document_count()} total):'\\n                )\\n            for i, meta in enumerate(rag_manager.metadata):\\n                print(f\\\"  {i + 1}. {meta['content']}\\\")\\n        return\\n    if args.query:\\n        if rag_manager.get_document_count() == 0:\\n            print(\\n                'Index is empty. Use --init to add sample data or --add to add documents.'\\n                )\\n            return\\n        results = rag_manager.search(args.query, k=3)\\n        print(f\\\"Top 3 results for '{args.query}':\\\")\\n        for i, (doc, score, meta) in enumerate(results, 1):\\n            print(f'  {i}. [Score: {score:.4f}] {doc}')\\n        return\\n    parser.print_help()\\n\\n\\ndef main():\\n    \\\"\\\"\\\"Main function demonstrating RAG through CLI.\\\"\\\"\\\"\\n    parser = argparse.ArgumentParser(description='RAG CLI Example')\\n    parser.add_argument('--query', '-q', type=str, help='Query to search for')\\n    parser.add_argument('--add', '-a', type=str, help=\\n        'Add a new document to the index')\\n    parser.add_argument('--list', '-l', action='store_true', help=\\n        'List all documents in the index')\\n    parser.add_argument('--clear', '-c', action='store_true', help=\\n        'Clear the index')\\n    parser.add_argument('--init', '-i', action='store_true', help=\\n        'Initialize with sample data')\\n    args = parser.parse_args()\\n    rag_manager = RAGManager()\\n    if args.clear:\\n        rag_manager.clear_index()\\n        print('Index cleared.')\\n        return\\n    if args.init:\\n        documents = create_sample_data()\\n        rag_manager.add_documents(documents)\\n        print(f'Added {len(documents)} sample documents to index.')\\n        return\\n    if args.add:\\n        rag_manager.add_documents([args.add])\\n        print(f'Added document: {args.add}')\\n        return\\n    if args.list:\\n        if rag_manager.get_document_count() == 0:\\n            print(\\n                'No documents in index. Use --init to add sample data or --add to add documents.'\\n                )\\n        else:\\n            print(\\n                f'Documents in index ({rag_manager.get_document_count()} total):'\\n                )\\n            for i, meta in enumerate(rag_manager.metadata):\\n                print(f\\\"  {i + 1}. {meta['content']}\\\")\\n        return\\n    if args.query:\\n        if rag_manager.get_document_count() == 0:\\n            print(\\n                'Index is empty. Use --init to add sample data or --add to add documents.'\\n                )\\n            return\\n        results = rag_manager.search(args.query, k=3)\\n        print(f\\\"Top 3 results for '{args.query}':\\\")\\n        for i, (doc, score, meta) in enumerate(results, 1):\\n            print(f'  {i}. [Score: {score:.4f}] {doc}')\\n        return\\n    parser.print_help()\\n\\n\\nif __name__ == '__main__':\\n    main()\\n\",\n    \"file\": \"/mnt/ProjectData/omni/rag_cli_example.py\"\n  },\n  {\n    \"id\": 11,\n    \"hash\": \"f59a27842056dbb0dc95d0f29c1c5de8\",\n    \"content\": \"Hello! I'm OmniForge, an AI coding assistant. I can help you scan, understand, and refactor your codebase using local (Ollama) or remote (OpenRouter) LLMs.\\n\\nI see you've opened the OmniForge project directory. Some key things I can help with:\\n\\n1. **Project Analysis**: I can look at your files to understand the structure\\n2. **Code Editing**: I can make precise edits to functions/classes using AST-based transformations\\n3. **Refactoring**: I can perform multi-file architectural changes with a plan\\n4. **File Creation**: I can generate new files based on instructions\\n\\nFor example:\\n- `look .` to scan the project\\n- `edit code_editor.py \\\"add docstrings to main methods\\\"`\\n- `refactor \\\"extract git operations into a separate module\\\"`\\n- `create new_module.py \\\"implement a simple HTTP client\\\"`\\n\\nWhat would you like to do with this project?\",\n    \"file\": \"/mnt/ProjectData/omni/requirements.txt\"\n  },\n  {\n    \"id\": 12,\n    \"hash\": \"dbd143839288b400cafdedb655258434\",\n    \"content\": \"import os\\nimport json\\nimport hashlib\\nfrom typing import List, Dict, Optional, Tuple\\nfrom sentence_transformers import SentenceTransformer\\nimport numpy as np\\nimport faiss\\nfrom pathlib import Path\\n\\n\\nclass VectorDBManager:\\n    \\\"\\\"\\\"Manages vector database operations for RAG using sentence transformers and FAISS.\\\"\\\"\\\"\\n\\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\\n        Optional[str]=None):\\n        \\\"\\\"\\\"\\n        Initialize the VectorDB manager.\\n\\n        Args:\\n            model_name: Name of the sentence transformer model to use\\n            index_path: Path to save/load the FAISS index\\n        \\\"\\\"\\\"\\n        self.model = SentenceTransformer(model_name)\\n        self.index_path = index_path or 'vectordb_index.bin'\\n        self.metadata_path = self.index_path.replace('.bin', '_metadata.json')\\n        self.dimension = self.model.get_sentence_embedding_dimension()\\n        self.index = None\\n        self.metadata: List[Dict] = []\\n        self._initialize_index()\\n\\n    def _initialize_index(self):\\n        \\\"\\\"\\\"Initialize or load the FAISS index.\\\"\\\"\\\"\\n        if os.path.exists(self.index_path) and os.path.exists(self.\\n            metadata_path):\\n            self.index = faiss.read_index(self.index_path)\\n            with open(self.metadata_path, 'r') as f:\\n                self.metadata = json.load(f)\\n        else:\\n            self.index = faiss.IndexFlatIP(self.dimension)\\n            self.metadata = []\\n\\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\\n        Dict]]=None):\\n        \\\"\\\"\\\"\\n        Add documents to the vector database.\\n\\n        Args:\\n            documents: List of text documents to add\\n            metadatas: Optional list of metadata for each document\\n        \\\"\\\"\\\"\\n        if metadatas is None:\\n            metadatas = [{}] * len(documents)\\n        embeddings = self.model.encode(documents)\\n        faiss.normalize_L2(embeddings)\\n        self.index.add(embeddings.astype(np.float32))\\n        for i, meta in enumerate(metadatas):\\n            doc_hash = hashlib.md5(documents[i].encode()).hexdigest()\\n            meta_entry = {'id': len(self.metadata), 'hash': doc_hash,\\n                'content': documents[i], **meta}\\n            self.metadata.append(meta_entry)\\n        self._save_index()\\n\\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\\n        \\\"\\\"\\\"\\n        Search for relevant documents.\\n\\n        Args:\\n            query: Query string\\n            k: Number of results to return\\n\\n        Returns:\\n            List of (document, score, metadata) tuples\\n        \\\"\\\"\\\"\\n        query_embedding = self.model.encode([query])\\n        faiss.normalize_L2(query_embedding)\\n        scores, indices = self.index.search(query_embedding.astype(np.\\n            float32), k)\\n        results = []\\n        for score, idx in zip(scores[0], indices[0]):\\n            if idx < len(self.metadata):\\n                doc_info = self.metadata[idx]\\n                results.append((doc_info['content'], float(score), doc_info))\\n        return results\\n\\n    def _save_index(self):\\n        \\\"\\\"\\\"Save the FAISS index and metadata to disk.\\\"\\\"\\\"\\n        faiss.write_index(self.index, self.index_path)\\n        with open(self.metadata_path, 'w') as f:\\n            json.dump(self.metadata, f, indent=2)\\n\\n    def get_document_count(self) ->int:\\n        \\\"\\\"\\\"Get the number of documents in the index.\\\"\\\"\\\"\\n        return len(self.metadata)\\n\\n    def clear_index(self):\\n        \\\"\\\"\\\"Clear the index and metadata.\\\"\\\"\\\"\\n        self.index = faiss.IndexFlatIP(self.dimension)\\n        self.metadata = []\\n        self._save_index()\\n\\n\\nclass VectorDBManager:\\n    \\\"\\\"\\\"Manages vector database operations for RAG using sentence transformers and FAISS.\\\"\\\"\\\"\\n\\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\\n        Optional[str]=None):\\n        \\\"\\\"\\\"\\n        Initialize the VectorDB manager.\\n\\n        Args:\\n            model_name: Name of the sentence transformer model to use\\n            index_path: Path to save/load the FAISS index\\n        \\\"\\\"\\\"\\n        self.model = SentenceTransformer(model_name)\\n        self.index_path = index_path or 'vectordb_index.bin'\\n        self.metadata_path = self.index_path.replace('.bin', '_metadata.json')\\n        self.dimension = self.model.get_sentence_embedding_dimension()\\n        self.index = None\\n        self.metadata: List[Dict] = []\\n        self._initialize_index()\\n\\n    def _initialize_index(self):\\n        \\\"\\\"\\\"Initialize or load the FAISS index.\\\"\\\"\\\"\\n        if os.path.exists(self.index_path) and os.path.exists(self.\\n            metadata_path):\\n            self.index = faiss.read_index(self.index_path)\\n            with open(self.metadata_path, 'r') as f:\\n                self.metadata = json.load(f)\\n        else:\\n            self.index = faiss.IndexFlatIP(self.dimension)\\n            self.metadata = []\\n\\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\\n        Dict]]=None):\\n        \\\"\\\"\\\"\\n        Add documents to the vector database.\\n\\n        Args:\\n            documents: List of text documents to add\\n            metadatas: Optional list of metadata for each document\\n        \\\"\\\"\\\"\\n        if metadatas is None:\\n            metadatas = [{}] * len(documents)\\n        embeddings = self.model.encode(documents)\\n        faiss.normalize_L2(embeddings)\\n        self.index.add(embeddings.astype(np.float32))\\n        for i, meta in enumerate(metadatas):\\n            doc_hash = hashlib.md5(documents[i].encode()).hexdigest()\\n            meta_entry = {'id': len(self.metadata), 'hash': doc_hash,\\n                'content': documents[i], **meta}\\n            self.metadata.append(meta_entry)\\n        self._save_index()\\n\\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\\n        \\\"\\\"\\\"\\n        Search for relevant documents.\\n\\n        Args:\\n            query: Query string\\n            k: Number of results to return\\n\\n        Returns:\\n            List of (document, score, metadata) tuples\\n        \\\"\\\"\\\"\\n        query_embedding = self.model.encode([query])\\n        faiss.normalize_L2(query_embedding)\\n        scores, indices = self.index.search(query_embedding.astype(np.\\n            float32), k)\\n        results = []\\n        for score, idx in zip(scores[0], indices[0]):\\n            if idx < len(self.metadata):\\n                doc_info = self.metadata[idx]\\n                results.append((doc_info['content'], float(score), doc_info))\\n        return results\\n\\n    def _save_index(self):\\n        \\\"\\\"\\\"Save the FAISS index and metadata to disk.\\\"\\\"\\\"\\n        faiss.write_index(self.index, self.index_path)\\n        with open(self.metadata_path, 'w') as f:\\n            json.dump(self.metadata, f, indent=2)\\n\\n    def get_document_count(self) ->int:\\n        \\\"\\\"\\\"Get the number of documents in the index.\\\"\\\"\\\"\\n        return len(self.metadata)\\n\\n    def clear_index(self):\\n        \\\"\\\"\\\"Clear the index and metadata.\\\"\\\"\\\"\\n        self.index = faiss.IndexFlatIP(self.dimension)\\n        self.metadata = []\\n        self._save_index()\\n\\n\\nclass VectorDBManager:\\n    \\\"\\\"\\\"Manages vector database operations for RAG using sentence transformers and FAISS.\\\"\\\"\\\"\\n\\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\\n        Optional[str]=None):\\n        \\\"\\\"\\\"\\n        Initialize the VectorDB manager.\\n\\n        Args:\\n            model_name: Name of the sentence transformer model to use\\n            index_path: Path to save/load the FAISS index\\n        \\\"\\\"\\\"\\n        self.model = SentenceTransformer(model_name)\\n        self.index_path = index_path or 'vectordb_index.bin'\\n        self.metadata_path = self.index_path.replace('.bin', '_metadata.json')\\n        self.dimension = self.model.get_sentence_embedding_dimension()\\n        self.index = None\\n        self.metadata: List[Dict] = []\\n        self._initialize_index()\\n\\n    def _initialize_index(self):\\n        \\\"\\\"\\\"Initialize or load the FAISS index.\\\"\\\"\\\"\\n        if os.path.exists(self.index_path) and os.path.exists(self.\\n            metadata_path):\\n            self.index = faiss.read_index(self.index_path)\\n            with open(self.metadata_path, 'r') as f:\\n                self.metadata = json.load(f)\\n        else:\\n            self.index = faiss.IndexFlatIP(self.dimension)\\n            self.metadata = []\\n\\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\\n        Dict]]=None):\\n        \\\"\\\"\\\"\\n        Add documents to the vector database.\\n\\n        Args:\\n            documents: List of text documents to add\\n            metadatas: Optional list of metadata for each document\\n        \\\"\\\"\\\"\\n        if metadatas is None:\\n            metadatas = [{}] * len(documents)\\n        embeddings = self.model.encode(documents)\\n        faiss.normalize_L2(embeddings)\\n        self.index.add(embeddings.astype(np.float32))\\n        for i, meta in enumerate(metadatas):\\n            doc_hash = hashlib.md5(documents[i].encode()).hexdigest()\\n            meta_entry = {'id': len(self.metadata), 'hash': doc_hash,\\n                'content': documents[i], **meta}\\n            self.metadata.append(meta_entry)\\n        self._save_index()\\n\\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\\n        \\\"\\\"\\\"\\n        Search for relevant documents.\\n\\n        Args:\\n            query: Query string\\n            k: Number of results to return\\n\\n        Returns:\\n            List of (document, score, metadata) tuples\\n        \\\"\\\"\\\"\\n        query_embedding = self.model.encode([query])\\n        faiss.normalize_L2(query_embedding)\\n        scores, indices = self.index.search(query_embedding.astype(np.\\n            float32), k)\\n        results = []\\n        for score, idx in zip(scores[0], indices[0]):\\n            if idx < len(self.metadata):\\n                doc_info = self.metadata[idx]\\n                results.append((doc_info['content'], float(score), doc_info))\\n        return results\\n\\n    def _save_index(self):\\n        \\\"\\\"\\\"Save the FAISS index and metadata to disk.\\\"\\\"\\\"\\n        faiss.write_index(self.index, self.index_path)\\n        with open(self.metadata_path, 'w') as f:\\n            json.dump(self.metadata, f, indent=2)\\n\\n    def get_document_count(self) ->int:\\n        \\\"\\\"\\\"Get the number of documents in the index.\\\"\\\"\\\"\\n        return len(self.metadata)\\n\\n    def clear_index(self):\\n        \\\"\\\"\\\"Clear the index and metadata.\\\"\\\"\\\"\\n        self.index = faiss.IndexFlatIP(self.dimension)\\n        self.metadata = []\\n        self._save_index()\\n\",\n    \"file\": \"/mnt/ProjectData/omni/vectordb_manager.py\"\n  },\n  {\n    \"id\": 13,\n    \"hash\": \"c27b11d2eaa8e13cdf8e698417efb75e\",\n    \"content\": \"import os\\nimport sys\\nfrom typing import List, Dict, Tuple\\n\\\"\\\"\\\"\\nExample script demonstrating RAG (Retrieval-Augmented Generation) usage.\\n\\nThis script shows how to use a simple RAG implementation to answer questions\\nbased on a given context or knowledge base.\\n\\\"\\\"\\\"\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__),\\n    '..')))\\n\\n\\nclass SimpleRAG:\\n    \\\"\\\"\\\"A simple RAG implementation for demonstration purposes.\\\"\\\"\\\"\\n\\n    def __init__(self, knowledge_base: List[str]):\\n        \\\"\\\"\\\"\\n        Initialize the RAG with a knowledge base.\\n        \\n        Args:\\n            knowledge_base: A list of strings representing the knowledge base.\\n        \\\"\\\"\\\"\\n        self.knowledge_base = knowledge_base\\n\\n    def retrieve(self, query: str, top_k: int=3) ->List[str]:\\n        \\\"\\\"\\\"\\n        Retrieve relevant documents from the knowledge base.\\n        \\n        This is a simplified implementation using keyword matching.\\n        In a real implementation, you would use embeddings and vector search.\\n        \\n        Args:\\n            query: The query string.\\n            top_k: Number of top documents to retrieve.\\n            \\n        Returns:\\n            A list of relevant documents.\\n        \\\"\\\"\\\"\\n        query_words = set(query.lower().split())\\n        scores = []\\n        for doc in self.knowledge_base:\\n            doc_words = set(doc.lower().split())\\n            score = len(query_words.intersection(doc_words))\\n            scores.append((score, doc))\\n        scores.sort(reverse=True)\\n        return [doc for score, doc in scores[:top_k]]\\n\\n    def generate(self, query: str, retrieved_docs: List[str]) ->str:\\n        \\\"\\\"\\\"\\n        Generate an answer based on the query and retrieved documents.\\n        \\n        In a real implementation, this would use an LLM to generate the response.\\n        For this example, we'll create a simple template-based response.\\n        \\n        Args:\\n            query: The query string.\\n            retrieved_docs: The retrieved documents.\\n            \\n        Returns:\\n            A generated answer.\\n        \\\"\\\"\\\"\\n        context = '\\\\n'.join(retrieved_docs)\\n        prompt = f\\\"\\\"\\\"\\n        Context information:\\n        {context}\\n        \\n        Question: {query}\\n        \\n        Based on the context provided above, please answer the question.\\n        If the context doesn't contain relevant information, say so.\\n        \\\"\\\"\\\"\\n        return self._simple_response_generator(query, retrieved_docs)\\n\\n    def _simple_response_generator(self, query: str, retrieved_docs: List[str]\\n        ) ->str:\\n        \\\"\\\"\\\"\\n        A simple response generator for demonstration.\\n        \\\"\\\"\\\"\\n        for doc in retrieved_docs:\\n            if 'example' in doc.lower():\\n                return (\\n                    f'Based on the context provided, I found information about examples. {query} relates to the examples in the knowledge base.'\\n                    )\\n        return (\\n            f\\\"I couldn't find specific information about '{query}' in the provided context. Please provide more details or check the knowledge base.\\\"\\n            )\\n\\n    def query(self, query: str, top_k: int=3) ->str:\\n        \\\"\\\"\\\"\\n        Process a query through the full RAG pipeline.\\n        \\n        Args:\\n            query: The query string.\\n            top_k: Number of top documents to retrieve.\\n            \\n        Returns:\\n            The generated answer.\\n        \\\"\\\"\\\"\\n        retrieved_docs = self.retrieve(query, top_k)\\n        answer = self.generate(query, retrieved_docs)\\n        return answer\\n\\n\\ndef main():\\n    \\\"\\\"\\\"Main function demonstrating the RAG implementation.\\\"\\\"\\\"\\n    knowledge_base = ['This is an example document about machine learning.',\\n        'Natural language processing is a subfield of artificial intelligence.'\\n        , 'Python is a popular programming language for data science.',\\n        'The quick brown fox jumps over the lazy dog.',\\n        'RAG stands for Retrieval-Augmented Generation.',\\n        'Vector databases are used for similarity search in RAG systems.',\\n        'Transformers are a type of neural network architecture.',\\n        'This example shows how to implement a simple RAG system.']\\n    rag = SimpleRAG(knowledge_base)\\n    queries = ['What is RAG?', 'How is Python used in data science?',\\n        'Tell me about machine learning']\\n    print('Simple RAG Example')\\n    print('=' * 50)\\n    for query in queries:\\n        print(f'\\\\nQuery: {query}')\\n        answer = rag.query(query)\\n        print(f'Answer: {answer}')\\n        print('-' * 30)\\n    print(\\\"\\\\nInteractive Mode (type 'quit' to exit):\\\")\\n    while True:\\n        try:\\n            user_query = input('\\\\nEnter your question: ').strip()\\n            if user_query.lower() in ['quit', 'exit', 'q']:\\n                break\\n            if user_query:\\n                answer = rag.query(user_query)\\n                print(f'Answer: {answer}')\\n        except KeyboardInterrupt:\\n            print('\\\\nGoodbye!')\\n            break\\n        except EOFError:\\n            break\\n\\n\\nif __name__ == '__main__':\\n    main()\\n\",\n    \"file\": \"/mnt/ProjectData/omni/rag_example.py\"\n  },\n  {\n    \"id\": 14,\n    \"hash\": \"f151986bf2efa7602242590a357fab83\",\n    \"content\": \"import os\\nimport json\\nimport hashlib\\nfrom typing import List, Dict, Optional, Tuple\\nfrom sentence_transformers import SentenceTransformer\\nimport numpy as np\\nimport faiss\\nfrom pathlib import Path\\n\\n\\nclass RAGManager:\\n    \\\"\\\"\\\"Manages Retrieval-Augmented Generation operations using sentence transformers.\\\"\\\"\\\"\\n\\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\\n        Optional[str]=None):\\n        \\\"\\\"\\\"\\n    Initialize the RAG manager.\\n\\n    Args:\\n        model_name: Name of the sentence transformer model to use\\n        index_path: Path to save/load the FAISS index\\n    \\\"\\\"\\\"\\n        self.model_name = model_name\\n        from vectordb_manager import VectorDBManager\\n        self.vectordb = VectorDBManager(model_name, index_path)\\n        self.model = self.vectordb.model\\n        self.index_path = self.vectordb.index_path\\n        self.metadata_path = self.vectordb.metadata_path\\n        self.dimension = self.vectordb.dimension\\n        self.index = self.vectordb.index\\n        self.metadata = self.vectordb.metadata\\n\\n    def _initialize_index(self):\\n        \\\"\\\"\\\"Initialize or load the FAISS index.\\\"\\\"\\\"\\n        if os.path.exists(self.index_path) and os.path.exists(self.\\n            metadata_path):\\n            self.index = faiss.read_index(self.index_path)\\n            with open(self.metadata_path, 'r') as f:\\n                self.metadata = json.load(f)\\n        else:\\n            self.index = faiss.IndexFlatIP(self.dimension)\\n            self.metadata = []\\n\\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\\n        Dict]]=None):\\n        \\\"\\\"\\\"\\n        Add documents to the RAG index.\\n\\n        Args:\\n            documents: List of text documents to add\\n            metadatas: Optional list of metadata for each document\\n        \\\"\\\"\\\"\\n        if metadatas is None:\\n            metadatas = [{}] * len(documents)\\n        for i, meta in enumerate(metadatas):\\n            if 'file' not in meta:\\n                meta['file'] = f'document_{len(self.metadata) + i}'\\n        self.vectordb.add_documents(documents, metadatas)\\n        self.metadata = self.vectordb.metadata\\n\\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\\n        \\\"\\\"\\\"\\n        Search for relevant documents.\\n\\n        Args:\\n            query: Query string\\n            k: Number of results to return\\n\\n        Returns:\\n            List of (document, score, metadata) tuples\\n        \\\"\\\"\\\"\\n        from vectordb_manager import VectorDBManager\\n        temp_vdb = VectorDBManager(model_name=self.model_name, index_path=\\n            self.index_path)\\n        results = temp_vdb.search(query, k)\\n        return results\\n\\n    def _save_index(self):\\n        \\\"\\\"\\\"Save the FAISS index and metadata to disk.\\\"\\\"\\\"\\n        faiss.write_index(self.index, self.index_path)\\n        with open(self.metadata_path, 'w') as f:\\n            json.dump(self.metadata, f, indent=2)\\n\\n    def get_document_count(self) ->int:\\n        \\\"\\\"\\\"Get the number of documents in the index.\\\"\\\"\\\"\\n        return len(self.metadata)\\n\\n    def clear_index(self):\\n        \\\"\\\"\\\"Clear the index and metadata.\\\"\\\"\\\"\\n        self.index = faiss.IndexFlatIP(self.dimension)\\n        self.metadata = []\\n        self._save_index()\\n\\n\\nclass RAGManager:\\n    \\\"\\\"\\\"Manages Retrieval-Augmented Generation operations using sentence transformers.\\\"\\\"\\\"\\n\\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\\n        Optional[str]=None):\\n        \\\"\\\"\\\"\\n        Initialize the RAG manager.\\n\\n        Args:\\n            model_name: Name of the sentence transformer model to use\\n            index_path: Path to save/load the FAISS index\\n        \\\"\\\"\\\"\\n        self.model_name = model_name\\n        from vectordb_manager import VectorDBManager\\n        self.vectordb = VectorDBManager(model_name, index_path)\\n        self.model = self.vectordb.model\\n        self.index_path = self.vectordb.index_path\\n        self.metadata_path = self.vectordb.metadata_path\\n        self.dimension = self.vectordb.dimension\\n        self.index = self.vectordb.index\\n        self.metadata = self.vectordb.metadata\\n\\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\\n        Dict]]=None):\\n        \\\"\\\"\\\"\\n        Add documents to the RAG index.\\n\\n        Args:\\n            documents: List of text documents to add\\n            metadatas: Optional list of metadata for each document\\n        \\\"\\\"\\\"\\n        if metadatas is None:\\n            metadatas = [{}] * len(documents)\\n        for i, meta in enumerate(metadatas):\\n            if 'file' not in meta:\\n                meta['file'] = f'document_{len(self.metadata) + i}'\\n        self.vectordb.add_documents(documents, metadatas)\\n        self.metadata = self.vectordb.metadata\\n\\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\\n        \\\"\\\"\\\"\\n        Search for relevant documents.\\n\\n        Args:\\n            query: Query string\\n            k: Number of results to return\\n\\n        Returns:\\n            List of (document, score, metadata) tuples\\n        \\\"\\\"\\\"\\n        from vectordb_manager import VectorDBManager\\n        temp_vdb = VectorDBManager(model_name=self.model_name, index_path=\\n            self.index_path)\\n        results = temp_vdb.search(query, k)\\n        return results\\n\\n    def get_document_count(self) ->int:\\n        \\\"\\\"\\\"Get the number of documents in the index.\\\"\\\"\\\"\\n        return len(self.metadata)\\n\\n    def clear_index(self):\\n        \\\"\\\"\\\"Clear the index and metadata.\\\"\\\"\\\"\\n        self.index = faiss.IndexFlatIP(self.dimension)\\n        self.metadata = []\\n\",\n    \"file\": \"/mnt/ProjectData/omni/rag_manager.py\"\n  }\n]",
    "file": "/mnt/ProjectData/omni/vectordb_index_metadata.json"
  },
  {
    "id": 16,
    "hash": "6774a805052356775d8c4f0367253dc1",
    "content": "# OmniForge\n\n**Project\u2011Aware AI CLI for AST\u2011Precise Edits (\u201cScalpel\u201d) & Multi\u2011File Refactors (\u201cBlueprint\u201d).**\n\n> *OmniForge (a.k.a. the \"DualForge\" engine) helps you **scan**, **understand**, and **reforge** your codebase with local (Ollama) or remote (OpenRouter) LLMs. It is an independent open\u2011source project not affiliated with other products using the name \u201cOmni.\u201d*\n\n---\n\n## Core Features\n\n* **Project\u2011Aware Refactoring (`refactor`)**: Provide a high\u2011level goal (e.g. *\u201cmove all DB logic into `db/` and centralize config\u201d*). OmniForge generates & shows a multi\u2011step plan before touching files.\n* **Surgical AST Editing (`edit`)**: Target a single function/class. OmniForge parses the file, locates the node, applies a structural change, and shows you a diff.\n* **Context Ingestion (`look`)**: Scan a directory or single file to build a project manifest (paths + hashes + size + language hints) that guides subsequent edits/refactors.\n* **Dual Backends**: Seamlessly switch between **`openrouter`** (remote models) and **`ollama`** (local models) via `backend <name>`.\n* **Interactive Model Picker (`models`)**: Filter by source/provider and select from an up\u2011to\u2011date model list inside the terminal.\n* **Session Memory & History**: `history` shows conversation / action log; `memory clear` resets the working context.\n* **Safe AST\u2011Based Transformations**: Reduces syntax breakage vs naive text substitution. New imports / dependencies are auto\u2011inserted when possible.\n* **Dry\u2011Run Transparency**: Every multi\u2011file refactor presents (1) a generated plan, (2) per\u2011file proposed changes, (3) a consolidated diff for confirmation.\n\n---\n\n## Conceptual Modes\n\n| Mode          | Command(s) | Purpose                                              | Analogy              |\n| ------------- | ---------- | ---------------------------------------------------- | -------------------- |\n| **Survey**    | `look`     | Index project / ingest context                       | Mapping the terrain  |\n| **Scalpel**   | `edit`     | Precise single\u2011file structural change                | Surgery              |\n| **Blueprint** | `refactor` | Multi\u2011file architectural transformation              | Architecture         |\n| **Cast**      | `run`      | Execute last generated runnable block (if supported) | Forging the artifact |\n\nYou can reference these terms in docs, help screens, or UI banners for clarity.\n\n---\n\n## Installation\n\n> Requires Python 3.10+ and a Unix\u2011like shell.\n\n```bash\n# 1. Clone\ngit clone https://github.com/Snawyyy/omniforge.git\ncd omniforge\n\n# 2. (Optional) Inspect requirements\ncat requirements.txt\n\n# 3. Run installer (creates venv + installs deps)\nchmod +x install.sh\n./install.sh\n\n# 4. Activate environment\nsource venv/bin/activate\n\n# 5. Run CLI (interactive shell)\n./omni        # or: python omni.py\n```\n\nIf you later package it, expose a script entry point named `omni` while keeping an internal package name like `omniforge` to avoid namespace collisions.\n\n---\n\n## Configuration\n\nCreate a `.env` file to supply secrets (e.g. OpenRouter API key).\n\n```bash\ncp .env.example .env   # if example exists; otherwise create manually\n```\n\nIn `.env`:\n\n```dotenv\nOPENROUTER_API_KEY=\"sk-or-...\"\n```\n\n(Do **not** commit real keys. Ensure `.env` is listed in `.gitignore`.)\n\nOptional future keys can include local model paths, default model name, or feature flags.\n\n---\n\n## Quick Start Workflow\n\n```text\n> look .\n(Index project: builds manifest)\n\n> refactor \"extract hardcoded API URL to config/settings.py and update all imports\"\n(Shows plan \u2192 ask for confirmation)\n\n> edit src/utils.py \"add a doctring for calculate_average explaining parameters and return value\"\n(Shows targeted diff \u2192 confirm or abort)\n\n> models openai\n(Choose a different model)\n\n> backend ollama\n(Switch to local inference)\n```\n\n---\n\n## Command Reference\n\n| Command               | Example                                         | Description                                                                 |\n| --------------------- | ----------------------------------------------- | --------------------------------------------------------------------------- |\n| `look <path>`         | `look .`                                        | Scan directory or file; update project context manifest.                    |\n| `edit <file> \"instr\"` | `edit utils.py \"add error handling\"`            | AST\u2011guided targeted modification of a single file element or whole file.    |\n| `refactor \"goal\"`     | `refactor \"split monolith api.py into package\"` | High\u2011level multi\u2011file transformation with plan + diff review.               |\n| `send <prompt>`       | `send summarize module layout`                  | Generic prompt to current model with current context summary.               |\n| `models [filter]`     | `models google`                                 | Interactive model selector (filter by provider/source).                     |\n| `backend <name>`      | `backend ollama`                                | Switch between `openrouter` / `ollama`.                                     |\n| `history`             | `history`                                       | Display session interaction log.                                            |\n| `memory clear`        | `memory clear`                                  | Clear internal AI context memory (project manifest may persist separately). |\n| `run`                 | `run`                                           | Execute the last generated runnable code snippet (if feature implemented).  |\n| `help`                | `help`                                          | Show help / usage summary.                                                  |\n\n---\n\n## How It Works (High Level)\n\n1. **Ingestion (`look`)**: Walks the directory (configurable ignore patterns), captures file metadata (path, size, language), optionally caches abbreviated content or hashes.\n2. **Prompt Construction**: Merges user instruction + contextual manifest + focused snippets (for `edit`) or diff summaries (for `refactor`).\n3. **AST Layer** (Python initially): Parses target file(s); identifies node(s) (function/class) based on name + heuristic similarity; applies transformations guided by model output (structured hints or patch templates) rather than raw blind overwrite.\n4. **Plan (Refactor)**: Model proposes steps (CREATE / MODIFY / RENAME / DELETE). User approves. Each step executed & validated; errors surface early.\n5. **Diff Presentation**: A unified colorized diff shown; user confirms to write changes.\n6. **Safety**: If AST parse fails or patch violates syntax, changes abort (or fallback to text patch with warning).\n\n---\n\n## Roadmap (Indicative)\n\n| Milestone | Features                                                                    |\n| --------- | --------------------------------------------------------------------------- |\n| 0.1.0     | Core commands (`look`, `edit`, `refactor`, `models`, `backend`, `history`). |\n| 0.2.0     | Config file (`omniforge.toml`), ignore patterns, improved diff viewer.      |\n| 0.3.0     | Multi-language AST adapters (JS/TS via `tree-sitter`).                      |\n| 0.4.0     | Test impact analysis & auto test file generation.                           |\n| 0.5.0     | Refactor preview metrics (LOC touched, complexity delta).                   |\n| 0.6.0     | Inline security / lint feedback integration.                                |\n| 0.7.0     | Partial GUI / TUI dashboard mode.                                           |\n\n(Adjust roadmap as project evolves.)\n\n---\n\n## Developer Notes\n\n* Keep `.env`, virtual environment dirs (`venv/`), caches, large artifacts in `.gitignore`.\n* Consider adding a thin `omniforge/` package directory now for future PyPI packaging.\n* Implement a pluggable *Model Adapter* layer to abstract provider differences (OpenRouter vs Ollama).\n* Logging: Provide `--verbose` or `OMNIFORGE_DEBUG=1` env flag.\n* Add a JSONL log of commands & decisions to enable replay or supervised improvements.\n\n---\n\n## Contributing\n\n1. Fork & create a feature branch: `git checkout -b feat/<short-name>`\n2. Run and add tests (if/when test harness exists in `tests/`).\n3. Ensure style/format (e.g. `ruff`, `black`) passes.\n4. Submit pull request with concise description + before/after examples.\n\nIssues / discussions welcome for architecture proposals, AST adapters, or performance improvements.\n\n---\n\n## License\n\nReleased under the **MIT License**. See `LICENSE` file for full text.\n\n---\n\n## Name & Trademark Disclaimer\n\n\u201cOmniForge\u201d is an independent open-source tool and not affiliated with The Omni Group or any other third-party products using similar names. If a future naming conflict arises, a soft alias strategy (keeping the `omni` CLI) will preserve user workflows.\n\n---\n\n## Example Session (Illustrative)\n\n```text\n$ ./omni\nOMNIFORGE 0.1.0  (backend=openrouter | model=anthropic/claude-3)\nType 'help' or 'look .' to begin.\n\n> look .\nIndexed 42 files (Python=30, Markdown=5, JSON=7)\n\n> refactor \"centralize logging into logging_util.py and update imports\"\nPlan:\n  [1] CREATE logging_util.py\n  [2] MODIFY app.py (replace inline log setup)\n  [3] MODIFY worker.py (import logging_util)\nProceed? (y/n) y\n... (diff preview) ...\nApply changes? (y/n) y\nRefactor complete in 3.2s.\n\n> edit worker.py \"add retry with exponential backoff to fetch_data\"\nParsed worker.py \u2713\nApplied modification (function: fetch_data)\nDiff shown. Accept? (y/n) y\nSaved.\n```\n\n---\n\n## Philosophy\n\n*The next wave of developer AI goes beyond paste\u2011in prompts.* OmniForge treats your repository as *structured material*, enabling incremental, auditable, and safer evolution rather than opaque code dumps. By combining **context scanning**, **AST precision**, and **human approval loops**, it aims to become a trustworthy co\u2011developer rather than an occasionally helpful code generator.\n\n---\n\n## Feedback / Support\n\nOpen a GitHub Issue for bugs & feature requests. For conceptual discussions (roadmap, design choices), use Discussions.\n\n---\n\nHappy forging!\n",
    "file": "/mnt/ProjectData/omni/README.md"
  },
  {
    "id": 17,
    "hash": "17ef2c48d5827115cc9579bd067e6ec9",
    "content": "\"\"\"\nCodeEditor - Enhanced AST-based Python code manipulation tool\n\nThis module provides a class to programmatically parse, analyze,\nand edit Python source code using Abstract Syntax Trees (AST).\nIt supports both full element replacement and surgical partial edits\nwithin functions and classes using asttokens for precise source mapping.\n\"\"\"\nimport ast\nimport astor\nimport difflib\nfrom typing import List, Optional, Dict, Union, Tuple\ntry:\n    import asttokens\n    ASTTOKENS_AVAILABLE = True\nexcept ImportError:\n    ASTTOKENS_AVAILABLE = False\n    print(\"Warning: asttokens not installed. Partial edits will be limited.\")\n\n\nclass CodeEditor:\n    \"\"\"An enhanced class to safely edit Python files using AST with partial edit support.\"\"\"\n\n    def __init__(self, file_path: str):\n        self.file_path = file_path\n        self.source_code = self._read_file()\n        self.tree: ast.AST = self._parse_source()\n        self.nodes: Dict[str, ast.AST] = self._map_nodes()\n        # Enhanced source tracking with asttokens\n        if ASTTOKENS_AVAILABLE:\n            self.atok = asttokens.ASTTokens(self.source_code, parse=True)\n        else:\n            self.atok = None\n\n    def _read_file(self) -> str:\n        try:\n            with open(self.file_path, 'r') as f:\n                return f.read()\n        except FileNotFoundError:\n            raise ValueError(f'File not found: {self.file_path}')\n\n    def _parse_source(self) -> ast.AST:\n        try:\n            return ast.parse(self.source_code)\n        except SyntaxError as e:\n            raise ValueError(f'Invalid Python syntax in {self.file_path}: {e}')\n\n    def _map_nodes(self) -> Dict[str, ast.AST]:\n        nodes = {}\n        for node in ast.walk(self.tree):\n            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n                if node.name not in nodes:\n                    nodes[node.name] = node\n            elif isinstance(node, ast.Assign):\n                for target in node.targets:\n                    if isinstance(target, ast.Name) and target.id not in nodes:\n                        nodes[target.id] = node\n            elif isinstance(node, ast.Import):\n                for alias in node.names:\n                    name = alias.asname or alias.name\n                    if name not in nodes:\n                        nodes[name] = node\n            elif isinstance(node, ast.ImportFrom):\n                for alias in node.names:\n                    name = alias.asname or alias.name\n                    if name not in nodes:\n                        nodes[name] = node\n        return nodes\n\n    def list_elements(self) -> List[str]:\n        return list(self.nodes.keys())\n\n    def get_source_of(self, element_name: str) -> Optional[str]:\n        node = self.nodes.get(element_name)\n        return astor.to_source(node) if node else None\n\n    def get_element_structure(self, element_name: str) -> Optional[Dict]:\n        \"\"\"\n        Get detailed structure information about an element including\n        line numbers and internal components.\n        \"\"\"\n        node = self.nodes.get(element_name)\n        if not node:\n            return None\n        \n        structure = {\n            'name': element_name,\n            'type': node.__class__.__name__,\n            'line_start': node.lineno if hasattr(node, 'lineno') else None,\n            'line_end': node.end_lineno if hasattr(node, 'end_lineno') else None,\n            'body_items': []\n        }\n        \n        if hasattr(node, 'body'):\n            for i, item in enumerate(node.body):\n                item_info = {\n                    'index': i,\n                    'type': item.__class__.__name__,\n                    'line_start': item.lineno if hasattr(item, 'lineno') else None,\n                    'line_end': item.end_lineno if hasattr(item, 'end_lineno') else None\n                }\n                \n                # Add more details for common statement types\n                if isinstance(item, ast.Assign) and item.targets:\n                    target = item.targets[0]\n                    if isinstance(target, ast.Name):\n                        item_info['assigns'] = target.id\n                elif isinstance(item, (ast.If, ast.While, ast.For)):\n                    item_info['has_body'] = bool(item.body)\n                elif isinstance(item, ast.Return):\n                    item_info['returns'] = True\n                    \n                structure['body_items'].append(item_info)\n                \n        return structure\n\n    def _find_statement_in_body(self, body: List[ast.AST], line_start: int, line_end: Optional[int] = None) -> Optional[Tuple[int, ast.AST]]:\n        \"\"\"\n        Find a statement in a body by line number range.\n        Returns (index, statement) or None.\n        \"\"\"\n        for i, stmt in enumerate(body):\n            if hasattr(stmt, 'lineno'):\n                if line_end:\n                    stmt_end = stmt.end_lineno if hasattr(stmt, 'end_lineno') else stmt.lineno\n                    if stmt.lineno <= line_start <= stmt_end or stmt.lineno <= line_end <= stmt_end:\n                        return (i, stmt)\n                else:\n                    if stmt.lineno == line_start:\n                        return (i, stmt)\n        return None\n\n    def replace_partial(self, element_name: str, new_code: str, \n                       line_start: Optional[int] = None, line_end: Optional[int] = None,\n                       statement_index: Optional[int] = None) -> bool:\n        \"\"\"\n        Replace a partial section of an element (function/class).\n        \n        Args:\n            element_name: Name of the function/class to modify\n            new_code: New code to insert (can be multiple statements)\n            line_start: Starting line number within the element (1-based, absolute)\n            line_end: Ending line number within the element (inclusive)\n            statement_index: Alternative to line numbers - replace the Nth statement\n            \n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n        if element_name not in self.nodes:\n            return False\n            \n        node = self.nodes[element_name]\n        if not hasattr(node, 'body') or not isinstance(node.body, list):\n            return False\n            \n        try:\n            # Parse the new code\n            if new_code.strip().startswith('def ') or new_code.strip().startswith('class '):\n                # If it's a full function/class, extract just the body\n                new_ast = ast.parse(new_code)\n                new_statements = new_ast.body[0].body\n            else:\n                # Parse as a module and get all statements\n                new_ast = ast.parse(new_code)\n                new_statements = new_ast.body\n        except SyntaxError:\n            return False\n            \n        if not new_statements:\n            return False\n            \n        # Find what to replace\n        if statement_index is not None:\n            # Replace by index\n            if 0 <= statement_index < len(node.body):\n                node.body[statement_index:statement_index+1] = new_statements\n                return True\n        elif line_start is not None:\n            # Replace by line number\n            result = self._find_statement_in_body(node.body, line_start, line_end)\n            if result:\n                idx, _ = result\n                if line_end:\n                    # Find end index\n                    end_idx = idx\n                    for i in range(idx + 1, len(node.body)):\n                        stmt = node.body[i]\n                        if hasattr(stmt, 'end_lineno') and stmt.end_lineno <= line_end:\n                            end_idx = i\n                        else:\n                            break\n                    node.body[idx:end_idx+1] = new_statements\n                else:\n                    node.body[idx:idx+1] = new_statements\n                return True\n                \n        return False\n\n    def insert_in_element(self, element_name: str, new_code: str, \n                         position: str = 'end', \n                         after_line: Optional[int] = None,\n                         before_line: Optional[int] = None) -> bool:\n        \"\"\"\n        Insert new code into an element without replacing existing code.\n        \n        Args:\n            element_name: Name of the function/class\n            new_code: Code to insert\n            position: 'start', 'end', or use after_line/before_line\n            after_line: Insert after this line number\n            before_line: Insert before this line number\n            \n        Returns:\n            True if successful\n        \"\"\"\n        if element_name not in self.nodes:\n            return False\n            \n        node = self.nodes[element_name]\n        if not hasattr(node, 'body') or not isinstance(node.body, list):\n            return False\n            \n        try:\n            new_ast = ast.parse(new_code)\n            new_statements = new_ast.body\n        except SyntaxError:\n            return False\n            \n        if not new_statements:\n            return False\n            \n        if position == 'start':\n            node.body[0:0] = new_statements\n        elif position == 'end':\n            node.body.extend(new_statements)\n        elif after_line:\n            result = self._find_statement_in_body(node.body, after_line)\n            if result:\n                idx, _ = result\n                node.body[idx+1:idx+1] = new_statements\n            else:\n                return False\n        elif before_line:\n            result = self._find_statement_in_body(node.body, before_line)\n            if result:\n                idx, _ = result\n                node.body[idx:idx] = new_statements\n            else:\n                return False\n        else:\n            return False\n            \n        return True\n\n    def get_element_body_snippet(self, element_name: str, line_start: int, line_end: int) -> Optional[str]:\n        \"\"\"\n        Get a specific snippet from within an element's body by line numbers.\n        \"\"\"\n        if not self.atok:\n            return None\n            \n        node = self.nodes.get(element_name)\n        if not node or not hasattr(node, 'body'):\n            return None\n            \n        # Find statements in the range\n        statements = []\n        for stmt in node.body:\n            if hasattr(stmt, 'lineno') and hasattr(stmt, 'end_lineno'):\n                if line_start <= stmt.lineno <= line_end or line_start <= stmt.end_lineno <= line_end:\n                    statements.append(stmt)\n                    \n        if not statements:\n            return None\n            \n        # Generate source for the statements\n        return '\\n'.join(astor.to_source(stmt).strip() for stmt in statements)\n\n    def _add_imports(self, new_import_nodes: List[Union[ast.Import, ast.ImportFrom]]) -> None:\n        \"\"\"Intelligently adds new import nodes to the top of the AST.\"\"\"\n        existing_imports_str = set()\n        last_import_index = -1\n        for i, node in enumerate(self.tree.body):\n            if isinstance(node, (ast.Import, ast.ImportFrom)):\n                existing_imports_str.add(astor.to_source(node).strip())\n                last_import_index = i\n        for new_import in reversed(new_import_nodes):\n            new_import_str = astor.to_source(new_import).strip()\n            if new_import_str not in existing_imports_str:\n                self.tree.body.insert(last_import_index + 1, new_import)\n\n    def replace_element(self, element_name: str, new_code: str) -> bool:\n        \"\"\"\n        Replaces a target element with a new block of code.\n\n        This method is more robust than a simple node-for-node replacement.\n        It parses the `new_code`, separates imports (which are moved to the top\n        of the file) from the main code body (functions, classes, variables, etc.),\n        and then replaces the single original element node in the AST with the\n        entire new code body. This allows the AI to return code blocks that\n        include helper constants or other statements along with the primary\n        function or class definition.\n\n        Args:\n            element_name: The name of the function or class to replace.\n            new_code: A string containing the new Python code.\n\n        Returns:\n            True if the replacement was successful, False otherwise.\n        \"\"\"\n        if element_name not in self.nodes:\n            return False\n        try:\n            new_ast_module = ast.parse(new_code)\n        except SyntaxError:\n            return False\n        new_imports = [n for n in new_ast_module.body if isinstance(n, (ast.Import, ast.ImportFrom))]\n        new_code_body = [n for n in new_ast_module.body if not isinstance(n, (ast.Import, ast.ImportFrom))]\n        if not new_code_body:\n            return False\n        if new_imports:\n            self._add_imports(new_imports)\n        for node in ast.walk(self.tree):\n            if hasattr(node, 'body') and isinstance(node.body, list):\n                try:\n                    old_node = self.nodes[element_name]\n                    idx = node.body.index(old_node)\n                    node.body.pop(idx)\n                    for i, new_node in enumerate(new_code_body):\n                        node.body.insert(idx + i, new_node)\n                    del self.nodes[element_name]\n                    for n in new_code_body:\n                        if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n                            self.nodes[n.name] = n\n                    return True\n                except (ValueError, KeyError):\n                    continue\n        return False\n\n    def add_element(self, new_code: str, anchor_name: Optional[str] = None,\n                   before: bool = False) -> bool:\n        \"\"\"\n        Adds a new block of code (function, class, variables) to the file.\n\n        This method intelligently handles code snippets from the AI. It separates\n        any import statements and adds them to the top of the file. The rest of\n        the code block is inserted at an appropriate location, determined either\n        by an `anchor_name` or by placing it before the main execution block\n        (`if __name__ == '__main__'`).\n\n        Args:\n            new_code: The string containing the new function/class/variables.\n            anchor_name: The name of an existing element to insert relative to.\n            before: If True, insert before the anchor; otherwise, after.\n\n        Returns:\n            True if the element block was added successfully, False otherwise.\n        \"\"\"\n        try:\n            new_ast_module = ast.parse(new_code)\n        except (SyntaxError, IndexError):\n            return False\n        new_imports = [n for n in new_ast_module.body if isinstance(n, (ast.Import, ast.ImportFrom))]\n        new_code_body = [n for n in new_ast_module.body if not isinstance(n, (ast.Import, ast.ImportFrom))]\n        if not new_code_body:\n            return False\n        if new_imports:\n            self._add_imports(new_imports)\n        insertion_index = -1\n        main_block_idx = -1\n        for i, node in enumerate(self.tree.body):\n            if isinstance(node, ast.If) and isinstance(node.test, ast.Compare) and isinstance(node.test.left, ast.Name) and node.test.left.id == '__name__' and len(node.test.ops) == 1 and isinstance(node.test.ops[0], ast.Eq) and len(node.test.comparators) == 1:\n                comp = node.test.comparators[0]\n                if isinstance(comp, ast.Constant) and comp.value == '__main__' or isinstance(comp, ast.Str) and comp.s == '__main__':\n                    main_block_idx = i\n                    break\n        if anchor_name:\n            if anchor_name not in self.nodes:\n                return False\n            anchor_node = self.nodes[anchor_name]\n            try:\n                anchor_idx = self.tree.body.index(anchor_node)\n                proposed_index = anchor_idx if before else anchor_idx + 1\n                if main_block_idx != -1:\n                    insertion_index = min(proposed_index, main_block_idx)\n                else:\n                    insertion_index = proposed_index\n            except ValueError:\n                return False\n        elif main_block_idx != -1:\n            insertion_index = main_block_idx\n        if insertion_index == -1:\n            self.tree.body.extend(new_code_body)\n        else:\n            for i, new_node in enumerate(new_code_body):\n                self.tree.body.insert(insertion_index + i, new_node)\n        for node in new_code_body:\n            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n                self.nodes[node.name] = node\n        return True\n\n    def delete_element(self, element_name: str) -> bool:\n        \"\"\"\n        Deletes a function, class, variable (ast.Assign), or import (ast.Import/ast.ImportFrom) by its name from the AST.\n        Handles top-level elements primarily, with walking for functions and classes.\n\n        Args:\n            element_name: The name of the element to delete (function, class, variable, or imported name).\n\n        Returns:\n            True if the element was successfully deleted, False otherwise.\n        \"\"\"\n        deleted = False\n        if element_name in self.nodes:\n            node_to_delete = self.nodes[element_name]\n            for node in ast.walk(self.tree):\n                if hasattr(node, 'body') and isinstance(node.body, list):\n                    try:\n                        node.body.remove(node_to_delete)\n                        deleted = True\n                        break\n                    except ValueError:\n                        continue\n            if deleted:\n                del self.nodes[element_name]\n                self.nodes = self._map_nodes()\n                return True\n        new_body = []\n        for node in self.tree.body:\n            if isinstance(node, ast.Assign):\n                match = False\n                for target in node.targets:\n                    if isinstance(target, ast.Name) and target.id == element_name:\n                        match = True\n                        break\n                if match:\n                    deleted = True\n                    continue\n            elif isinstance(node, ast.Import):\n                new_aliases = []\n                for alias in node.names:\n                    if (alias.name == element_name or alias.asname == element_name):\n                        deleted = True\n                    else:\n                        new_aliases.append(alias)\n                if new_aliases:\n                    node.names = new_aliases\n                elif deleted:\n                    continue\n            elif isinstance(node, ast.ImportFrom):\n                new_aliases = []\n                for alias in node.names:\n                    if (alias.name == element_name or alias.asname == element_name):\n                        deleted = True\n                    else:\n                        new_aliases.append(alias)\n                if new_aliases:\n                    node.names = new_aliases\n                elif deleted:\n                    continue\n            new_body.append(node)\n        self.tree.body = new_body\n        if deleted:\n            self.nodes = self._map_nodes()\n        return deleted\n\n    def apply_arbitrary_change(self, new_source_code: str) -> bool:\n        \"\"\"\n        Rewrites the entire file's AST from a new source string.\n\n        This method parses the provided new_source_code into a new AST.\n        If the code is syntactically valid, it replaces the class's internal\n        AST (`self.tree`), remaps the file's nodes, and returns True. If\n        parsing fails, it returns False.\n        \"\"\"\n        try:\n            new_tree = ast.parse(new_source_code)\n            self.tree = new_tree\n            self.nodes = self._map_nodes()\n            # Recreate asttokens if available\n            if ASTTOKENS_AVAILABLE:\n                self.source_code = new_source_code\n                self.atok = asttokens.ASTTokens(self.source_code, parse=True)\n            return True\n        except SyntaxError:\n            return False\n\n    def get_modified_source(self) -> str:\n        return astor.to_source(self.tree)\n\n    def get_diff(self) -> str:\n        modified_source = self.get_modified_source()\n        return ''.join(difflib.unified_diff(\n            self.source_code.splitlines(keepends=True), \n            modified_source.splitlines(keepends=True),\n            fromfile=f'{self.file_path} (original)', \n            tofile=f'{self.file_path} (modified)'\n        ))\n\n    def save_changes(self) -> None:\n        modified_source = self.get_modified_source()\n        with open(self.file_path, 'w') as f:\n            f.write(modified_source)",
    "file": "/mnt/ProjectData/omni/code_editor.py"
  },
  {
    "id": 18,
    "hash": "cc52aa524d0929867d5d2ea3697229e6",
    "content": "\"\"\"\nGenerated file.\n\"\"\"\nimport os\n\"\"\"\nFileCreator - A utility for creating files.\n\nThis module provides a simple, encapsulated way to create files,\nensuring that their parent directories exist before writing.\n\"\"\"\n\n\nclass FileCreator:\n    \"\"\"\n    A utility class to handle the creation of new files.\n    \"\"\"\n\n    @staticmethod\n    def create(file_path: str, content: str) ->None:\n        \"\"\"\n        Creates a file at the specified path with the given content.\n\n        This method will automatically create any necessary parent\n        directories for the file path.\n\n        Args:\n            file_path: The full path where the file should be created.\n            content: The string content to write to the file.\n\n        Raises:\n            IOError: If there is an error creating the directories or writing the file.\n        \"\"\"\n        try:\n            directory = os.path.dirname(file_path)\n            if directory:\n                os.makedirs(directory, exist_ok=True)\n            with open(file_path, 'w', encoding='utf-8') as f:\n                f.write(content)\n        except (IOError, OSError) as e:\n            raise IOError(f\"Failed to create file '{file_path}': {e}\") from e\n",
    "file": "/mnt/ProjectData/omni/file_creator.py"
  },
  {
    "id": 19,
    "hash": "04a24ceab008bd649d0530d309c72e42",
    "content": "import subprocess\nimport os\nfrom typing import List, Optional, Union\n\n\nclass GitManager:\n    \"\"\"\n    Encapsulates Git-related logic for a specific repository.\n\n    This class provides methods to perform common Git operations such as\n    checking status, getting diffs, staging, committing, and pushing changes.\n    It relies on the Git command-line tool being installed and accessible\n    in the system's PATH.\n    \"\"\"\n\n    def __init__(self, repo_path: str):\n        \"\"\"\n        Initializes the GitManager for a given repository path.\n\n        Args:\n            repo_path: The absolute or relative path to the Git repository.\n\n        Raises:\n            ValueError: If the provided path is not a valid Git repository.\n        \"\"\"\n        if not os.path.isdir(os.path.join(repo_path, '.git')):\n            raise ValueError(\n                f\"The path '{repo_path}' is not a valid Git repository.\")\n        self.repo_path = repo_path\n\n    def _run_command(self, command: List[str]) ->str:\n        \"\"\"\n        Executes a Git command in the repository's directory.\n\n        Args:\n            command: A list of command arguments, starting with 'git'.\n\n        Returns:\n            The standard output of the command as a string.\n\n        Raises:\n            subprocess.CalledProcessError: If the command returns a non-zero exit code.\n        \"\"\"\n        try:\n            result = subprocess.run(command, cwd=self.repo_path, check=True,\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True,\n                encoding='utf-8')\n            return result.stdout.strip()\n        except subprocess.CalledProcessError as e:\n            error_message = (\n                f\"Git command failed: {' '.join(command)}\\nError: {e.stderr.strip()}\"\n                )\n            raise subprocess.CalledProcessError(e.returncode, e.cmd, output\n                =e.stdout, stderr=error_message) from e\n\n    def get_status(self) ->str:\n        \"\"\"\n        Gets the repository status in a condensed format.\n\n        Uses `git status --porcelain` for a machine-readable output.\n\n        Returns:\n            A string representing the repository's status.\n        \"\"\"\n        return self._run_command(['git', 'status', '--porcelain'])\n        \n    def get_changed_files(self) -> List[str]:\n        \"\"\"\n        Gets a list of all changed (modified, added, deleted, untracked, renamed, or copied) files.\n\n        This parses the output of `git status --porcelain`, which provides a stable,\n        machine-readable format.\n\n        Returns:\n            A list of file paths relative to the repository root. For renamed or\n            copied files, it returns the new path.\n        \"\"\"\n        status_output = self.get_status()\n        if not status_output:\n            return []\n        \n        changed_files = []\n        for line in status_output.splitlines():\n            if len(line) < 3:  # Skip malformed lines\n                continue\n                \n            # Git status porcelain format: XY filename\n            # X = index status, Y = working tree status\n            index_status = line[0]\n            worktree_status = line[1]\n            \n            # The filename starts after the two status characters and a space\n            # But let's be more defensive about finding where the filename actually starts\n            filename_start = 2\n            while filename_start < len(line) and line[filename_start] == ' ':\n                filename_start += 1\n                \n            if filename_start >= len(line):\n                continue  # Skip if no filename found\n                \n            path_info = line[filename_start:]\n            \n            # Check if either index or working tree status indicates rename/copy\n            if index_status in ('R', 'C') or worktree_status in ('R', 'C'):\n                if ' -> ' in path_info:\n                    _, new_path = path_info.split(' -> ', 1)  # Use maxsplit=1 for safety\n                    changed_files.append(new_path.strip())\n                else:\n                    # Fallback if format is unexpected\n                    changed_files.append(path_info.strip())\n            else:\n                changed_files.append(path_info.strip())\n        \n        # Remove duplicates while preserving order\n        return list(dict.fromkeys(changed_files))\n\n    def get_diff(self, file_path: Optional[str]=None, staged: bool=False\n        ) ->str:\n        \"\"\"\n        Gets the diff of changes in the repository.\n\n        Args:\n            file_path: Optional. Path to a specific file to diff.\n            staged: If True, shows the diff for staged changes (`--cached`).\n                    Otherwise, shows the diff for unstaged changes.\n\n        Returns:\n            The git diff output as a string.\n        \"\"\"\n        cmd = ['git', 'diff']\n        if staged:\n            cmd.append('--cached')\n        if file_path:\n            cmd.append('--')\n            cmd.append(file_path)\n        return self._run_command(cmd)\n\n    def add(self, files: Union[str, List[str]]) ->None:\n        \"\"\"\n    Stages one or more files, handling each one individually for robustness.\n\n    This approach prevents a single invalid file path from causing the\n    entire 'git add' operation to fail. Warnings will be printed for any\n    files that could not be staged.\n\n    Args:\n        files: A single file path or a list of file paths to stage.\n               Can also be '.' to stage all changes.\n    \"\"\"\n        if isinstance(files, str):\n            files = [files]\n        for file_path in files:\n            try:\n                cmd = ['git', 'add', '--', file_path]\n                self._run_command(cmd)\n            except subprocess.CalledProcessError as e:\n                print(\n                    f\"Warning: Could not stage file '{file_path}'. Git reported: {e.stderr}\"\n                    )\n\n    def commit(self, message: str) ->str:\n        \"\"\"\n        Commits staged changes with a given message.\n\n        Args:\n            message: The commit message.\n\n        Returns:\n            The stdout from the git commit command.\n        \"\"\"\n        return self._run_command(['git', 'commit', '-m', message])\n\n    def push(self, remote: str='origin', branch: Optional[str]=None) ->str:\n        \"\"\"\n        Pushes commits to a remote repository.\n\n        Args:\n            remote: The name of the remote to push to (default: 'origin').\n            branch: The branch to push. If None, uses the current branch.\n\n        Returns:\n            The stdout from the git push command.\n        \"\"\"\n        if branch is None:\n            branch = self.get_current_branch()\n        return self._run_command(['git', 'push', remote, branch])\n\n    def get_current_branch(self) ->str:\n        \"\"\"\n        Determines the current active branch name.\n\n        Returns:\n            The name of the current branch.\n        \"\"\"\n        return self._run_command(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n",
    "file": "/mnt/ProjectData/omni/git_manager.py"
  },
  {
    "id": 20,
    "hash": "31509b45a6fce6faa59ab9857bf02c0b",
    "content": "import json\nimport os\nfrom typing import List, Dict, Optional\nfrom rag_manager import RAGManager\n\n\nclass MemoryManager:\n    \"\"\"Manages persistent chat memory, look data, and RAG integration via JSON.\"\"\"\n\n    def __init__(self, memory_file: str):\n        self.memory_file = memory_file\n        self.memory: Dict[str, List] = self.load_memory()\n        self.rag_manager = RAGManager()\n\n    def load_memory(self) ->Dict[str, List]:\n        try:\n            with open(self.memory_file, 'r') as f:\n                return json.load(f)\n        except FileNotFoundError:\n            default = {'chat': [], 'look': []}\n            self.save_memory(default)\n            return default\n        except json.JSONDecodeError:\n            print('[yellow]Invalid memory file. Resetting.[/]')\n            return {'chat': [], 'look': []}\n\n    def save_memory(self, memory: Optional[Dict[str, List]]=None) ->None:\n        if memory is None:\n            memory = self.memory\n        with open(self.memory_file, 'w') as f:\n            json.dump(memory, f, indent=4)\n\n    def add_message(self, role: str, content: str) ->None:\n        self.memory['chat'].append({'role': role, 'content': content})\n        self.save_memory()\n\n    def add_look_data(self, file_path: str, content: str) ->None:\n        \"\"\"\n    Adds a watched item (directory or file) to memory, distinguishing its type.\n\n    This method stores structured data that differentiates between a project\n    directory (containing a manifest) and a single file (containing its content).\n    It also prevents duplicate entries by updating existing ones.\n\n    Args:\n        file_path: The path to the directory or file.\n        content: The manifest for a directory or the content for a file.\n    \"\"\"\n        item_type = 'directory' if os.path.isdir(file_path) else 'file'\n        for item in self.memory['look']:\n            if item.get('file') == file_path:\n                item['content'] = content\n                item['type'] = item_type\n                self.save_memory()\n                return\n        self.memory['look'].append({'type': item_type, 'file': file_path,\n            'content': content})\n        self.save_memory()\n        if item_type == 'file':\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    file_content = f.read()\n                self.rag_manager.add_documents([file_content], [{'file':\n                    file_path}])\n            except Exception as e:\n                print(\n                    f'[yellow]Warning: Could not add {file_path} to RAG index: {e}[/]'\n                    )\n\n    def get_project_root(self) -> Optional[str]:\n        \"\"\"\n        Finds the root directory of the project currently in memory.\n\n        The project root is defined as the first item in the 'look' memory\n        that is of type 'directory'.\n\n        Returns:\n            The absolute path to the project root directory, or None if not found.\n        \"\"\"\n        for item in self.memory.get('look', []):\n            if item.get('type') == 'directory':\n                return item.get('file')\n        return None\n\n    def get_memory_context(self) -> str:\n        \"\"\"\n        Dynamically builds the context using RAG and chat history.\n\n        It retrieves relevant file content from the RAG index based on the latest\n        user query, includes project manifests for directories, and appends the\n        recent chat history.\n        \"\"\"\n        context = ''\n        # 1. Add project manifests for any watched directories\n        for look in self.memory.get('look', []):\n            path = look.get('file')\n            if path and os.path.isdir(path):\n                content = look.get('content', '')\n                context += f'--- Project Manifest for {path} ---\\n{content}\\n\\n'\n\n        # 2. Find the last user message to use as a query for the RAG system\n        last_user_message = next((msg['content'] for msg in reversed(self.memory.get('chat', [])) if msg['role'] == 'user'), None)\n\n        # 3. If a user message exists, search the RAG index for relevant context\n        if last_user_message:\n            rag_results = self.search_rag(last_user_message, k=3)\n            if rag_results:\n                context += '--- Relevant context from RAG ---\\n'\n                # Format and add each RAG result to the context\n                for doc, score, meta in rag_results:\n                    file_path = meta.get('file', 'Unknown source')\n                    context += f'Source: {file_path} (Score: {score:.4f})\\n'\n                    context += f'Content: {doc}\\n---\\n'\n                context += '\\n'\n\n        # 4. Append the full chat history for conversational context\n        for msg in self.memory.get('chat', []):\n            context += f\"{msg['role'].capitalize()}: {msg['content']}\\n\"\n\n        return context.strip()\n\n    def clear_memory(self) ->None:\n        self.memory = {'chat': [], 'look': []}\n        self.save_memory()\n        self.rag_manager.clear_index()\n\n    def search_rag(self, query: str, k: int=3) ->List[tuple]:\n        \"\"\"\n        Search the RAG index for relevant documents.\n        \n        Args:\n            query: The search query\n            k: Number of results to return\n            \n        Returns:\n            List of (document_content, score, metadata) tuples\n        \"\"\"\n        return self.rag_manager.search(query, k)",
    "file": "/mnt/ProjectData/omni/memory_manager.py"
  },
  {
    "id": 21,
    "hash": "1b61704bafe182154c395ea08a3acb39",
    "content": "\"\"\"\nOmni - AI-powered code generation and project-aware editing CLI tool\n\nIntegrates modular UI, memory, personality, and AST-based code editing.\n\"\"\"\nimport os\nimport subprocess\nimport requests\nimport json\nimport sys\nimport re\nimport argparse\nimport ast\nfrom datetime import datetime\nimport threading\nimport queue as Queue\nimport time\nfrom typing import List, Dict, Optional\nfrom rich import print\nfrom rich.panel import Panel\nfrom rich.console import Console\nfrom rich.tree import Tree\nfrom ui_manager import UIManager\nfrom personality_manager import PersonalityManager\nfrom memory_manager import MemoryManager\nfrom code_editor import CodeEditor\nfrom file_creator import FileCreator\nfrom git_manager import GitManager\nimport traceback\nDEFAULT_BACKEND = 'openrouter'\nOLLAMA_MODEL = 'phi4-reasoning'\nOPENROUTER_MODEL = 'qwen/qwen3-coder'\nDEFAULT_SAVE_DIR = os.path.expanduser('/mnt/ProjectData/omni/omni_saves/')\nCONFIG_FILE = 'config.json'\nMEMORY_FILE = 'memory.json'\nOLLAMA_API_URL = 'http://localhost:11434/api/generate'\nOPENROUTER_API_URL = 'https://openrouter.ai/api/v1/chat/completions'\nOPENROUTER_MODELS_API_URL = 'https://openrouter.ai/api/v1/models'\nOPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')\ncurrent_backend = DEFAULT_BACKEND\ncurrent_model = (OLLAMA_MODEL if DEFAULT_BACKEND == 'ollama' else\n    OPENROUTER_MODEL)\nOLLAMA_MODELS = {'deepseek': 'deepseek-coder:6.7b', 'codellama':\n    'codellama:13b', 'mistral': 'mistral:latest', 'llama2': 'llama2:latest',\n    'phind': 'phind-codellama:34b'}\nos.makedirs(DEFAULT_SAVE_DIR, exist_ok=True)\nTEMPLATES = {'flask':\n    \"\"\"from flask import Flask, jsonify\n\napp = Flask(__name__)\n\n@app.route('/')\ndef home():\n    return '<h1>Hello, Flask!</h1>'\n\nif __name__ == '__main__':\n    app.run(debug=True)\"\"\"\n    , 'html5':\n    \"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>New Page</title>\n</head>\n<body>\n    <h1>Hello World</h1>\n</body>\n</html>\"\"\"\n    , 'scraper':\n    \"\"\"import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.content, 'html.parser')\n        print(soup.title.text)\n    except Exception as e:\n        print(f\"[bold red]Error scraping {url}:[/] {e}\")\n\nif __name__ == \"__main__\":\n    scrape(\"https://example.com\")\"\"\"\n    }\nconsole = Console()\npersonality_manager = PersonalityManager(CONFIG_FILE)\nmemory_manager = MemoryManager(MEMORY_FILE)\nui_manager = UIManager()\nlast_query: Optional[str] = None\nlast_response: Optional[str] = None\nlast_code: Optional[str] = None\n\n\ndef start_ollama_server() ->None:\n    if current_backend != 'ollama':\n        return\n    try:\n        requests.get('http://localhost:11434', timeout=1)\n    except requests.exceptions.ConnectionError:\n        print('[cyan]Starting Ollama server...[/]')\n        subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL)\n\n\ndef query_llm(prompt: str) ->str:\n    personality = personality_manager.get_current_personality()\n    system_prompt = personality.get('system_prompt', '') if personality else ''\n    memory_context = memory_manager.get_memory_context()\n    rag_context = ''\n    try:\n        from rag_manager import RAGManager\n        project_root = memory_manager.get_project_root()\n        if project_root:\n            rag_manager = RAGManager()\n            if rag_manager.get_document_count() > 0:\n                results = rag_manager.search(prompt, k=3)\n                if results:\n                    rag_context = '\\n\\nRelevant context from codebase:\\n'\n                    for i, (doc, score, meta) in enumerate(results, 1):\n                        file_path = meta.get('file', 'Unknown')\n                        rag_context += f'{i}. [{file_path}] {doc}\\n'\n    except Exception:\n        pass\n    full_prompt = (\n        f'{system_prompt}\\n\\n{memory_context}{rag_context}\\n\\nUser: {prompt}')\n    with ui_manager.show_spinner('AI is listening and thinking...'):\n        if current_backend == 'ollama':\n            response = query_ollama(full_prompt)\n        elif current_backend == 'openrouter':\n            response = query_openrouter(full_prompt)\n        else:\n            response = '[bold red]Error:[/] Unknown backend'\n    return response\n\n\ndef query_openrouter(prompt: str) ->str:\n    if not OPENROUTER_API_KEY:\n        return '[bold red]Error:[/] OPENROUTER_API_KEY not set.'\n    headers = {'Authorization': f'Bearer {OPENROUTER_API_KEY}',\n        'Content-Type': 'application/json'}\n    payload = {'model': current_model, 'messages': [{'role': 'user',\n        'content': prompt}]}\n    try:\n        response = requests.post(OPENROUTER_API_URL, headers=headers, json=\n            payload, timeout=90)\n        response.raise_for_status()\n        return response.json()['choices'][0]['message']['content']\n    except Exception as e:\n        error_details = ''\n        try:\n            error_details = response.json()\n        except:\n            error_details = response.text if hasattr(response, 'text'\n                ) else str(e)\n        return (\n            f'[bold red]OpenRouter Error:[/] {e}\\n[dim]Details: {error_details}[/dim]'\n            )\n\n\ndef query_ollama(prompt: str) ->str:\n    payload = {'model': current_model, 'prompt': prompt, 'stream': False}\n    try:\n        response = requests.post(OLLAMA_API_URL, json=payload, timeout=90)\n        response.raise_for_status()\n        return response.json()['response']\n    except Exception as e:\n        return f'[bold red]Ollama Error:[/] {e}'\n\n\ndef extract_code(text: str) ->List[tuple[str, str]]:\n    matches = re.findall('```(\\\\w*)\\\\n([\\\\s\\\\S]*?)```', text)\n    return [(lang or 'text', code.strip()) for lang, code in matches\n        ] if matches else []\n\n\ndef list_models(args: list=None) ->None:\n    if current_backend == 'ollama':\n        print('[bold cyan]Popular Ollama Models:[/]')\n        for name, model_id in OLLAMA_MODELS.items():\n            print(\n                f\"{'\u2b50' if model_id == current_model else '  '} [yellow]{name:12}[/] \u2192 {model_id}\"\n                )\n    elif current_backend == 'openrouter':\n        list_openrouter_models(args or [])\n\n\ndef list_openrouter_models(args: list):\n    try:\n        from simple_term_menu import TerminalMenu\n    except ImportError:\n        ui_manager.show_error(\n            \"'simple-term-menu' is required. `pip install simple-term-menu`\")\n        return\n    try:\n        with ui_manager.show_spinner('Fetching models...'):\n            response = requests.get(OPENROUTER_MODELS_API_URL)\n            response.raise_for_status()\n        api_models_data = response.json().get('data', [])\n    except requests.RequestException as e:\n        ui_manager.show_error(f'Error fetching models: {e}')\n        return\n    all_models, sources = [], set()\n    for model_data in api_models_data:\n        if (model_id := model_data.get('id')):\n            sources.add(model_id.split('/')[0])\n            pricing = model_data.get('pricing', {})\n            is_free = pricing.get('prompt') == '0' and pricing.get('completion'\n                ) == '0'\n            all_models.append({'id': model_id, 'name': model_data.get(\n                'name'), 'source': model_id.split('/')[0], 'is_free': is_free})\n    all_models.sort(key=lambda x: (x['source'], x['name']))\n    if args and args[0].lower() == 'sources':\n        print('[bold cyan]Available Model Sources:[/]')\n        [print(f'  [yellow]{s}[/]') for s in sorted(list(sources))]\n        return\n    models_to_display, title = all_models, 'Select an OpenRouter Model'\n    if args:\n        filter_keyword = args[0].lower()\n        title = f\"Select a Model from '{filter_keyword}'\"\n        models_to_display = [m for m in all_models if filter_keyword in m[\n            'source'].lower()]\n        if not models_to_display:\n            ui_manager.show_error(f\"No models for source: '{filter_keyword}'\")\n            return\n    menu_entries = [\n        f\"{'\u2b50' if m['id'] == current_model else '  '} {m['name']} [dim]({m['id']}){' [green](FREE)[/]' if m['is_free'] else ''}[/dim]\"\n         for m in models_to_display]\n    try:\n        cursor_idx = next((i for i, m in enumerate(models_to_display) if m[\n            'id'] == current_model), 0)\n        chosen_index = TerminalMenu(menu_entries, title=f'{title}',\n            cursor_index=cursor_idx, cycle_cursor=True, clear_screen=True\n            ).show()\n        if chosen_index is not None:\n            set_model(models_to_display[chosen_index]['id'])\n        else:\n            print('Model selection cancelled.')\n    except Exception as e:\n        ui_manager.show_error(f'Menu display error: {e}')\n\n\ndef set_model(model_id: str) ->None:\n    global current_model\n    current_model = model_id\n    ui_manager.show_success(f'Model set to: {current_model}')\n\n\ndef switch_backend(backend_name: str) ->None:\n    global current_backend, current_model\n    backend_name = backend_name.lower()\n    if backend_name not in ['ollama', 'openrouter']:\n        ui_manager.show_error(f'Unknown backend: {backend_name}')\n        return\n    current_backend = backend_name\n    current_model = (OLLAMA_MODEL if backend_name == 'ollama' else\n        OPENROUTER_MODEL)\n    ui_manager.show_success(\n        f'Switched to {backend_name} backend with model: {current_model}')\n    if backend_name == 'ollama':\n        start_ollama_server()\n\n\ndef generate_project_manifest(path: str) ->tuple[str, List[str]]:\n    manifest = ''\n    file_paths = []\n    tree = Tree(f'[bold cyan]Project: {os.path.basename(path)}[/]')\n    exclude_dirs = {'__pycache__', '.git', 'venv', 'node_modules', '.idea'}\n    for root, dirs, files in os.walk(path):\n        dirs[:] = [d for d in dirs if d not in exclude_dirs and not d.\n            startswith('.')]\n        relative_path = os.path.relpath(root, path)\n        branch = tree\n        if relative_path != '.':\n            parts = relative_path.split(os.sep)\n            for part in parts:\n                child = next((c for c in branch.children if c.label ==\n                    f'[magenta]{part}[/]'), None)\n                if not child:\n                    child = branch.add(f'[magenta]{part}[/]')\n                branch = child\n        for fname in sorted(files):\n            ext = os.path.splitext(fname)[1]\n            if ext not in ('.py', '.js', '.html', '.css', '.md', '.txt'):\n                continue\n            rel_path = os.path.join(relative_path, fname\n                ) if relative_path != '.' else fname\n            file_paths.append(rel_path)\n            branch.add(f'[green]{fname}[/]' if ext == '.py' else\n                f'[dim]{fname}[/]')\n            manifest += f'File: {rel_path}\\n\\n'\n    console.print(tree)\n    return manifest.strip(), file_paths\n\n\ndef look_command(path: str) ->None:\n    \"\"\"\n    Scans a directory or file and loads it into memory. It can resolve paths\n    relative to the current working directory or the project root in memory.\n    If a new directory is scanned, the previous 'look' context is cleared\n    to ensure the context remains relevant.\n    \"\"\"\n    resolved_path = resolve_file_path(path)\n    if not resolved_path:\n        ui_manager.show_error(f'\u274c Path not found: {path}')\n        return\n    if resolved_path != os.path.abspath(path):\n        ui_manager.show_success(\n            f\"Found '{path}' in project. Using: {resolved_path}\")\n    if os.path.isdir(resolved_path):\n        ui_manager.show_success(\n            \"New project directory detected. Clearing previous 'look' context.\"\n            )\n        memory_manager.memory['look'] = []\n        with ui_manager.show_spinner('Generating project manifest...'):\n            manifest = generate_project_manifest(resolved_path)\n        memory_manager.add_look_data(resolved_path, manifest)\n        ui_manager.show_success('\u2705 Project manifest added to memory.')\n    else:\n        try:\n            with ui_manager.show_spinner('Loading file...'):\n                with open(resolved_path, 'r', encoding='utf-8') as f:\n                    content = f.read().strip()\n            for item in memory_manager.memory['look']:\n                if item.get('file') == resolved_path:\n                    item['content'] = content\n                    memory_manager.save_memory()\n                    ui_manager.show_success(\n                        '\u2705 Refreshed file content in memory.')\n                    return\n            memory_manager.add_look_data(resolved_path, content)\n            ui_manager.show_success('\u2705 File content added to memory.')\n        except Exception as e:\n            ui_manager.show_error(f'\u274c Error reading file: {e}')\n\n\ndef resolve_file_path(path: str) ->Optional[str]:\n    \"\"\"Resolves a file path, checking CWD first, then against project root in memory.\"\"\"\n    if os.path.exists(path):\n        return os.path.abspath(path)\n    project_root = memory_manager.get_project_root()\n    if project_root:\n        full_path = os.path.join(project_root, path)\n        if os.path.exists(full_path):\n            return full_path\n    return None\n\n\ndef _create_prompt_for_file_creation(file_name: str, instruction: str) ->str:\n    \"\"\"\n    Generate a robust prompt for file creation that instructs the AI to act as an expert,\n    produce complete and clean code, and avoid any extra commentary.\n    \"\"\"\n    return f\"\"\"You are an expert programmer tasked with creating a new file. Your goal is to generate complete, production-ready content based on the user's instruction.\n\nIMPORTANT RULES:\n- Provide ONLY the raw file content - no explanations, notes, or commentary outside the file itself\n- Include all necessary imports, boilerplate, and complete implementations\n- If creating code, ensure it's syntactically correct and follows best practices\n- For configuration files, use appropriate formatting (JSON, YAML, etc.)\n- For documentation files, use proper markdown formatting\n\nFile to create: {file_name}\nUser instruction: {instruction}\n\nGenerate the complete file content now:\"\"\"\n\n\ndef handle_file_create_command(file_path: str, instruction: str):\n    \"\"\"\n    Uses the LLM to generate content for a new file based on an instruction.\n    \"\"\"\n    global last_code\n    if os.path.exists(file_path):\n        if ui_manager.get_user_input(\n            f\"File '{file_path}' already exists. Overwrite? (y/n): \").lower(\n            ) not in ['yes', 'y']:\n            ui_manager.show_error('File creation cancelled.')\n            return\n    prompt = _create_prompt_for_file_creation(os.path.basename(file_path),\n        instruction)\n    with ui_manager.show_spinner(\n        f\"AI is generating content for '{file_path}'...\"):\n        response = query_llm(prompt)\n    code_blocks = extract_code(response)\n    if code_blocks:\n        new_content = code_blocks[0][1]\n    else:\n        new_content = response.strip()\n    if not new_content:\n        ui_manager.show_error('AI did not return any content.')\n        print(Panel(response, title=\"[yellow]AI's Raw Response[/]\"))\n        return\n    print(Panel(new_content, title=\n        f'[bold yellow]Proposed content for {file_path}[/]', border_style=\n        'yellow'))\n    if ui_manager.get_user_input('Create this file? (y/n): ').lower() in ['yes'\n        , 'y']:\n        try:\n            FileCreator.create(file_path, new_content)\n            last_code = new_content\n            ui_manager.show_success(f'File created successfully: {file_path}')\n        except IOError as e:\n            ui_manager.show_error(f'Error creating file: {e}')\n    else:\n        ui_manager.show_error('File creation cancelled.')\n\n\ndef look_all_command() ->None:\n    \"\"\"\n    Finds the project manifest in memory, reads every file listed, and adds their content to memory.\n    \"\"\"\n    project_root = memory_manager.get_project_root()\n    if not project_root:\n        ui_manager.show_error(\n            \"No project context in memory. Use 'look <directory>' first to generate a manifest.\"\n            )\n        return\n    manifest_data = None\n    for item in memory_manager.memory.get('look', []):\n        if item.get('file') == project_root and item.get('type'\n            ) == 'directory':\n            manifest_data = item.get('content')\n            break\n    if not manifest_data or not isinstance(manifest_data, (list, tuple)\n        ) or len(manifest_data) != 2:\n        ui_manager.show_error(\n            \"Could not find a valid project manifest in memory. Please run 'look <directory>' again.\"\n            )\n        return\n    file_paths = manifest_data[1]\n    if not file_paths:\n        ui_manager.show_error('No files found in the project manifest.')\n        return\n    total_files = len(file_paths)\n    loaded_count = 0\n    with ui_manager.show_spinner(\n        f'Loading {total_files} files from project manifest...'):\n        for file_path_relative in file_paths:\n            full_path = os.path.join(project_root, file_path_relative)\n            if os.path.isfile(full_path):\n                try:\n                    with open(full_path, 'r', encoding='utf-8') as f:\n                        content = f.read().strip()\n                    if not any(look['file'] == full_path for look in\n                        memory_manager.memory['look']):\n                        memory_manager.add_look_data(full_path, content)\n                        loaded_count += 1\n                except Exception as e:\n                    print(\n                        f\"[yellow]Skipping '{file_path_relative}': {e}[/yellow]\"\n                        )\n    ui_manager.show_success(\n        f'\u2705 Loaded content for {loaded_count} new files into memory.')\n\n\ndef _load_all_project_files_if_needed():\n    \"\"\"\n    Checks if a project is loaded and automatically loads any files from its\n    manifest that are not already in the 'look' memory. This ensures a\n    complete context for editing and refactoring commands.\n    \"\"\"\n    project_root = memory_manager.get_project_root()\n    if not project_root:\n        return\n    manifest_content = None\n    for item in memory_manager.memory.get('look', []):\n        if item.get('file') == project_root and 'File:' in item.get('content',\n            ''):\n            manifest_content = item['content']\n            break\n    if not manifest_content:\n        return\n    existing_file_paths = {item['file'] for item in memory_manager.memory.\n        get('look', []) if item.get('type') == 'file'}\n    file_paths_relative = re.findall('File: (.*)', manifest_content)\n    files_to_load = []\n    for rel_path in file_paths_relative:\n        full_path = os.path.join(project_root, rel_path)\n        if full_path not in existing_file_paths and os.path.isfile(full_path):\n            files_to_load.append((full_path, rel_path))\n    if not files_to_load:\n        return\n    loaded_count = 0\n    with ui_manager.show_spinner(\n        f'Auto-loading {len(files_to_load)} project files for context...'):\n        for full_path, file_path_relative in files_to_load:\n            try:\n                with open(full_path, 'r', encoding='utf-8') as f:\n                    content = f.read().strip()\n                memory_manager.add_look_data(full_path, content)\n                loaded_count += 1\n            except Exception as e:\n                print(f\"[yellow]Skipping '{file_path_relative}': {e}[/yellow]\")\n    if loaded_count > 0:\n        ui_manager.show_success(\n            f'\u2705 Loaded {loaded_count} new file(s) into memory for full project context.'\n            )\n\n\ndef _create_prompt_for_element_selection(file_name: str, instruction: str,\n    elements: List[str], element_structures: Dict[str, Dict]) ->str:\n    \"\"\"\n    Create a helper for the first stage of the 'edit' command. This prompt asks the AI\n    to analyze the user's instruction and intelligently select the most relevant code element to modify.\n    \"\"\"\n    element_details = []\n    for elem in elements:\n        if elem in element_structures:\n            struct = element_structures[elem]\n            detail = (\n                f\"{elem} ({struct['type']}, lines {struct['line_start']}-{struct['line_end']})\"\n                )\n            element_details.append(detail)\n        else:\n            element_details.append(elem)\n    return f\"\"\"You are an expert code analyzer. Your task is to identify what should be modified based on the user's instruction.\n\nFile: {file_name}\nAvailable elements: {', '.join(element_details) if element_details else 'None'}\n\nUser instruction: {instruction}\n\nRESPONSE FORMAT:\nChoose one of these response types:\n1. \"ELEMENT: <element_name>\" - to edit an entire function/class\n2. \"PARTIAL: <element_name> LINES: <start>-<end>\" - to edit specific lines within an element\n3. \"FILE\" - to edit the entire file or multiple elements\n\nRULES:\n- If the instruction mentions specific line numbers or a specific part of a function, use PARTIAL\n- If the instruction targets an entire function/class, use ELEMENT\n- If the instruction requires changes to multiple elements or file structure, use FILE\n- For PARTIAL edits, provide absolute line numbers from the original file\n\nWhat should be edited?\"\"\"\n\n\ndef _create_prompt_for_element_rewrite(file_name: str, element_name: str,\n    instruction: str, original_code: str, is_full_file: bool=False) ->str:\n    \"\"\"\n    Create a helper for the second stage of the 'edit' command. This prompt instructs the AI\n    to rewrite a specific code element (or the whole file) based on the user's request,\n    demanding a complete and syntactically correct code block as output.\n    \"\"\"\n    if is_full_file:\n        return f\"\"\"You are an expert programmer. Rewrite the entire file to accomplish the user's task.\n\nIMPORTANT RULES:\n- Provide ONLY the complete, updated code for the entire file\n- Ensure all syntax is correct and the code is ready to run\n- Preserve existing functionality unless explicitly asked to change it\n- Include all necessary imports and maintain the file's structure\n- Do NOT include any explanations or comments outside the code\n\nFile: {file_name}\nTask: {instruction}\n\nCurrent file content:\n```python\n{original_code}\n```\n\nGenerate the complete updated file now:\"\"\"\n    else:\n        return f\"\"\"You are an expert programmer. Rewrite the specified element to accomplish the user's task.\n\nIMPORTANT RULES:\n- Provide ONLY the complete, updated code for the element\n- Include any necessary imports at the top of your code block\n- Ensure the code is syntactically correct and maintains the same interface\n- Do NOT include explanations or comments outside the code block\n- The code must be a drop-in replacement for the original element\n\nFile: {file_name}\nElement to modify: {element_name}\nTask: {instruction}\n\nCurrent element code:\n```python\n{original_code}\n```\n\nGenerate the complete updated element now:\"\"\"\n\n\ndef _create_prompt_for_partial_edit(file_name: str, element_name: str,\n    instruction: str, original_snippet: str, line_start: int, line_end: int,\n    full_element_code: str) ->str:\n    \"\"\"\n    Create a prompt for partial edits within a function or class.\n    This allows surgical changes to specific parts of code.\n    \"\"\"\n    return f\"\"\"You are an expert programmer. Make a surgical edit to a specific part of a function/class.\n\nCONTEXT:\n- File: {file_name}\n- Element: {element_name}\n- Lines to modify: {line_start}-{line_end}\n- Task: {instruction}\n\nIMPORTANT RULES:\n- Provide ONLY the code that will replace lines {line_start}-{line_end}\n- Your code must fit seamlessly into the existing function\n- Maintain proper indentation (the code will be auto-indented)\n- Do NOT include the function definition or other parts\n- Do NOT include explanations outside the code\n\nFull element for context:\n```python\n{full_element_code}\n```\n\nCode section to replace (lines {line_start}-{line_end}):\n```python\n{original_snippet}\n```\n\nGenerate ONLY the replacement code for the specified lines:\"\"\"\n\n\ndef handle_file_edit_command(file_path: str, instruction: str):\n    \"\"\"\n    Handles the entire workflow for editing a single file, ensuring full\n    project context is loaded before the AI makes any decisions.\n    Now supports partial edits within functions.\n    \"\"\"\n    global last_code\n    _load_all_project_files_if_needed()\n    resolved_path = resolve_file_path(file_path)\n    if not resolved_path:\n        ui_manager.show_error(f'File not found: {file_path}')\n        return\n    if resolved_path != os.path.abspath(file_path):\n        ui_manager.show_success(\n            f\"Found '{file_path}' in project. Using: {resolved_path}\")\n    try:\n        editor = CodeEditor(resolved_path)\n    except (ValueError, FileNotFoundError) as e:\n        ui_manager.show_error(str(e))\n        return\n    elements = editor.list_elements()\n    element_structures = {}\n    for elem in elements:\n        struct = editor.get_element_structure(elem)\n        if struct:\n            element_structures[elem] = struct\n    prompt1 = _create_prompt_for_element_selection(os.path.basename(\n        resolved_path), instruction, elements, element_structures)\n    with ui_manager.show_spinner('AI is analyzing file...'):\n        ai_response = query_llm(prompt1).strip()\n    if ai_response.upper() == 'FILE':\n        ui_manager.show_success('AI has chosen to edit the entire file.')\n        original_snippet = editor.source_code\n        prompt2 = _create_prompt_for_element_rewrite(os.path.basename(\n            resolved_path), 'entire file', instruction, original_snippet,\n            is_full_file=True)\n        edit_type = 'FILE'\n        element_to_edit = None\n        line_range = None\n    elif ai_response.startswith('PARTIAL:'):\n        parts = ai_response.split()\n        element_to_edit = parts[1]\n        if 'LINES:' in ai_response:\n            line_part = ai_response.split('LINES:')[1].strip()\n            if '-' in line_part:\n                line_start, line_end = map(int, line_part.split('-'))\n                line_range = line_start, line_end\n            else:\n                ui_manager.show_error('Invalid line range format')\n                return\n        else:\n            ui_manager.show_error('Missing line range for partial edit')\n            return\n        if element_to_edit not in elements:\n            ui_manager.show_error(f\"Element '{element_to_edit}' not found\")\n            return\n        ui_manager.show_success(\n            f\"AI selected partial edit of '{element_to_edit}' (lines {line_start}-{line_end})\"\n            )\n        original_snippet = editor.get_element_body_snippet(element_to_edit,\n            line_start, line_end)\n        if not original_snippet:\n            original_snippet = editor.get_source_of(element_to_edit)\n        full_element_code = editor.get_source_of(element_to_edit)\n        prompt2 = _create_prompt_for_partial_edit(os.path.basename(\n            resolved_path), element_to_edit, instruction, original_snippet,\n            line_start, line_end, full_element_code)\n        edit_type = 'PARTIAL'\n    elif ai_response.startswith('ELEMENT:'):\n        element_to_edit = ai_response.split(':', 1)[1].strip()\n        if element_to_edit not in elements:\n            ui_manager.show_error(\n                f\"AI identified '{element_to_edit}', which is not a valid element. Aborting.\"\n                )\n            return\n        ui_manager.show_success(f\"AI selected '{element_to_edit}' for editing.\"\n            )\n        original_snippet = editor.get_source_of(element_to_edit)\n        prompt2 = _create_prompt_for_element_rewrite(os.path.basename(\n            resolved_path), element_to_edit, instruction, original_snippet)\n        edit_type = 'ELEMENT'\n        line_range = None\n    else:\n        element_to_edit = ai_response.splitlines()[0]\n        if element_to_edit not in elements:\n            ui_manager.show_error(\n                f\"AI identified '{element_to_edit}', which is not a valid element. Aborting.\"\n                )\n            return\n        ui_manager.show_success(f\"AI selected '{element_to_edit}' for editing.\"\n            )\n        original_snippet = editor.get_source_of(element_to_edit)\n        prompt2 = _create_prompt_for_element_rewrite(os.path.basename(\n            resolved_path), element_to_edit, instruction, original_snippet)\n        edit_type = 'ELEMENT'\n        line_range = None\n    with ui_manager.show_spinner(f'AI is editing...'):\n        response = query_llm(prompt2)\n    code_blocks = extract_code(response)\n    if not code_blocks:\n        ui_manager.show_error('AI did not return a valid code block.')\n        print(Panel(response, title=\"[yellow]AI's Raw Response[/]\"))\n        return\n    new_code = code_blocks[0][1]\n    success = False\n    if edit_type == 'FILE':\n        try:\n            editor.tree = ast.parse(new_code)\n            success = True\n        except SyntaxError as e:\n            ui_manager.show_error(f'AI returned invalid Python syntax: {e}')\n            print(Panel(response, title=\"[yellow]AI's Raw Response[/]\"))\n            return\n    elif edit_type == 'PARTIAL':\n        success = editor.replace_partial(element_to_edit, new_code,\n            line_start=line_range[0], line_end=line_range[1])\n        if not success:\n            ui_manager.show_error('Failed to apply partial edit.')\n            print(Panel(response, title=\"[yellow]AI's Raw Response[/]\"))\n            return\n    else:\n        success = editor.replace_element(element_to_edit, new_code)\n        if not success:\n            ui_manager.show_error(\n                'AI returned invalid code; could not be parsed or applied.')\n            print(Panel(response, title=\"[yellow]AI's Raw Response[/]\"))\n            return\n    if not (diff := editor.get_diff()):\n        ui_manager.show_success('AI made no changes.')\n        return\n    print(Panel(diff, title=\n        f'[bold yellow]Proposed Changes for {resolved_path}[/]'))\n    if ui_manager.get_user_input('Apply changes? (y/n): ').lower() in ['yes',\n        'y']:\n        editor.save_changes()\n        last_code = editor.get_modified_source()\n        ui_manager.show_success(f'Changes saved to {resolved_path}.')\n    else:\n        ui_manager.show_error('Changes discarded.')\n\n\ndef _create_prompt_for_refactor_plan(instruction: str, memory_context: str\n    ) ->str:\n    \"\"\"\n    Create a specialized prompt-generation function for the 'refactor' command.\n    This prompt will explicitly define the required JSON structure for the plan\n    and instruct the AI to act as an expert project manager.\n    \"\"\"\n    return f\"\"\"You are an expert project manager and software architect. Analyze the project context and create a detailed refactoring plan.\n\nYour plan must be a valid JSON object with this exact structure:\n{{\n    \"actions\": [\n        {{\n            \"type\": \"MODIFY\" | \"CREATE\" | \"DELETE\" | \"PARTIAL\",\n            \"file\": \"relative/path/to/file.py\",\n            \"element\": \"function_or_class_name\",  // For MODIFY/DELETE/PARTIAL\n            \"element_name\": \"new_element_name\",    // For CREATE\n            \"line_start\": 10,                      // For PARTIAL only\n            \"line_end\": 20,                        // For PARTIAL only\n            \"reason\": \"Clear explanation of why this change is needed\",\n            \"description\": \"What this action will accomplish\",\n            \"anchor_element\": \"optional_anchor\",   // Optional for CREATE\n            \"position\": \"before\" | \"after\"         // Optional for CREATE\n        }}\n    ]\n}}\n\nACTION TYPES:\n- MODIFY: Change an entire function, class, or method\n- PARTIAL: Change specific lines within a function/class (requires line_start and line_end)\n- CREATE: Add new functions, classes, or files\n- DELETE: Remove functions, classes, variables, or imports\n\nRULES FOR YOUR PLAN:\n- Use PARTIAL when you only need to change a small part of a function\n- Use MODIFY when restructuring an entire function or class\n- Each action must have all required fields based on its type\n- File paths must be relative to the project root\n- Be specific and surgical - avoid unnecessary changes\n- Consider dependencies between changes\n- Order actions logically (e.g., create dependencies before using them)\n\n### Project Context ###\n{memory_context}\n\n### Refactoring Request ###\n{instruction}\n\nGenerate ONLY the JSON plan - no explanations or markdown:\"\"\"\n\n\ndef _get_refactor_plan(instruction: str) ->Optional[List[Dict]]:\n    \"\"\"\n    Generates a refactoring plan from the LLM.\n\n    This function encapsulates the logic for checking project context,\n    constructing a prompt, querying the LLM, and parsing the resulting\n    JSON plan for a refactoring task.\n\n    Args:\n        instruction: The user's high-level refactoring instruction.\n\n    Returns:\n        A list of action dictionaries if a valid plan is generated,\n        otherwise None.\n    \"\"\"\n    if not memory_manager.get_project_root():\n        ui_manager.show_error(\n            \"No project context in memory. Use 'look <directory>' first.\")\n        return None\n    memory_context = memory_manager.get_memory_context()\n    plan_prompt = _create_prompt_for_refactor_plan(instruction, memory_context)\n    with ui_manager.show_spinner('AI is creating an execution plan...'):\n        plan_str = query_llm(plan_prompt)\n    try:\n        match = re.search('\\\\{.*\\\\}', plan_str, re.DOTALL)\n        if not match:\n            raise ValueError('No JSON object found in the response.')\n        plan = json.loads(match.group(0))\n        actions = plan.get('actions', [])\n        if not actions:\n            raise ValueError(\"No 'actions' key found in plan or plan is empty.\"\n                )\n        return actions\n    except (json.JSONDecodeError, ValueError) as e:\n        ui_manager.show_error(f'AI failed to generate a valid plan: {e}')\n        print(Panel(plan_str, title=\"[yellow]AI's Invalid Plan Response[/]\",\n            border_style='yellow'))\n        return None\n\n\ndef _display_and_confirm_plan(plan: Dict) ->bool:\n    \"\"\"\n    Displays the generated execution plan to the user and asks for confirmation.\n\n    This helper function separates the UI interaction of plan confirmation from\n    the main refactoring logic.\n\n    Args:\n        plan: A dictionary, expected to contain an 'actions' key with a list of action dicts.\n\n    Returns:\n        True if the user confirms the plan, False otherwise.\n    \"\"\"\n    actions = plan.get('actions', [])\n    if not actions:\n        ui_manager.show_error('The generated plan is empty. Aborting.')\n        return False\n    ui_manager.show_success('AI has created a plan:')\n    for i, action in enumerate(actions):\n        action_type = action.get('type', 'N/A')\n        element = action.get('element') or action.get('element_name', 'N/A')\n        reason = action.get('reason') or action.get('description', '')\n        file_path = action.get('file', '')\n        if action_type == 'PARTIAL':\n            line_start = action.get('line_start', '?')\n            line_end = action.get('line_end', '?')\n            print(\n                f'  [cyan]{i + 1}. {action_type}:[/] {file_path}/{element} (lines {line_start}-{line_end}) - {reason}'\n                )\n        else:\n            print(\n                f'  [cyan]{i + 1}. {action_type}:[/] {file_path}/{element} - {reason}'\n                )\n    if ui_manager.get_user_input('\\nProceed with this plan? (y/n): ').lower(\n        ) in ['yes', 'y']:\n        return True\n    else:\n        ui_manager.show_error('Execution aborted by user.')\n        return False\n\n\ndef _apply_refactor_changes(editors: Dict[str, CodeEditor]) ->None:\n    \"\"\"\n    Consolidates changes from multiple CodeEditor instances, shows a unified\n    diff, and prompts the user to apply them.\n    \n    This helper function abstracts the final step of a refactor, ensuring\n    all proposed modifications are presented to the user for a final review\n    before any files are written to disk.\n\n    Args:\n        editors: A dictionary mapping absolute file paths to their\n                 corresponding CodeEditor instances which hold the\n                 proposed changes in their AST.\n    \"\"\"\n    full_diff = ''\n    for editor in editors.values():\n        diff = editor.get_diff()\n        if diff:\n            full_diff += diff + '\\n'\n    if not full_diff.strip():\n        ui_manager.show_success('AI made no changes.')\n        return\n    print(Panel(full_diff, title=\n        '[bold yellow]Proposed Project-Wide Changes[/]'))\n    if ui_manager.get_user_input('Apply all changes? (y/n): ').lower() in [\n        'yes', 'y']:\n        for editor in editors.values():\n            editor.save_changes()\n        ui_manager.show_success('\u2705 Project changes applied successfully.')\n    else:\n        ui_manager.show_error('Changes discarded.')\n\n\ndef _create_prompt_for_refactor_action(action_type: str, file_path: str,\n    action_details: Dict) ->str:\n    \"\"\"\n    Create a helper to generate prompts for individual 'CREATE' or 'MODIFY' steps\n    within a refactor plan. This ensures the AI produces code for the specific\n    sub-task in the correct context.\n    \"\"\"\n    if action_type == 'MODIFY':\n        element_name = action_details['element_name']\n        reason = action_details['reason']\n        original_code = action_details['original_code']\n        return f\"\"\"You are implementing a specific refactoring task as part of a larger plan.\n\nREFACTORING CONTEXT:\n- File: {file_path}\n- Element: {element_name}\n- Reason for change: {reason}\n\nRULES:\n- Provide ONLY the complete updated code for the element\n- Include any necessary imports at the top\n- Ensure the code integrates properly with the rest of the file\n- Maintain the same function/class signature unless the change requires otherwise\n- No explanations outside the code block\n\nCurrent element code:\n```python\n{original_code}\n```\n\nGenerate the updated element code:\"\"\"\n    elif action_type == 'CREATE':\n        element_name = action_details['element_name']\n        description = action_details['description']\n        return f\"\"\"You are implementing a specific refactoring task as part of a larger plan.\n\nREFACTORING CONTEXT:\n- File: {file_path}\n- New element to create: {element_name}\n- Purpose: {description}\n\nRULES:\n- Provide ONLY the complete code for the new element\n- Include all necessary imports at the top\n- Follow the coding style and patterns used in the project\n- Ensure the code is production-ready and well-structured\n- For non-Python files, provide the complete file content\n- No explanations outside the code block\n\nGenerate the new element code:\"\"\"\n\n\ndef _process_refactor_action(action: Dict, project_base_path: str, editors:\n    Dict) ->bool:\n    \"\"\"\n    Processes a single refactoring action from the plan.\n\n    This function handles the execution of a single action from the refactoring plan,\n    including LLM code generation and applying changes to in-memory editors or files.\n\n    Args:\n        action: A dictionary containing action details (type, file, element, etc.)\n        project_base_path: The absolute path to the project root\n        editors: Dictionary mapping file paths to their CodeEditor instances\n\n    Returns:\n        True if the action was processed successfully, False otherwise\n    \"\"\"\n    file_path_relative = action.get('file')\n    if not file_path_relative:\n        ui_manager.show_error(\n            f\"Action is missing 'file' key. Skipping: {action}\")\n        return False\n    file_path_absolute = os.path.join(project_base_path, file_path_relative)\n    action_type = action.get('type', '').upper()\n    prompt, element_name = '', ''\n    if action_type == 'MODIFY':\n        element_name = action.get('element')\n        reason = action.get('reason')\n        if file_path_relative.endswith('.py'):\n            if file_path_absolute not in editors:\n                try:\n                    editors[file_path_absolute] = CodeEditor(file_path_absolute\n                        )\n                except Exception as e:\n                    ui_manager.show_error(\n                        f'Error loading file {file_path_absolute}: {e}')\n                    return False\n            editor = editors[file_path_absolute]\n            original_snippet = editor.get_source_of(element_name)\n            if not original_snippet:\n                ui_manager.show_error(\n                    f\"Element '{element_name}' in '{file_path_relative}' not found. Skipping.\"\n                    )\n                return False\n            action_details = {'element_name': element_name, 'reason':\n                reason, 'original_code': original_snippet}\n            prompt = _create_prompt_for_refactor_action('MODIFY',\n                file_path_relative, action_details)\n    elif action_type == 'PARTIAL':\n        element_name = action.get('element')\n        reason = action.get('reason')\n        line_start = action.get('line_start')\n        line_end = action.get('line_end')\n        if not all([element_name, line_start, line_end]):\n            ui_manager.show_error(\n                f'PARTIAL action missing required fields. Skipping: {action}')\n            return False\n        if file_path_relative.endswith('.py'):\n            if file_path_absolute not in editors:\n                try:\n                    editors[file_path_absolute] = CodeEditor(file_path_absolute\n                        )\n                except Exception as e:\n                    ui_manager.show_error(\n                        f'Error loading file {file_path_absolute}: {e}')\n                    return False\n            editor = editors[file_path_absolute]\n            original_snippet = editor.get_element_body_snippet(element_name,\n                line_start, line_end)\n            if not original_snippet:\n                original_snippet = editor.get_source_of(element_name)\n                if not original_snippet:\n                    ui_manager.show_error(\n                        f\"Element '{element_name}' in '{file_path_relative}' not found. Skipping.\"\n                        )\n                    return False\n            full_element_code = editor.get_source_of(element_name)\n            prompt = _create_prompt_for_partial_edit(file_path_relative,\n                element_name, reason, original_snippet, line_start,\n                line_end, full_element_code)\n    elif action_type == 'CREATE':\n        element_name = action.get('element_name')\n        description = action.get('description')\n        action_details = {'element_name': element_name, 'description':\n            description}\n        prompt = _create_prompt_for_refactor_action('CREATE',\n            file_path_relative, action_details)\n    else:\n        ui_manager.show_error(f\"Invalid action type '{action_type}'. Skipping.\"\n            )\n        return False\n    with ui_manager.show_spinner(\n        f\"AI: {action_type} on '{element_name or file_path_relative}'...\"):\n        response = query_llm(prompt)\n    code_blocks = extract_code(response)\n    new_content = code_blocks[0][1] if code_blocks else response.strip()\n    if not new_content:\n        ui_manager.show_error(\n            f'AI failed to generate content for action: {action}')\n        print(Panel(response, title=\"[yellow]AI's Raw Response[/]\"))\n        return False\n    if not file_path_relative.endswith('.py'):\n        try:\n            FileCreator.create(file_path_absolute, new_content)\n            ui_manager.show_success(\n                f\"File '{file_path_relative}' created/updated.\")\n            return True\n        except IOError as e:\n            ui_manager.show_error(\n                f\"Failed to create file '{file_path_relative}': {e}\")\n            return False\n    if file_path_absolute not in editors:\n        try:\n            if not os.path.exists(file_path_absolute):\n                os.makedirs(os.path.dirname(file_path_absolute), exist_ok=True)\n                with open(file_path_absolute, 'w') as f:\n                    f.write('')\n            editors[file_path_absolute] = CodeEditor(file_path_absolute)\n        except Exception as e:\n            ui_manager.show_error(\n                f'Error loading file {file_path_absolute}: {e}')\n            return False\n    editor = editors[file_path_absolute]\n    if action_type == 'MODIFY':\n        if not editor.replace_element(element_name, new_content):\n            ui_manager.show_error(\n                f\"Failed to apply MODIFY change to '{element_name}'.\")\n            print(Panel(new_content, title=\n                f\"[red]Problematic MODIFY Code for '{element_name}'[/]\",\n                border_style='red'))\n            return False\n    elif action_type == 'PARTIAL':\n        if not editor.replace_partial(element_name, new_content, line_start,\n            line_end):\n            ui_manager.show_error(\n                f\"Failed to apply PARTIAL change to '{element_name}'.\")\n            print(Panel(new_content, title=\n                f\"[red]Problematic PARTIAL Code for '{element_name}'[/]\",\n                border_style='red'))\n            return False\n    elif action_type == 'CREATE':\n        anchor = action.get('anchor_element')\n        position = action.get('position', 'after')\n        if not editor.add_element(new_content, anchor_name=anchor, before=\n            position == 'before'):\n            ui_manager.show_error(\n                f\"Failed to apply CREATE change for '{element_name}'.\")\n            print(Panel(new_content, title=\n                f\"[red]Problematic CREATE Code for '{element_name}'[/]\",\n                border_style='red'))\n            return False\n    return True\n\n\ndef handle_project_refactor_command(instruction: str):\n    \"\"\"\n    Orchestrates a multi-file, multi-step code refactoring process.\n    \n    This function serves as a high-level orchestrator that delegates specific tasks\n    to helper functions, improving readability, modularity, and maintainability.\n    \"\"\"\n    _load_all_project_files_if_needed()\n    actions = _get_refactor_plan(instruction)\n    if not actions:\n        return\n    plan = {'actions': actions}\n    if not _display_and_confirm_plan(plan):\n        return\n    editors: Dict[str, CodeEditor] = {}\n    project_base_path = memory_manager.get_project_root()\n    successful_actions = 0\n    total_actions = len(actions)\n    for i, action in enumerate(actions, 1):\n        ui_manager.show_success(f'Processing action {i}/{total_actions}...')\n        action_type = action.get('type', '').upper()\n        file_path_relative = action.get('file')\n        if not file_path_relative:\n            ui_manager.show_error(\n                f\"Action is missing 'file' key. Skipping: {action}\")\n            continue\n        file_path_absolute = os.path.join(project_base_path, file_path_relative\n            )\n        if action_type == 'DELETE':\n            element_name = action.get('element')\n            if not element_name:\n                ui_manager.show_error(\n                    f\"DELETE action missing 'element' key. Skipping: {action}\")\n                continue\n            if not file_path_relative.endswith('.py'):\n                ui_manager.show_error(\n                    f'DELETE actions are only supported for Python files. Skipping.'\n                    )\n                continue\n            if file_path_absolute not in editors:\n                try:\n                    editors[file_path_absolute] = CodeEditor(file_path_absolute\n                        )\n                except Exception as e:\n                    ui_manager.show_error(\n                        f'Error loading file {file_path_absolute}: {e}')\n                    continue\n            editor = editors[file_path_absolute]\n            if editor.delete_element(element_name):\n                successful_actions += 1\n                ui_manager.show_success(\n                    f\"Successfully deleted '{element_name}' from '{file_path_relative}'.\"\n                    )\n            else:\n                ui_manager.show_error(\n                    f\"Failed to delete '{element_name}' from '{file_path_relative}'.\"\n                    )\n        elif _process_refactor_action(action, project_base_path, editors):\n            successful_actions += 1\n        else:\n            ui_manager.show_error(\n                f'Action {i} failed, continuing with remaining actions...')\n    if successful_actions == 0:\n        ui_manager.show_error('No actions were successfully executed.')\n        return\n    elif successful_actions < total_actions:\n        ui_manager.show_error(\n            f'Only {successful_actions}/{total_actions} actions completed successfully.'\n            )\n    else:\n        ui_manager.show_success(\n            f'All {total_actions} actions completed successfully.')\n    _apply_refactor_changes(editors)\n\n\ndef _create_prompt_for_commit_message(diff: str) ->str:\n    \"\"\"\n    Create a dedicated prompt function for the 'commit' command. This prompt will\n    instruct the AI to analyze a git diff and generate a concise commit message\n    following the Conventional Commits standard.\n    \"\"\"\n    return f\"\"\"You are an expert developer writing a Git commit message. Your task is to analyze the provided git diff and create a professional commit message.\n\nCOMMIT MESSAGE RULES:\n- Follow the Conventional Commits specification\n- Format: <type>(<optional scope>): <subject>\n- Types: feat, fix, docs, style, refactor, perf, test, build, ci, chore, revert\n- Subject line: max 50 characters, imperative mood, no period\n- Optional body: explain what and why (not how), wrap at 72 characters\n- Be specific and concise\n- Focus on the intent and impact of the changes\n\nEXAMPLES OF GOOD COMMIT MESSAGES:\n- feat(auth): add OAuth2 integration for Google login\n- fix(api): handle null response in user endpoint\n- refactor(database): optimize query performance for large datasets\n- docs(readme): update installation instructions for Windows\n\nRespond with ONLY the commit message - no markdown, quotes, or explanations.\n\n--- GIT DIFF TO ANALYZE ---\n{diff}\n\nGenerate the commit message:\"\"\"\n\n\ndef handle_commit_command():\n    \"\"\"\n    Orchestrates an AI-assisted Git commit workflow with improved error handling.\n    \"\"\"\n    project_root = memory_manager.get_project_root()\n    if not project_root:\n        ui_manager.show_error(\n            \"No project context in memory. Use 'look <directory>' first.\")\n        return\n    try:\n        git_manager = GitManager(project_root)\n    except ValueError as e:\n        ui_manager.show_error(str(e))\n        return\n    changed_files = git_manager.get_changed_files()\n    if not changed_files:\n        ui_manager.show_success(\n            'No changes to commit. Everything is up to date.')\n        return\n    staged_diff = git_manager.get_diff(staged=True)\n    unstaged_diff = git_manager.get_diff()\n    full_diff = f'{staged_diff}\\n{unstaged_diff}'.strip()\n    if not full_diff.strip():\n        ui_manager.show_success(\n            'No content changes detected (e.g., only file mode changes).')\n        return\n    prompt = _create_prompt_for_commit_message(full_diff)\n    commit_message = query_llm(prompt).strip()\n    if not commit_message:\n        ui_manager.show_error(\n            'AI failed to generate a commit message. Aborting.')\n        return\n    files_to_commit_str = '\\n'.join(f'- {f}' for f in changed_files)\n    plan_panel_content = f\"\"\"[bold]Files to be staged:[/]\n[yellow]{files_to_commit_str}[/]\n\n[bold]AI-Generated Commit Message:[/]\n[green]{commit_message}[/]\"\"\"\n    print(Panel(plan_panel_content, title='[bold cyan]Commit Plan[/]',\n        border_style='cyan'))\n    if ui_manager.get_user_input('\\nProceed with commit? (y/n): ').lower() in [\n        'yes', 'y']:\n        try:\n            git_manager.add(changed_files)\n            ui_manager.show_success('\u2705 Files staged.')\n        except subprocess.CalledProcessError as e:\n            ui_manager.show_error(f'Staging failed: {e.stderr}')\n            return\n        try:\n            git_manager.commit(commit_message)\n            ui_manager.show_success('\u2705 Commit successful.')\n        except subprocess.CalledProcessError as e:\n            ui_manager.show_error(f'Commit failed: {e.stderr}')\n            return\n        if ui_manager.get_user_input('Push changes to remote? (y/n): ').lower(\n            ) in ['yes', 'y']:\n            try:\n                git_manager.push()\n                ui_manager.show_success('\u2705 Push successful.')\n            except subprocess.CalledProcessError as e:\n                ui_manager.show_error(f'Push failed: {e.stderr}')\n        else:\n            ui_manager.show_error('Push cancelled.')\n    else:\n        ui_manager.show_error('Commit aborted by user.')\n\n\ndef handle_rag_query_command(query: str):\n    \"\"\"\n    Handles RAG query commands in the CLI.\n    \n    This function provides a way to query the RAG system from the command line interface.\n    It loads the RAG manager, performs the query, and displays the results.\n    \n    Args:\n        query: The query string to search for in the RAG system.\n    \"\"\"\n    try:\n        rag_manager = RAGManager()\n        if rag_manager.get_document_count() == 0:\n            ui_manager.show_error('RAG index is empty. Add documents first.')\n            return\n        results = rag_manager.search(query, k=3)\n        if not results:\n            ui_manager.show_error('No relevant documents found.')\n            return\n        print(Panel(f'[bold cyan]RAG Query:[/bold cyan] {query}', title=\n            '[bold]Retrieval-Augmented Generation Results[/bold]',\n            border_style='cyan'))\n        for i, (doc, score, metadata) in enumerate(results, 1):\n            file_info = metadata.get('file', 'Unknown source')\n            content_preview = doc[:200] + '...' if len(doc) > 200 else doc\n            result_panel = Panel(\n                f\"\"\"[dim]Source:[/] {file_info}\n[dim]Relevance:[/] {score:.4f}\n\n{content_preview}\"\"\"\n                , title=f'[bold]Result {i}[/bold]', border_style='blue',\n                expand=False)\n            print(result_panel)\n        if ui_manager.get_user_input(\n            '\\nGenerate detailed response with AI? (y/n): ').lower() in ['yes',\n            'y']:\n            context = '\\n\\n'.join([\n                f'Document {i} (Score: {score:.4f}):\\n{doc}' for i, (doc,\n                score, _) in enumerate(results, 1)])\n            prompt = f\"\"\"Based on the following retrieved documents, please answer the query: \"{query}\"\n\nRetrieved Documents:\n{context}\n\nPlease provide a comprehensive answer based only on the information in the documents above.\nIf the documents don't contain enough information to answer the query, please say so.\"\"\"\n            with ui_manager.show_spinner('AI is generating response...'):\n                response = query_llm(prompt)\n            print(Panel(response, title=\n                '[bold green]AI-Generated Response[/bold green]',\n                border_style='green'))\n    except Exception as e:\n        ui_manager.show_error(f'Error processing RAG query: {e}')\n        if os.getenv('OMNIFORGE_DEBUG'):\n            import traceback\n            traceback.print_exc()\n\n\ndef handle_rag_query_command(query: str):\n    \"\"\"\n    Handles RAG query commands in the CLI.\n    \n    This function provides a way to query the RAG system from the command line interface.\n    It loads the RAG manager, performs the query, and displays the results.\n    \n    Args:\n        query: The query string to search for in the RAG system.\n    \"\"\"\n    try:\n        rag_manager = RAGManager()\n        if rag_manager.get_document_count() == 0:\n            ui_manager.show_error('RAG index is empty. Add documents first.')\n            return\n        results = rag_manager.search(query, k=3)\n        if not results:\n            ui_manager.show_error('No relevant documents found.')\n            return\n        print(Panel(f'[bold cyan]RAG Query:[/bold cyan] {query}', title=\n            '[bold]Retrieval-Augmented Generation Results[/bold]',\n            border_style='cyan'))\n        for i, (doc, score, metadata) in enumerate(results, 1):\n            file_info = metadata.get('file', 'Unknown source')\n            content_preview = doc[:200] + '...' if len(doc) > 200 else doc\n            result_panel = Panel(\n                f\"\"\"[dim]Source:[/] {file_info}\n[dim]Relevance:[/] {score:.4f}\n\n{content_preview}\"\"\"\n                , title=f'[bold]Result {i}[/bold]', border_style='blue',\n                expand=False)\n            print(result_panel)\n        if ui_manager.get_user_input(\n            '\\nGenerate detailed response with AI? (y/n): ').lower() in ['yes',\n            'y']:\n            context = '\\n\\n'.join([\n                f'Document {i} (Score: {score:.4f}):\\n{doc}' for i, (doc,\n                score, _) in enumerate(results, 1)])\n            prompt = f\"\"\"Based on the following retrieved documents, please answer the query: \"{query}\"\n\nRetrieved Documents:\n{context}\n\nPlease provide a comprehensive answer based only on the information in the documents above.\nIf the documents don't contain enough information to answer the query, please say so.\"\"\"\n            with ui_manager.show_spinner('AI is generating response...'):\n                response = query_llm(prompt)\n            print(Panel(response, title=\n                '[bold green]AI-Generated Response[/bold green]',\n                border_style='green'))\n    except Exception as e:\n        ui_manager.show_error(f'Error processing RAG query: {e}')\n        if os.getenv('OMNIFORGE_DEBUG'):\n            import traceback\n            traceback.print_exc()\n\n\ndef interactive_mode() ->None:\n    global last_query, last_response, last_code\n    try:\n        from Testing.overlay_engine import show_sequential_popup\n        gui_available = True\n    except ImportError:\n        gui_available = False\n    print(Panel(\n        \"\"\"[bold cyan]Omni Interactive Mode[/]\n[dim]Type 'help' for commands, 'exit' to quit.[/dim]\"\"\"\n        , border_style='cyan'))\n    personality_name = personality_manager.get_current_personality().get('name'\n        , 'Default')\n    gui_enabled = False\n    if gui_available:\n        try:\n            with open(CONFIG_FILE, 'r') as f:\n                config = json.load(f)\n                if config.get('gui_enabled', False):\n                    gui_enabled = True\n        except (FileNotFoundError, json.JSONDecodeError):\n            pass\n    refresh_status_panel(personality_name)\n    while True:\n        try:\n            user_input = ui_manager.get_user_input('\\n> ')\n            if not user_input:\n                continue\n            command, *args = user_input.split(maxsplit=1)\n            arg_str = args[0] if args else ''\n            if command == 'exit':\n                memory_manager.save_memory()\n                print('[bold cyan]Goodbye![/]')\n                break\n            elif command == 'help':\n                print(\n                    \"\"\"[bold]Commands:[/]\n\n  [bold cyan]Core & Project Commands[/]\n  [yellow]send <prompt>[/]        - Ask the LLM a question.\n  [yellow]look <path>[/]          - Read file or scan directory into memory.\n  [yellow]look_all[/]            - Recursively scan the project directory into memory.\n  [yellow]create <file> \"instr\"[/] - Create a new file using AI.\n  [yellow]edit <file> \"instr\"[/]   - Edit a specific file using AI.\n  [yellow]refactor \"instr\"[/]      - Refactor project in memory based on instruction.\n  [yellow]commit[/]               - Commit changes with an AI-generated message.\n  [yellow]rag <query>[/]         - Query the RAG system for context retrieval.\n\n  [bold cyan]File & Code Management[/]\n  [yellow]save <filename>[/]     - Save last AI response to a file.\n  [yellow]list[/]               - List saved files.\n  [yellow]run[/]                 - Run the last generated Python code.\n\n  [bold cyan]Session & Config[/]\n  [yellow]history[/]             - Show the full chat history.\n  [yellow]memory clear[/]        - Clear the chat and file memory.\n  [yellow]backend <name>[/]      - Switch AI backend (e.g., openrouter, ollama).\n  [yellow]models [src][/]        - Interactively list and select models.\n  [yellow]set model <id>[/]      - Set the model directly by its ID.\n  [yellow]personality <cmd>[/]   - Manage AI personalities ('list', 'set', 'add').\n\"\"\"\n                    )\n            elif command == 'send':\n                last_query = arg_str\n                response = query_llm(arg_str)\n                last_response = response\n                if gui_enabled:\n                    threading.Thread(target=show_sequential_popup, args=(\n                        100, 100, response, f'Omni - {personality_name}'),\n                        daemon=True).start()\n                print(Panel(response, title='[cyan]Response[/]'))\n                if (code_blocks := extract_code(response)):\n                    last_code = code_blocks[0][1]\n            elif command == 'look':\n                look_command(arg_str)\n            elif command == 'look_all':\n                look_all_command()\n            elif command == 'create':\n                try:\n                    file_path, instruction = arg_str.split(' ', 1)\n                    handle_file_create_command(file_path.strip('\"'),\n                        instruction.strip('\"'))\n                except (ValueError, IndexError):\n                    ui_manager.show_error(\n                        'Usage: create <file_path> \"<instruction>\"')\n            elif command == 'edit':\n                try:\n                    file_path, instruction = arg_str.split(' ', 1)\n                    handle_file_edit_command(file_path.strip('\"'),\n                        instruction.strip('\"'))\n                except (ValueError, IndexError):\n                    ui_manager.show_error(\n                        'Usage: edit <file_path> \"<instruction>\"')\n            elif command == 'refactor':\n                if not arg_str:\n                    ui_manager.show_error('Usage: refactor \"<instruction>\"')\n                else:\n                    handle_project_refactor_command(arg_str.strip('\"'))\n            elif command == 'commit':\n                handle_commit_command()\n            elif command == 'models':\n                list_models(arg_str.split())\n            elif command == 'set' and arg_str.startswith('model '):\n                set_model(arg_str[6:])\n            elif command == 'backend':\n                switch_backend(arg_str)\n            elif command == 'history':\n                ui_manager.display_history(memory_manager.get_memory_context())\n            elif command == 'memory' and arg_str == 'clear':\n                memory_manager.clear_memory()\n                ui_manager.show_success('\u2705 Memory cleared')\n            elif command == 'personality':\n                p_args = arg_str.split(maxsplit=1)\n                cmd = p_args[0] if p_args else ''\n                p_arg_str = p_args[1] if len(p_args) > 1 else ''\n                if cmd == 'list':\n                    for p in personality_manager.list_personalities():\n                        print(f\"- {p['name']}: {p['description']}\")\n                elif cmd == 'set' and p_arg_str:\n                    if personality_manager.set_current_personality(p_arg_str):\n                        personality_name = p_arg_str\n                        ui_manager.show_success(\n                            f'Set personality to {personality_name}')\n                    else:\n                        ui_manager.show_error('Personality not found.')\n                else:\n                    ui_manager.show_error(\n                        \"Invalid personality command. Use 'list' or 'set <name>'.\"\n                        )\n            elif command == 'run':\n                run_python_code()\n            elif command == 'save':\n                if last_response:\n                    save_code(last_response, arg_str or\n                        f\"omni_save_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n                        )\n                else:\n                    ui_manager.show_error('No response to save.')\n            elif command == 'list':\n                files = sorted(os.listdir(DEFAULT_SAVE_DIR))\n                print('\\n'.join(f'  - {f}' for f in files) if files else\n                    '[yellow]No saved files.[/]')\n            elif command == 'rag':\n                if not arg_str:\n                    ui_manager.show_error('Usage: rag \"<query>\"')\n                else:\n                    from rag_manager import RAGManager\n                    project_root = memory_manager.get_project_root()\n                    if not project_root:\n                        ui_manager.show_error(\n                            \"No project context in memory. Use 'look <directory>' first.\"\n                            )\n                        continue\n                    rag = RAGManager()\n                    if rag.get_document_count() == 0:\n                        ui_manager.show_error(\n                            'RAG index is empty. Please add documents first.')\n                        continue\n                    results = rag.search(arg_str, k=3)\n                    if not results:\n                        ui_manager.show_error('No relevant documents found.')\n                        continue\n                    print(Panel('[bold]RAG Results:[/]', title=\n                        '[cyan]Retrieval-Augmented Generation[/]'))\n                    for i, (content, score, metadata) in enumerate(results, 1):\n                        file_path = metadata.get('file', 'Unknown')\n                        print(\n                            f'[bold cyan]{i}. {file_path}[/] (Score: {score:.4f})'\n                            )\n                        print(Panel(content[:500] + '...' if len(content) >\n                            500 else content, border_style='dim'))\n                    follow_up = ui_manager.get_user_input(\n                        \"\"\"\nWould you like to ask a follow-up question with this context? (y/n): \"\"\"\n                        )\n                    if follow_up.lower() in ['y', 'yes']:\n                        follow_up_query = ui_manager.get_user_input(\n                            'Follow-up query: ')\n                        if follow_up_query:\n                            context_parts = [\n                                f'Document {i} (Score: {score:.4f}):\\n{content}'\n                                 for i, (content, score, _) in enumerate(\n                                results, 1)]\n                            context = '\\n\\n'.join(context_parts)\n                            rag_prompt = f\"\"\"Based on the following context, please answer the question.\n\nContext:\n{context}\n\nQuestion: {follow_up_query}\"\"\"\n                            response = query_llm(rag_prompt)\n                            print(Panel(response, title=\n                                '[cyan]RAG-Augmented Response[/]'))\n            else:\n                ui_manager.show_error(\"Unknown command. Type 'help'.\")\n            refresh_status_panel(personality_name)\n        except KeyboardInterrupt:\n            memory_manager.save_memory()\n            print('\\n[bold cyan]Goodbye![/]')\n            break\n        except Exception as e:\n            ui_manager.show_error(f'An unexpected error occurred: {e}')\n\n\ndef run_python_code() ->None:\n    global last_code\n    if not last_code:\n        ui_manager.show_error('No Python code in memory to run.')\n        return\n    temp_file = os.path.join(DEFAULT_SAVE_DIR, 'temp_run.py')\n    try:\n        with open(temp_file, 'w') as f:\n            f.write(last_code)\n        print('[bold cyan]\\n--- Running Code ---\\n[/]')\n        subprocess.run([sys.executable, temp_file], check=True)\n        print('[bold cyan]\\n--- Code Finished ---\\n[/]')\n    except Exception as e:\n        ui_manager.show_error(f'Error running code: {e}')\n    finally:\n        if os.path.exists(temp_file):\n            os.remove(temp_file)\n\n\ndef save_code(content: str, filename: str) ->None:\n    filepath = os.path.join(DEFAULT_SAVE_DIR, filename)\n    try:\n        with open(filepath, 'w') as f:\n            f.write(content)\n        ui_manager.show_success(f'Saved to: {filepath}')\n    except IOError as e:\n        ui_manager.show_error(f'Error saving file: {e}')\n\n\ndef main() ->None:\n    try:\n        import astor\n    except ImportError:\n        print(\"[bold red]Error:[/] 'astor' is required. `pip install astor`\")\n        sys.exit(1)\n    try:\n        import simple_term_menu\n    except ImportError:\n        print(\n            \"[bold red]Error:[/] 'simple-term-menu' is required. `pip install simple-term-menu`\"\n            )\n        sys.exit(1)\n    parser = argparse.ArgumentParser(description=\n        'Omni - AI-powered code tool', add_help=False)\n    parser.add_argument('command', nargs='?', help='Main command.')\n    parser.add_argument('args', nargs='*', help='Arguments for the command.')\n    parser.add_argument('-h', '--help', action='store_true')\n    args, _ = parser.parse_known_args()\n    if args.help or not args.command:\n        interactive_mode()\n    elif args.command == 'look' and args.args:\n        look_command(args.args[0])\n    elif args.command == 'edit' and len(args.args) >= 2:\n        handle_file_edit_command(args.args[0], ' '.join(args.args[1:]))\n    elif args.command == 'models':\n        list_models(args.args)\n    else:\n        interactive_mode()\n\n\ndef refresh_status_panel(personality_name: str) ->None:\n    ui_manager.display_status_panel(personality_name, current_backend,\n        current_model, len(memory_manager.memory.get('chat', [])), len(\n        memory_manager.memory.get('look', [])))\n\n\nif __name__ == '__main__':\n    main()\n",
    "file": "/mnt/ProjectData/omni/omni.py"
  },
  {
    "id": 22,
    "hash": "db1585da7840e9dc17f3ab8c0a19f6f2",
    "content": "import json\nfrom typing import Dict, List, Optional\n\nclass PersonalityManager:\n    \"\"\"Manages AI personalities via JSON config.\"\"\"\n\n    def __init__(self, config_file: str):\n        self.config_file = config_file\n        self.personalities: List[Dict[str, str]] = self.load_personalities()\n        self.current_personality: Optional[str] = \"default\"  # Default\n\n    def load_personalities(self) -> List[Dict[str, str]]:\n        \"\"\"Load personalities from JSON; auto-create if missing.\"\"\"\n        default = [{\"name\": \"default\", \"description\": \"Helpful assistant\", \"system_prompt\": \"You are a helpful AI assistant.\"}]\n        default_gui = {\n            \"gui_enabled\": True,\n            \"wake_word\": \"Jarvis\",\n            \"overlay_opacity\": 0.8,\n            \"font_size\": 16\n        }\n        try:\n            with open(self.config_file, 'r') as f:\n                data = json.load(f)\n                if \"personalities\" not in data:\n                    data[\"personalities\"] = default\n                if \"gui_enabled\" not in data:\n                    data.update(default_gui)\n                    self.save_personalities(data[\"personalities\"])  # Save with defaults\n                \n                return data[\"personalities\"]\n        except FileNotFoundError:\n            self.save_personalities(default)\n            return default\n        except json.JSONDecodeError:\n            print(\"[yellow]Invalid config. Resetting to default.[/]\")\n            self.save_personalities(default)\n            return default\n\n    def save_personalities(self, personalities: List[Dict[str, str]]) -> None:\n        \"\"\"Save with GUI config.\"\"\"\n        data = {\"personalities\": personalities}\n        data.update({\"gui_enabled\": True, \"wake_word\": \"Jarvis\", \"overlay_opacity\": 0.8, \"font_size\": 16})  # Defaults\n        with open(self.config_file, 'w') as f:\n            json.dump(data, f, indent=4)\n\n    def add_personality(self, name: str, description: str, system_prompt: str) -> None:\n        \"\"\"Add a new personality.\"\"\"\n        self.personalities.append({\"name\": name, \"description\": description, \"system_prompt\": system_prompt})\n        self.save_personalities(self.personalities)\n\n    def list_personalities(self) -> List[Dict[str, str]]:\n        \"\"\"List all personalities.\"\"\"\n        return self.personalities\n\n    def set_current_personality(self, name: str) -> bool:\n        \"\"\"Set the current personality.\"\"\"\n        for p in self.personalities:\n            if p[\"name\"] == name:\n                self.current_personality = name\n                return True\n        return False\n\n    def get_current_personality(self) -> Dict[str, str]:\n        \"\"\"Get the current personality dict.\"\"\"\n        for p in self.personalities:\n            if p[\"name\"] == self.current_personality:\n                return p\n        return {}  # Fallback",
    "file": "/mnt/ProjectData/omni/personality_manager.py"
  },
  {
    "id": 23,
    "hash": "2914352eefed9d60de098e021dd79b67",
    "content": "import os\nimport sys\nimport argparse\nfrom typing import List\nfrom rag_manager import RAGManager\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__),\n    '..')))\n\n\ndef create_sample_data() ->List[str]:\n    \"\"\"Create sample documents for the RAG example.\"\"\"\n    return [\n        'RAG stands for Retrieval-Augmented Generation, a technique that combines information retrieval with language generation.'\n        ,\n        'The RAG model retrieves relevant documents from a knowledge base and uses them to generate more accurate responses.'\n        ,\n        'FAISS is a library for efficient similarity search and clustering of dense vectors, often used in RAG systems.'\n        ,\n        'Sentence transformers are used to create embeddings for documents and queries in RAG systems.'\n        ,\n        'Retrieval-Augmented Generation improves language models by allowing them to access external knowledge sources.'\n        ,\n        'In RAG systems, documents are indexed and searchable by their semantic embeddings rather than just keywords.'\n        ,\n        'Python is a great language for implementing RAG systems due to libraries like sentence-transformers and faiss.'\n        ,\n        'The retrieval component of RAG is crucial for finding relevant information before generation.'\n        ]\n\n\ndef main():\n    \"\"\"Main function demonstrating RAG through CLI.\"\"\"\n    parser = argparse.ArgumentParser(description='RAG CLI Example')\n    parser.add_argument('--query', '-q', type=str, help='Query to search for')\n    parser.add_argument('--add', '-a', type=str, help=\n        'Add a new document to the index')\n    parser.add_argument('--list', '-l', action='store_true', help=\n        'List all documents in the index')\n    parser.add_argument('--clear', '-c', action='store_true', help=\n        'Clear the index')\n    parser.add_argument('--init', '-i', action='store_true', help=\n        'Initialize with sample data')\n    args = parser.parse_args()\n    rag_manager = RAGManager()\n    if args.clear:\n        rag_manager.clear_index()\n        print('Index cleared.')\n        return\n    if args.init:\n        documents = create_sample_data()\n        rag_manager.add_documents(documents)\n        print(f'Added {len(documents)} sample documents to index.')\n        return\n    if args.add:\n        rag_manager.add_documents([args.add])\n        print(f'Added document: {args.add}')\n        return\n    if args.list:\n        if rag_manager.get_document_count() == 0:\n            print(\n                'No documents in index. Use --init to add sample data or --add to add documents.'\n                )\n        else:\n            print(\n                f'Documents in index ({rag_manager.get_document_count()} total):'\n                )\n            for i, meta in enumerate(rag_manager.metadata):\n                print(f\"  {i + 1}. {meta['content']}\")\n        return\n    if args.query:\n        if rag_manager.get_document_count() == 0:\n            print(\n                'Index is empty. Use --init to add sample data or --add to add documents.'\n                )\n            return\n        results = rag_manager.search(args.query, k=3)\n        print(f\"Top 3 results for '{args.query}':\")\n        for i, (doc, score, meta) in enumerate(results, 1):\n            print(f'  {i}. [Score: {score:.4f}] {doc}')\n        return\n    parser.print_help()\n\n\ndef main():\n    \"\"\"Main function demonstrating RAG through CLI.\"\"\"\n    parser = argparse.ArgumentParser(description='RAG CLI Example')\n    parser.add_argument('--query', '-q', type=str, help='Query to search for')\n    parser.add_argument('--add', '-a', type=str, help=\n        'Add a new document to the index')\n    parser.add_argument('--list', '-l', action='store_true', help=\n        'List all documents in the index')\n    parser.add_argument('--clear', '-c', action='store_true', help=\n        'Clear the index')\n    parser.add_argument('--init', '-i', action='store_true', help=\n        'Initialize with sample data')\n    args = parser.parse_args()\n    rag_manager = RAGManager()\n    if args.clear:\n        rag_manager.clear_index()\n        print('Index cleared.')\n        return\n    if args.init:\n        documents = create_sample_data()\n        rag_manager.add_documents(documents)\n        print(f'Added {len(documents)} sample documents to index.')\n        return\n    if args.add:\n        rag_manager.add_documents([args.add])\n        print(f'Added document: {args.add}')\n        return\n    if args.list:\n        if rag_manager.get_document_count() == 0:\n            print(\n                'No documents in index. Use --init to add sample data or --add to add documents.'\n                )\n        else:\n            print(\n                f'Documents in index ({rag_manager.get_document_count()} total):'\n                )\n            for i, meta in enumerate(rag_manager.metadata):\n                print(f\"  {i + 1}. {meta['content']}\")\n        return\n    if args.query:\n        if rag_manager.get_document_count() == 0:\n            print(\n                'Index is empty. Use --init to add sample data or --add to add documents.'\n                )\n            return\n        results = rag_manager.search(args.query, k=3)\n        print(f\"Top 3 results for '{args.query}':\")\n        for i, (doc, score, meta) in enumerate(results, 1):\n            print(f'  {i}. [Score: {score:.4f}] {doc}')\n        return\n    parser.print_help()\n\n\nif __name__ == '__main__':\n    main()\n",
    "file": "/mnt/ProjectData/omni/rag_cli_example.py"
  },
  {
    "id": 24,
    "hash": "c27b11d2eaa8e13cdf8e698417efb75e",
    "content": "import os\nimport sys\nfrom typing import List, Dict, Tuple\n\"\"\"\nExample script demonstrating RAG (Retrieval-Augmented Generation) usage.\n\nThis script shows how to use a simple RAG implementation to answer questions\nbased on a given context or knowledge base.\n\"\"\"\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__),\n    '..')))\n\n\nclass SimpleRAG:\n    \"\"\"A simple RAG implementation for demonstration purposes.\"\"\"\n\n    def __init__(self, knowledge_base: List[str]):\n        \"\"\"\n        Initialize the RAG with a knowledge base.\n        \n        Args:\n            knowledge_base: A list of strings representing the knowledge base.\n        \"\"\"\n        self.knowledge_base = knowledge_base\n\n    def retrieve(self, query: str, top_k: int=3) ->List[str]:\n        \"\"\"\n        Retrieve relevant documents from the knowledge base.\n        \n        This is a simplified implementation using keyword matching.\n        In a real implementation, you would use embeddings and vector search.\n        \n        Args:\n            query: The query string.\n            top_k: Number of top documents to retrieve.\n            \n        Returns:\n            A list of relevant documents.\n        \"\"\"\n        query_words = set(query.lower().split())\n        scores = []\n        for doc in self.knowledge_base:\n            doc_words = set(doc.lower().split())\n            score = len(query_words.intersection(doc_words))\n            scores.append((score, doc))\n        scores.sort(reverse=True)\n        return [doc for score, doc in scores[:top_k]]\n\n    def generate(self, query: str, retrieved_docs: List[str]) ->str:\n        \"\"\"\n        Generate an answer based on the query and retrieved documents.\n        \n        In a real implementation, this would use an LLM to generate the response.\n        For this example, we'll create a simple template-based response.\n        \n        Args:\n            query: The query string.\n            retrieved_docs: The retrieved documents.\n            \n        Returns:\n            A generated answer.\n        \"\"\"\n        context = '\\n'.join(retrieved_docs)\n        prompt = f\"\"\"\n        Context information:\n        {context}\n        \n        Question: {query}\n        \n        Based on the context provided above, please answer the question.\n        If the context doesn't contain relevant information, say so.\n        \"\"\"\n        return self._simple_response_generator(query, retrieved_docs)\n\n    def _simple_response_generator(self, query: str, retrieved_docs: List[str]\n        ) ->str:\n        \"\"\"\n        A simple response generator for demonstration.\n        \"\"\"\n        for doc in retrieved_docs:\n            if 'example' in doc.lower():\n                return (\n                    f'Based on the context provided, I found information about examples. {query} relates to the examples in the knowledge base.'\n                    )\n        return (\n            f\"I couldn't find specific information about '{query}' in the provided context. Please provide more details or check the knowledge base.\"\n            )\n\n    def query(self, query: str, top_k: int=3) ->str:\n        \"\"\"\n        Process a query through the full RAG pipeline.\n        \n        Args:\n            query: The query string.\n            top_k: Number of top documents to retrieve.\n            \n        Returns:\n            The generated answer.\n        \"\"\"\n        retrieved_docs = self.retrieve(query, top_k)\n        answer = self.generate(query, retrieved_docs)\n        return answer\n\n\ndef main():\n    \"\"\"Main function demonstrating the RAG implementation.\"\"\"\n    knowledge_base = ['This is an example document about machine learning.',\n        'Natural language processing is a subfield of artificial intelligence.'\n        , 'Python is a popular programming language for data science.',\n        'The quick brown fox jumps over the lazy dog.',\n        'RAG stands for Retrieval-Augmented Generation.',\n        'Vector databases are used for similarity search in RAG systems.',\n        'Transformers are a type of neural network architecture.',\n        'This example shows how to implement a simple RAG system.']\n    rag = SimpleRAG(knowledge_base)\n    queries = ['What is RAG?', 'How is Python used in data science?',\n        'Tell me about machine learning']\n    print('Simple RAG Example')\n    print('=' * 50)\n    for query in queries:\n        print(f'\\nQuery: {query}')\n        answer = rag.query(query)\n        print(f'Answer: {answer}')\n        print('-' * 30)\n    print(\"\\nInteractive Mode (type 'quit' to exit):\")\n    while True:\n        try:\n            user_query = input('\\nEnter your question: ').strip()\n            if user_query.lower() in ['quit', 'exit', 'q']:\n                break\n            if user_query:\n                answer = rag.query(user_query)\n                print(f'Answer: {answer}')\n        except KeyboardInterrupt:\n            print('\\nGoodbye!')\n            break\n        except EOFError:\n            break\n\n\nif __name__ == '__main__':\n    main()\n",
    "file": "/mnt/ProjectData/omni/rag_example.py"
  },
  {
    "id": 25,
    "hash": "f151986bf2efa7602242590a357fab83",
    "content": "import os\nimport json\nimport hashlib\nfrom typing import List, Dict, Optional, Tuple\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nimport faiss\nfrom pathlib import Path\n\n\nclass RAGManager:\n    \"\"\"Manages Retrieval-Augmented Generation operations using sentence transformers.\"\"\"\n\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\n        Optional[str]=None):\n        \"\"\"\n    Initialize the RAG manager.\n\n    Args:\n        model_name: Name of the sentence transformer model to use\n        index_path: Path to save/load the FAISS index\n    \"\"\"\n        self.model_name = model_name\n        from vectordb_manager import VectorDBManager\n        self.vectordb = VectorDBManager(model_name, index_path)\n        self.model = self.vectordb.model\n        self.index_path = self.vectordb.index_path\n        self.metadata_path = self.vectordb.metadata_path\n        self.dimension = self.vectordb.dimension\n        self.index = self.vectordb.index\n        self.metadata = self.vectordb.metadata\n\n    def _initialize_index(self):\n        \"\"\"Initialize or load the FAISS index.\"\"\"\n        if os.path.exists(self.index_path) and os.path.exists(self.\n            metadata_path):\n            self.index = faiss.read_index(self.index_path)\n            with open(self.metadata_path, 'r') as f:\n                self.metadata = json.load(f)\n        else:\n            self.index = faiss.IndexFlatIP(self.dimension)\n            self.metadata = []\n\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\n        Dict]]=None):\n        \"\"\"\n        Add documents to the RAG index.\n\n        Args:\n            documents: List of text documents to add\n            metadatas: Optional list of metadata for each document\n        \"\"\"\n        if metadatas is None:\n            metadatas = [{}] * len(documents)\n        for i, meta in enumerate(metadatas):\n            if 'file' not in meta:\n                meta['file'] = f'document_{len(self.metadata) + i}'\n        self.vectordb.add_documents(documents, metadatas)\n        self.metadata = self.vectordb.metadata\n\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\n        \"\"\"\n        Search for relevant documents.\n\n        Args:\n            query: Query string\n            k: Number of results to return\n\n        Returns:\n            List of (document, score, metadata) tuples\n        \"\"\"\n        from vectordb_manager import VectorDBManager\n        temp_vdb = VectorDBManager(model_name=self.model_name, index_path=\n            self.index_path)\n        results = temp_vdb.search(query, k)\n        return results\n\n    def _save_index(self):\n        \"\"\"Save the FAISS index and metadata to disk.\"\"\"\n        faiss.write_index(self.index, self.index_path)\n        with open(self.metadata_path, 'w') as f:\n            json.dump(self.metadata, f, indent=2)\n\n    def get_document_count(self) ->int:\n        \"\"\"Get the number of documents in the index.\"\"\"\n        return len(self.metadata)\n\n    def clear_index(self):\n        \"\"\"Clear the index and metadata.\"\"\"\n        self.index = faiss.IndexFlatIP(self.dimension)\n        self.metadata = []\n        self._save_index()\n\n\nclass RAGManager:\n    \"\"\"Manages Retrieval-Augmented Generation operations using sentence transformers.\"\"\"\n\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\n        Optional[str]=None):\n        \"\"\"\n        Initialize the RAG manager.\n\n        Args:\n            model_name: Name of the sentence transformer model to use\n            index_path: Path to save/load the FAISS index\n        \"\"\"\n        self.model_name = model_name\n        from vectordb_manager import VectorDBManager\n        self.vectordb = VectorDBManager(model_name, index_path)\n        self.model = self.vectordb.model\n        self.index_path = self.vectordb.index_path\n        self.metadata_path = self.vectordb.metadata_path\n        self.dimension = self.vectordb.dimension\n        self.index = self.vectordb.index\n        self.metadata = self.vectordb.metadata\n\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\n        Dict]]=None):\n        \"\"\"\n        Add documents to the RAG index.\n\n        Args:\n            documents: List of text documents to add\n            metadatas: Optional list of metadata for each document\n        \"\"\"\n        if metadatas is None:\n            metadatas = [{}] * len(documents)\n        for i, meta in enumerate(metadatas):\n            if 'file' not in meta:\n                meta['file'] = f'document_{len(self.metadata) + i}'\n        self.vectordb.add_documents(documents, metadatas)\n        self.metadata = self.vectordb.metadata\n\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\n        \"\"\"\n        Search for relevant documents.\n\n        Args:\n            query: Query string\n            k: Number of results to return\n\n        Returns:\n            List of (document, score, metadata) tuples\n        \"\"\"\n        from vectordb_manager import VectorDBManager\n        temp_vdb = VectorDBManager(model_name=self.model_name, index_path=\n            self.index_path)\n        results = temp_vdb.search(query, k)\n        return results\n\n    def get_document_count(self) ->int:\n        \"\"\"Get the number of documents in the index.\"\"\"\n        return len(self.metadata)\n\n    def clear_index(self):\n        \"\"\"Clear the index and metadata.\"\"\"\n        self.index = faiss.IndexFlatIP(self.dimension)\n        self.metadata = []\n",
    "file": "/mnt/ProjectData/omni/rag_manager.py"
  },
  {
    "id": 26,
    "hash": "f59a27842056dbb0dc95d0f29c1c5de8",
    "content": "Hello! I'm OmniForge, an AI coding assistant. I can help you scan, understand, and refactor your codebase using local (Ollama) or remote (OpenRouter) LLMs.\n\nI see you've opened the OmniForge project directory. Some key things I can help with:\n\n1. **Project Analysis**: I can look at your files to understand the structure\n2. **Code Editing**: I can make precise edits to functions/classes using AST-based transformations\n3. **Refactoring**: I can perform multi-file architectural changes with a plan\n4. **File Creation**: I can generate new files based on instructions\n\nFor example:\n- `look .` to scan the project\n- `edit code_editor.py \"add docstrings to main methods\"`\n- `refactor \"extract git operations into a separate module\"`\n- `create new_module.py \"implement a simple HTTP client\"`\n\nWhat would you like to do with this project?",
    "file": "/mnt/ProjectData/omni/requirements.txt"
  },
  {
    "id": 27,
    "hash": "4b879b8e64b677ec78d27f19b384623a",
    "content": "from typing import List\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.text import Text\nfrom rich.status import Status\nfrom rich import box\nfrom contextlib import contextmanager\nfrom rich.markup import escape\nimport gc\n\ntry:\n    from prompt_toolkit import prompt\n    from prompt_toolkit.completion import WordCompleter\n    from prompt_toolkit.history import InMemoryHistory\n    from prompt_toolkit.styles import Style\nexcept ImportError:\n    print('[yellow]prompt_toolkit not installed. Falling back to basic input.[/]')\n    prompt = input\n\n\nclass UIManager:\n    \"\"\"Manages interactive text-based UI with rich for display and prompt_toolkit for input.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initializes the UI manager with a rich console and prompt_toolkit components.\"\"\"\n        self.console = Console()\n        self.history = InMemoryHistory()\n        self.completer = WordCompleter([\n            'send', 'look', 'look_all', 'create', 'edit', 'refactor', 'commit',\n            'save', 'list', 'run', 'history', 'memory', 'backend', 'models',\n            'set', 'personality', 'help', 'exit'\n        ], ignore_case=True)\n        self.prompt_style = Style.from_dict({'prompt': 'bold cyan'})\n\n    def get_user_input(self, prompt_text: str) -> str:\n        try:\n            return prompt(prompt_text, history=self.history, completer=self.completer, style=self.prompt_style)\n        except Exception:\n            self.console.print('[yellow]Warning: Falling back to basic input.[/]')\n            return input(prompt_text)\n\n    def display_history(self, history_text: str) -> None:\n        if not history_text.strip():\n            self.console.print('[dim]No history yet.[/]')\n            return\n        \n        lines = history_text.split('\\n')\n        colored_text = Text()\n        for line in lines:\n            if line.startswith('User:'):\n                colored_text.append(line + '\\n', style='bold blue')\n            elif line.startswith('AI:'):\n                colored_text.append(line + '\\n', style='bold green')\n            elif line.startswith('File '):\n                colored_text.append(line + '\\n', style='yellow')\n            else:\n                colored_text.append(line + '\\n')\n        \n        self.console.print(Panel(\n            colored_text, \n            title='[bold magenta]Chat History[/]', \n            border_style='magenta', \n            expand=False, \n            box=box.ROUNDED\n        ))\n\n    @contextmanager\n    def show_spinner(self, message: str):\n        \"\"\"\n        Context manager that displays a spinner with the given message.\n        Handles Rich LiveError by cleaning up stuck live displays and \n        ensuring proper cleanup on exceptions.\n        \n        Usage:\n            with ui_manager.show_spinner(\"Loading...\"):\n                # do work here\n        \"\"\"\n        # Initialize spinner state if not exists\n        if not hasattr(self, '_spinner_active'):\n            self._spinner_active = False\n        \n        # If spinner is already active, just yield without additional output\n        if self._spinner_active:\n            try:\n                yield\n            except Exception as e:\n                raise e\n            return\n        \n        # Clean up any stuck Rich live displays before starting\n        self._cleanup_stuck_rich_displays()\n        \n        # Mark spinner as active\n        self._spinner_active = True\n        status_context = None\n        \n        try:\n            # Try to use rich status, but with fallback\n            try:\n                status_context = self.console.status(f'[bold yellow]{message}[/]')\n                status_context.__enter__()\n            except Exception:\n                # If rich.status fails, fallback to static print\n                status_context = None\n                self.console.print(f'[bold yellow]{message}[/]')\n            \n            yield\n            \n        except Exception as e:\n            # Re-raise actual work errors\n            raise e\n            \n        finally:\n            # Always cleanup spinner state and rich display\n            self._spinner_active = False\n            if status_context is not None:\n                try:\n                    status_context.__exit__(None, None, None)\n                except:\n                    pass  # Suppress cleanup errors\n            \n            # Force cleanup any remaining rich displays\n            self._cleanup_stuck_rich_displays()\n\n    def _cleanup_stuck_rich_displays(self):\n        \"\"\"\n        Clean up stuck Rich live displays that prevent new ones from starting.\n        Based on solution from: https://github.com/DLR-RM/stable-baselines3/issues/1645\n        \"\"\"\n        try:\n            # Method 1: Reset console live display if exists\n            if hasattr(self.console, '_live') and self.console._live is not None:\n                try:\n                    self.console._live.stop()\n                    self.console._live = None\n                except:\n                    pass\n            \n            # Method 2: Find and close stuck rich/tqdm objects using garbage collection\n            rich_objects = [obj for obj in gc.get_objects() \n                        if hasattr(obj, '__class__') and \n                        ('live' in type(obj).__name__.lower() or \n                            'progress' in type(obj).__name__.lower() or\n                            'status' in type(obj).__name__.lower()) and\n                        hasattr(obj, 'stop')]\n            \n            for rich_obj in rich_objects:\n                try:\n                    if hasattr(rich_obj, 'stop'):\n                        rich_obj.stop()\n                    elif hasattr(rich_obj, 'close'):\n                        rich_obj.close()\n                except:\n                    pass  # Suppress cleanup errors\n                    \n        except:\n            pass  # Suppress all cleanup errors to avoid breaking the main flow\n\n    def display_status_panel(self, personality: str, backend: str, model: str, msg_count: int, look_count: int) -> None:\n        status_text = (\n            f'[bold]Personality:[/] [green]{personality}[/] | '\n            f'[bold]Backend:[/] [green]{backend}[/] | '\n            f'[bold]Model:[/] [green]{model}[/] | '\n            f'[bold]Memory:[/] {msg_count} messages, {look_count} files'\n        )\n        self.console.print(Panel(\n            status_text, \n            title='[bold cyan]Status[/]', \n            border_style='cyan', \n            expand=False, \n            box=box.MINIMAL\n        ))\n\n    def show_success(self, message: str) -> None:\n        self.console.print(f'[green]\u2705 {message}[/]')\n\n    def show_error(self, message: str) -> None:\n        \"\"\"\n        Displays an error message, escaping any markup in the message.\n        \"\"\"\n        safe_message = escape(str(message))\n        self.console.print(f'[red]\u274c {safe_message}[/]')",
    "file": "/mnt/ProjectData/omni/ui_manager.py"
  },
  {
    "id": 28,
    "hash": "dbd143839288b400cafdedb655258434",
    "content": "import os\nimport json\nimport hashlib\nfrom typing import List, Dict, Optional, Tuple\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nimport faiss\nfrom pathlib import Path\n\n\nclass VectorDBManager:\n    \"\"\"Manages vector database operations for RAG using sentence transformers and FAISS.\"\"\"\n\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\n        Optional[str]=None):\n        \"\"\"\n        Initialize the VectorDB manager.\n\n        Args:\n            model_name: Name of the sentence transformer model to use\n            index_path: Path to save/load the FAISS index\n        \"\"\"\n        self.model = SentenceTransformer(model_name)\n        self.index_path = index_path or 'vectordb_index.bin'\n        self.metadata_path = self.index_path.replace('.bin', '_metadata.json')\n        self.dimension = self.model.get_sentence_embedding_dimension()\n        self.index = None\n        self.metadata: List[Dict] = []\n        self._initialize_index()\n\n    def _initialize_index(self):\n        \"\"\"Initialize or load the FAISS index.\"\"\"\n        if os.path.exists(self.index_path) and os.path.exists(self.\n            metadata_path):\n            self.index = faiss.read_index(self.index_path)\n            with open(self.metadata_path, 'r') as f:\n                self.metadata = json.load(f)\n        else:\n            self.index = faiss.IndexFlatIP(self.dimension)\n            self.metadata = []\n\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\n        Dict]]=None):\n        \"\"\"\n        Add documents to the vector database.\n\n        Args:\n            documents: List of text documents to add\n            metadatas: Optional list of metadata for each document\n        \"\"\"\n        if metadatas is None:\n            metadatas = [{}] * len(documents)\n        embeddings = self.model.encode(documents)\n        faiss.normalize_L2(embeddings)\n        self.index.add(embeddings.astype(np.float32))\n        for i, meta in enumerate(metadatas):\n            doc_hash = hashlib.md5(documents[i].encode()).hexdigest()\n            meta_entry = {'id': len(self.metadata), 'hash': doc_hash,\n                'content': documents[i], **meta}\n            self.metadata.append(meta_entry)\n        self._save_index()\n\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\n        \"\"\"\n        Search for relevant documents.\n\n        Args:\n            query: Query string\n            k: Number of results to return\n\n        Returns:\n            List of (document, score, metadata) tuples\n        \"\"\"\n        query_embedding = self.model.encode([query])\n        faiss.normalize_L2(query_embedding)\n        scores, indices = self.index.search(query_embedding.astype(np.\n            float32), k)\n        results = []\n        for score, idx in zip(scores[0], indices[0]):\n            if idx < len(self.metadata):\n                doc_info = self.metadata[idx]\n                results.append((doc_info['content'], float(score), doc_info))\n        return results\n\n    def _save_index(self):\n        \"\"\"Save the FAISS index and metadata to disk.\"\"\"\n        faiss.write_index(self.index, self.index_path)\n        with open(self.metadata_path, 'w') as f:\n            json.dump(self.metadata, f, indent=2)\n\n    def get_document_count(self) ->int:\n        \"\"\"Get the number of documents in the index.\"\"\"\n        return len(self.metadata)\n\n    def clear_index(self):\n        \"\"\"Clear the index and metadata.\"\"\"\n        self.index = faiss.IndexFlatIP(self.dimension)\n        self.metadata = []\n        self._save_index()\n\n\nclass VectorDBManager:\n    \"\"\"Manages vector database operations for RAG using sentence transformers and FAISS.\"\"\"\n\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\n        Optional[str]=None):\n        \"\"\"\n        Initialize the VectorDB manager.\n\n        Args:\n            model_name: Name of the sentence transformer model to use\n            index_path: Path to save/load the FAISS index\n        \"\"\"\n        self.model = SentenceTransformer(model_name)\n        self.index_path = index_path or 'vectordb_index.bin'\n        self.metadata_path = self.index_path.replace('.bin', '_metadata.json')\n        self.dimension = self.model.get_sentence_embedding_dimension()\n        self.index = None\n        self.metadata: List[Dict] = []\n        self._initialize_index()\n\n    def _initialize_index(self):\n        \"\"\"Initialize or load the FAISS index.\"\"\"\n        if os.path.exists(self.index_path) and os.path.exists(self.\n            metadata_path):\n            self.index = faiss.read_index(self.index_path)\n            with open(self.metadata_path, 'r') as f:\n                self.metadata = json.load(f)\n        else:\n            self.index = faiss.IndexFlatIP(self.dimension)\n            self.metadata = []\n\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\n        Dict]]=None):\n        \"\"\"\n        Add documents to the vector database.\n\n        Args:\n            documents: List of text documents to add\n            metadatas: Optional list of metadata for each document\n        \"\"\"\n        if metadatas is None:\n            metadatas = [{}] * len(documents)\n        embeddings = self.model.encode(documents)\n        faiss.normalize_L2(embeddings)\n        self.index.add(embeddings.astype(np.float32))\n        for i, meta in enumerate(metadatas):\n            doc_hash = hashlib.md5(documents[i].encode()).hexdigest()\n            meta_entry = {'id': len(self.metadata), 'hash': doc_hash,\n                'content': documents[i], **meta}\n            self.metadata.append(meta_entry)\n        self._save_index()\n\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\n        \"\"\"\n        Search for relevant documents.\n\n        Args:\n            query: Query string\n            k: Number of results to return\n\n        Returns:\n            List of (document, score, metadata) tuples\n        \"\"\"\n        query_embedding = self.model.encode([query])\n        faiss.normalize_L2(query_embedding)\n        scores, indices = self.index.search(query_embedding.astype(np.\n            float32), k)\n        results = []\n        for score, idx in zip(scores[0], indices[0]):\n            if idx < len(self.metadata):\n                doc_info = self.metadata[idx]\n                results.append((doc_info['content'], float(score), doc_info))\n        return results\n\n    def _save_index(self):\n        \"\"\"Save the FAISS index and metadata to disk.\"\"\"\n        faiss.write_index(self.index, self.index_path)\n        with open(self.metadata_path, 'w') as f:\n            json.dump(self.metadata, f, indent=2)\n\n    def get_document_count(self) ->int:\n        \"\"\"Get the number of documents in the index.\"\"\"\n        return len(self.metadata)\n\n    def clear_index(self):\n        \"\"\"Clear the index and metadata.\"\"\"\n        self.index = faiss.IndexFlatIP(self.dimension)\n        self.metadata = []\n        self._save_index()\n\n\nclass VectorDBManager:\n    \"\"\"Manages vector database operations for RAG using sentence transformers and FAISS.\"\"\"\n\n    def __init__(self, model_name: str='all-MiniLM-L6-v2', index_path:\n        Optional[str]=None):\n        \"\"\"\n        Initialize the VectorDB manager.\n\n        Args:\n            model_name: Name of the sentence transformer model to use\n            index_path: Path to save/load the FAISS index\n        \"\"\"\n        self.model = SentenceTransformer(model_name)\n        self.index_path = index_path or 'vectordb_index.bin'\n        self.metadata_path = self.index_path.replace('.bin', '_metadata.json')\n        self.dimension = self.model.get_sentence_embedding_dimension()\n        self.index = None\n        self.metadata: List[Dict] = []\n        self._initialize_index()\n\n    def _initialize_index(self):\n        \"\"\"Initialize or load the FAISS index.\"\"\"\n        if os.path.exists(self.index_path) and os.path.exists(self.\n            metadata_path):\n            self.index = faiss.read_index(self.index_path)\n            with open(self.metadata_path, 'r') as f:\n                self.metadata = json.load(f)\n        else:\n            self.index = faiss.IndexFlatIP(self.dimension)\n            self.metadata = []\n\n    def add_documents(self, documents: List[str], metadatas: Optional[List[\n        Dict]]=None):\n        \"\"\"\n        Add documents to the vector database.\n\n        Args:\n            documents: List of text documents to add\n            metadatas: Optional list of metadata for each document\n        \"\"\"\n        if metadatas is None:\n            metadatas = [{}] * len(documents)\n        embeddings = self.model.encode(documents)\n        faiss.normalize_L2(embeddings)\n        self.index.add(embeddings.astype(np.float32))\n        for i, meta in enumerate(metadatas):\n            doc_hash = hashlib.md5(documents[i].encode()).hexdigest()\n            meta_entry = {'id': len(self.metadata), 'hash': doc_hash,\n                'content': documents[i], **meta}\n            self.metadata.append(meta_entry)\n        self._save_index()\n\n    def search(self, query: str, k: int=5) ->List[Tuple[str, float, Dict]]:\n        \"\"\"\n        Search for relevant documents.\n\n        Args:\n            query: Query string\n            k: Number of results to return\n\n        Returns:\n            List of (document, score, metadata) tuples\n        \"\"\"\n        query_embedding = self.model.encode([query])\n        faiss.normalize_L2(query_embedding)\n        scores, indices = self.index.search(query_embedding.astype(np.\n            float32), k)\n        results = []\n        for score, idx in zip(scores[0], indices[0]):\n            if idx < len(self.metadata):\n                doc_info = self.metadata[idx]\n                results.append((doc_info['content'], float(score), doc_info))\n        return results\n\n    def _save_index(self):\n        \"\"\"Save the FAISS index and metadata to disk.\"\"\"\n        faiss.write_index(self.index, self.index_path)\n        with open(self.metadata_path, 'w') as f:\n            json.dump(self.metadata, f, indent=2)\n\n    def get_document_count(self) ->int:\n        \"\"\"Get the number of documents in the index.\"\"\"\n        return len(self.metadata)\n\n    def clear_index(self):\n        \"\"\"Clear the index and metadata.\"\"\"\n        self.index = faiss.IndexFlatIP(self.dimension)\n        self.metadata = []\n        self._save_index()\n",
    "file": "/mnt/ProjectData/omni/vectordb_manager.py"
  },
  {
    "id": 29,
    "hash": "47a91d3ac272236378053b23515fc9b0",
    "content": "import tkinter as tk\nfrom tkinter import ttk\nfrom time import strftime\n\n\ndef update_time():\n    current_time = strftime('%H:%M:%S %p\\n%Y-%m-%d')\n    time_label.config(text=current_time)\n    time_label.after(1000, update_time)\n\n\ndef center_window(root):\n    root.update_idletasks()\n    screen_width = root.winfo_screenwidth()\n    screen_height = root.winfo_screenheight()\n    window_width = root.winfo_width() + 4\n    window_height = root.winfo_height() + 4\n    x = (screen_width - window_width) // 2\n    y = (screen_height - window_height) // 2\n    root.geometry(f'{window_width}x{window_height}+{x}+{y}')\n\n\ndef create_3d_border(parent):\n    border_frame = tk.Frame(parent, bg='#c0c0c0')\n    border_frame.pack(padx=2, pady=2, expand=True, fill=tk.BOTH)\n    tk.Frame(border_frame, height=1, bg='#808080').pack(side=tk.TOP, fill=tk.X)\n    tk.Frame(border_frame, width=1, bg='#808080').pack(side=tk.LEFT, fill=tk.Y)\n    tk.Frame(border_frame, height=1, bg='#ffffff').pack(side=tk.BOTTOM,\n        fill=tk.X)\n    tk.Frame(border_frame, width=1, bg='#ffffff').pack(side=tk.RIGHT, fill=tk.Y\n        )\n    inner_container = tk.Frame(border_frame, bg='#c0c0c0')\n    inner_container.pack(side=tk.TOP, expand=True, fill=tk.BOTH, padx=1, pady=1\n        )\n    tk.Frame(inner_container, height=1, bg='#ffffff').pack(side=tk.TOP,\n        fill=tk.X)\n    tk.Frame(inner_container, width=1, bg='#ffffff').pack(side=tk.LEFT,\n        fill=tk.Y)\n    tk.Frame(inner_container, height=1, bg='#808080').pack(side=tk.BOTTOM,\n        fill=tk.X)\n    tk.Frame(inner_container, width=1, bg='#808080').pack(side=tk.RIGHT,\n        fill=tk.Y)\n    content_area = tk.Frame(inner_container, bg='#c0c0c0')\n    content_area.pack(side=tk.TOP, expand=True, fill=tk.BOTH, padx=1, pady=1)\n    return content_area\n\n\nroot = tk.Tk()\nroot.title('Current Time')\nroot.configure(bg='#008080')\nmain_frame = create_3d_border(root)\nmain_frame.config(padx=4, pady=4)\ninner_frame = tk.Frame(main_frame, bg='#c0c0c0', padx=8, pady=8)\ninner_frame.pack()\ntime_label = tk.Label(inner_frame, bg='#c0c0c0', fg='#000000', font=(\n    'MS Sans Serif', 18, 'bold'), padx=12, pady=8)\ntime_label.pack()\nroot.after(100, lambda : center_window(root))\nupdate_time()\nroot.mainloop()\n",
    "file": "/mnt/ProjectData/omni/omni_saves/watch.py"
  }
]